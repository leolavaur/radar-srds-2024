\section{\label{sec:archi}Architecture}

This section details \thecontrib's architecture.
It is divided into three main components:
\begin{enumerate*}[label=(\emph{\roman*})]
    \item our cross-evaluation scheme that provides local feedbacks on each participant's contributions~(\Cref{sec:archi.xeval});
    \item a similarity-based clustering algorithm that groups participants based on evaluations~(\Cref{sec:archi.cluster}); and 
    \item a reputation system that assesses participants' trustworthiness based on their past contributions (\Cref{sec:archi.reput}).
\end{enumerate*}
\Cref{fig:archi} depicts the overview of \thecontrib.

% The following notation conventions will be used in the rest of this article:
% variables are represented by lower-case letters such as \(p_i, n, m^r\),  
% sets are denoted by capital letters such as $P$ and $D$,  
% and families of sets are denoted by capital calligraphic letters such as \(\mathscr{C}\). 
% The set of integers $\mathbb{N} \cap [1,n]$ is denoted by $\llbracket 1,n\rrbracket$. 
% Finally, $\Pr\{A\}$ refers to the probability of event A.
% \Cref{tbl:notations} summarizes the employed notations.

\begin{figure}
    \centering
    \scalebox{0.7}{
    \input{includes/archi.tikz}
    }
    \caption{\label{fig:archi}{Architecture overview}}
\end{figure}


\subsection{Assessing Contributions with Cross-Evaluation}
\label{sec:archi.xeval}

As highlighted in~\Cref{sec:related}, most related works on poisoning mitigation in \gls{fl} rely on server-side models comparison~\cite{fung_limitations_2020,awan_contra_2021}. 
They measure distance between the parameters (for \glspl{dnn}, $n$-dimensional arrays containing the weights and biases of each neuron) using metrics such as cosine similarity~\cite{fung_limitations_2020} or Euclidean distance~\cite{ma_shieldfl_2022}.
However, models that are statistically further from others are not automatically of poor quality.
To cope with this limitation, as well as the absence of source of truth, we propose to rely on client-side evaluation~\cite{zhao_shielding_2020}.
The results of this evaluation can then be used by the server to either discard or weight contributions.
\thecontrib's workflow thus differs from typical approaches by adding an intermediate step for evaluating parameters:

\begin{enumerate}[label={\arabic*}.]
    \item \emph{client fitting} -- The server sends clients training instructions and initial parameters, \ie randoms values for the first round.
    For subsequent rounds, the initial parameters of each client are set to the model $\wbar[k][r-1]$ of the corresponding cluster, using the results of step~\ref{item:3} at round $r-1$.
    Each client trains its own model using the provided hyperparameters, and the initial parameters as a starting point before uploading their parameters $w_i^r$ to the server.\label{item:1}
    
    \item \emph{cross-evaluation} -- The server serializes all client parameters in a single list that is sent to every client. 
    Each client then locally evaluates each received model using its validation set, generating a set of metrics such as loss, accuracy, or F1-score.
    The metrics of all clients are then gathered server-side. \label{item:2}
    
    \item \emph{parameter aggregation} -- The server partitions clients into a set of clusters $\C$ based on the evaluations gathered in step~\ref{item:2}.
    For each cluster $\c \in \C$, the server computes the new model $\wbar = \sum_{i\in\c} \w \weight$, where the weight $\weight$ is given by the reputation system for the participant $\p$. \label{item:3}
    %the model parameters $W^r$, gathered in step~\ref{item:1} into a new set of models $\Wbar$, one model $\wbar$ for each cluster.
\end{enumerate}

The cross-evaluation step generates an evaluation matrix that is used twice in the architecture.
Since this matrix is not symmetric, the vector of \emph{issued evaluations} $\issue$ is used for clustering, while both the \emph{received evaluations} vector $\rece$ and the \emph{issued evaluations} vector $\issue$ are used in the reputation system. 
%An algorithmic depiction of the proposed workflow is available as appendix (\Cref{alg:xeval}).
\Cref{alg:xeval} (in Appendix) details the proposed workflow.

\subsection{Fighting Heterogeneity with Clustering}
\label{sec:archi.cluster}

One of \thecontrib's key characteristics is that the clustering uses the cross-evaluation results to gather similar participants together.
% so that they share a global model specific to their use case. 
% Effectively, the server maintains as many global models as there are clusters at each round.
Indeed, since all participants evaluate the same models, the variation in evaluation results reflects a difference in the evaluation datasets. 
Therefore, participants having similar datasets should issue similar evaluations.
% TODO réduire cette section en précisant explicitement dès le début qu'on utilise les évaluations émises. 
%Issued evaluations are made by a participant towards the others' model parameters, based on its own dataset. 
 
% It its defined as follows.
% separately evaluated two different similarity metrics, both metrics are noted $\sim$ but only one at a time is used.  
% The first one is the  $L^2$-norm $||\p-\p[j]||$, see~\Cref{eq:l2_norm}, also called Euclidean distance; this choice is inline with previous work that directly compare model distance~\cite{briggs_federated_2020, chen_zero_2021, peri_deep_2020}.
% \begin{equation} \label{eq:l2_norm}
% \|p_{i}-p_{j}\| = \sqrt{\sum_{s=1}^{n}{|{\e[i][s]}-{\e[j][s]}|}^{2}}
% \end{equation}

% \begin{equation}\label{eq:cosin_sim}
% %\begin{aligned}
% %    cs(\p[i],\p[j]) &= \frac{\e[i][*]\cdot \e[j][*]}{\|\e[i][*]\|\|\e[j][*]\|} \\
% %    &= {\frac{\displaystyle{\sum_{s=1}^{n}\e[i][s]\e[j][s]}}{
% %    \sqrt{\sum_{s=1}^{n}(\e[i][s])^{2}}
% %    \sqrt{\sum_{s=1}^{n}(\e[j][s])^{2}}
% %    }}
% %\end{aligned}
% cs(\p[i],\p[j]) = \frac{\e[i][*]\cdot \e[j][*]}{\|\e[i][*]\|\|\e[j][*]\|} = \frac{\displaystyle{\sum_{s=1}^{n}\e[i][s]\e[j][s]}}{
%     \sqrt{\sum_{s=1}^{n}(\e[i][s])^{2}}
%     \sqrt{\sum_{s=1}^{n}(\e[j][s])^{2}}
%     }
% \end{equation}

% \begin{equation} \label{eq:l2_norm}
% ||\textit{\p{i}}-\textit{\p{j}}||=\sqrt{\sum_{k=1}^{|P|}{|\textit{\e[i][x]}-\textit{\e[j][x]}|}^{2}}
% \end{equation}
We note $\pdist$ the distance between the evaluations of $\p[i]$ and $\p[j]$ at round $r$.
It is defined as the cosine similarity between their issued evaluation vectors $\issue$ and $\issue[j]$, or $\delta(\issue,\issue[j])$.
We then use it with hierarchical clustering to group similar participants into different clusters using an iterative process.
%Hierarchical clustering works as follows.
Initially, each participant is assigned to a different cluster.
Then, each closest pair of clusters is merged, thus reducing the number of clusters.
The process is repeated until the distance between the two closest clusters exceeds a given threshold. 

While hierarchical clustering does not require the number of clusters as an input, choosing the right threshold can be challenging.
%Unlike \citet{briggs_federated_2020} who use a fix threshold $\theta$ that is tuned for each dataset, 
\thecontrib leverages a dynamic threshold based on the mean inter-distance $\mdist$ between the clusters at round $r$.
The threshold $\theta$ is thus expressed as:
\begin{equation}\label{eq:mean_inter_cluster_distance}
    \theta = \beta\mdist = \beta\sum_{\substack{k,\ell\in\C{},\\ k \ne l}}\frac{\kdist}{|\C{}|(|\C{}|-1)} 
\end{equation}
where $\beta$ is a tunable hyperparameter, and $\kdist$ the distance between two clusters $\c$ and $\c[\ell]$, defined as the distance between their centroids: $\delta(\center,\center[\ell])$.
The centroid $\center$ of a cluster $\c$ is the average of the issued evaluations from its participants at round $r$, or $\frac{1}{|\c|}\sum_{i \in \c}\issue[i]$.

Based on the results of the clustering, the server can then aggregate the models of each cluster $\c$ separately, using the reputation system described in \Cref{sec:archi.reput}.
Consequently, the server maintains as many global models $\wbar$ as there are clusters at each round.



\subsection{Ensuring Quality Contributions with Reputation}
\label{sec:archi.reput}


The reputation system centrally computes the weights $\weight,i\in\c$ used in the aggregation of each cluster model $\wbar$ at round $r$ (see \Cref{sec:archi.xeval}).
Given the existence of methods for common tasks, such as contribution filtering, \thecontrib models trust using a multivalued Dirichlet probability distribution~\cite{fung_dirichlet-based_2011}. 
However, the evaluations $\rece[i]$ received by a participant $\p$ are continuous over $[0,1]$, and thus need to be discretized into a set of $q$ possible values $\mathcal{E} = \{\varepsilon_1, \varepsilon_2, \ldots, \varepsilon_q\}$. 

A Dirichlet distribution on the outcome of an unknown event (\ie, the mean of the received evaluation $\frac{1}{n}\sum_{\e \in \rece} \e$) is usually based on the combination of an initial belief vector and a series of cumulative observations~\cite{fung_dirichlet-based_2011}. 
\thecontrib does not rely on an initial belief vector to bootstrap the reputation but relies on the preceding cross evaluation Step~\ref{item:2}. 
Then, following the notation used by~\citet{fung_dirichlet-based_2011}, we note $\vec{\gamma}^r = \{\gamma_{1}^r,\gamma_{2}^r,\ldots,\gamma_{q}^r\}$ the cumulative evaluations received by $\p$: $\gamma_{2}^r=3$ means that three evaluations in $\rece$ had values bounded by $\left[\frac{1}{q},\frac{2}{q}\right[$.
We then note $\Prob = {\langle\prob[\varepsilon_1],\prob[\varepsilon_2], \ldots, \prob[\varepsilon_q]\rangle}$ the probability distribution vector for the received evaluation of a participant, where $\sum_{s=1}^{q}\prob[\varepsilon_s] = 1$.
Leveraging the cumulative evaluations $\vec{\gamma}^r$, the probability $\cond[\varepsilon_s]$ is given by $\cond[\varepsilon_s] = \gamma_{s} / \sum_{m=1}^{q}{\gamma_{m}}$.

The system further needs to limit the ability of potential malicious participants to manipulate their evaluations, either by badmouthing another participant, or by artificially raising their own ratings.
Consequently, the evaluations issued by a participant $\p \in \c$ are weighted according to their similarity with other cluster members'~\cite{xiong_peertrust_2004} as $\e[i][j][\prime] = \e sim(\issue,\issue[\c])$, where the similarity is defined as:

\begin{equation}\label{eq:sim_reput}
    sim(\issue,\issue[\c]) = 1 - \sqrt{
        \frac{
            \sum_{j=1}^{n}{
                {\left(\e - \sum_{i \in \c} \frac{\e}{|\c|}\right)}^{2}
            }
        }
        {
            |\P|
        }
    }
\end{equation}.


To prevent attacks phased over multiple rounds, while preventing past mistakes from permanently impacting a participant, we use an exponential decay as forgetting factor, noted $\lambda \in [0,1]$. The reputation $\rep$ of a participant $\p$ at round $r$ based on the prior knowledge $\gamma^r_i$ of this participant is given by \Cref{eq:reputation_history}.
Note that a small $\lambda$ gives more importance to recent evaluations: $\lambda=0$ only considers the last round while $\lambda=1$, considers all round with equal weight.  
Based on $\rep$, the weight $\weight$ of $\w$ for aggregation in $\wbar$ (see Step~\ref{item:3} in~\Cref{sec:archi.xeval}) is given by \Cref{eq:weight_computation}.

\vspace{-1cm}
\begin{multicols}{2}
\begin{equation}\label{eq:reputation_history}
    \rep = \sum_{\kappa=1}^{r}\lambda^{r-\kappa}\gamma_i^{\kappa}.
\end{equation}\break
\begin{equation}\label{eq:weight_computation}
    \weight = \frac{\rep}{\sum_{j=1}^{\c}\rep[j]},
\end{equation}
\end{multicols}

As such, the weight $\weight$ of $\p$ will be proportional to its reputation, and therefore the evaluations it received over time.
However, the differences between the weights inside a cluster is relatively small, the use of the exponential decay method, with $\lambda < 1$.
To more significantly penalize malicious participants, we apply a sigmoid function to the weights, using the normal distribution cumulative density function and adjusting its $\sigma$ parameter.





% The reputation system computes the weights used to aggregate the global model $\wbar$ for every cluster $k$ in a centralized way.   
% We denote by $\rep$, the reputation of $\p$ belonging in cluster $\c$. 
% % Reputation $\rep$ is based on the evaluations received from the others cluster members weighted by the similarity with other cluster's participant, history of the previous evaluations. 
% %
% Value of $\rep$ depends on the evaluations received from the others cluster members. 
% To circumvent the impact of abusive evaluations, received evaluations are weighted with respect to the similarity between the emitter evaluations and other cluster members. 
% This similarity is derived from the standard deviation between $\issue[i][]$ and $\center[k][]$. 
% %
% Additionally, in order to give greater weight to the evaluation from the latest rounds, a freshness function based on exponential decay is applied. 
% This function aims at quickly adjusting the reputation of a client when a behavior change occurs.
% %
% Finally, as shown in \Cref{fig:non_expanded_reputation}, while this system can reliably discriminate attackers from benign participants, it does so only by a small gap. 
% Hence, we widen the normalized reputation weight using a sigmoid function to obtain a final participant weight $\weight$ that penalize attackers more heavily. 
% A full description of the function chosen at each step can be found in the Annexes \Cref{sec:annexes.reput}. 

%Calcul de l'espérance. 
%Pourquoi ajouter un historique 
%Comment ajoute-t-on l'historique 
%Pourquoi pondérer les votes eux mêmes 
%Comment les pondérer
% \begin{algorithm}
% \caption{Reputation}\label{alg:cap}
% \begin{algorithmic}[1]
% \Require Cp \Comment{Evaluation des participants}
% \Require $Cluster\_participants$ \Comment{Poids d'appartenance à chacun des clusters}
% \State $y \gets 1$
% \State $X \gets x$
% \State $N \gets n$
% \Comment{Two methodologies are considered for weighting the reputation of each cluster : 1. Weighted average between all nodes that take part (membership >0.) in the cluster 2. use the membership for ponderation}
% \While{$N \neq 0$}
% \If{$N$ is even}
%     \State $X \gets X \times X$
%     \State $N \gets \frac{N}{2}$ \Comment{This is a comment}
% \ElsIf{$N$ is odd}
%     \State $y \gets y \times X$
%     \State $N \gets N - 1$
% \Comment{ Pi évalué en fonction des scores que les membres du nouveau cluster lui ont attribué dans le passé. Si Pi change de cluster c'est l'évaluation des membres du nouveau cluster qui compte}

% \EndIf
% \EndWhile
% \end{algorithmic}
% \end{algorithm}




% Old clustering algorithm 
% \begin{algorithm}
% \caption{Clustering\label{alg:cluster}}

% % notes : ne définit pas la mesure de distance, ni ce sur quoi les clusters 
% % sont comparés.
% \begin{algorithmic}
% \Require $P$,$E^t$
%     \ForAll{$i \in P$}
%         \State $C^t \gets C^t \cup c_k^i$ 
%     \EndFor
%     \Repeat
%     % Repeat et for mal notés, voir https://ctan.mines-albi.fr/macros/latex/contrib/algpseudocodex/algpseudocodex.pdf
%         \State $min \gets \infty$
%         \State $min\_couple \gets \bottom$
%         \For{$\forall k,j \in C^t$}
%             \State $\Delta_k \gets \Call{Distance intra cluster}{k}$
%             \State $\Delta_j \gets \Call{Distance intra cluster}{j}$
%             \State $\Delta_{k,j} \gets \Call{Distance inter cluster}{k,j}$
            
%             % Inutilement lourd pour trouver la distance min intercluster 
%             \If{$\Delta_{k,j} < min$}
%                 \State $min \gets \Delta_{k,j}$
%                 \State $min\_couple \gets (k,j)$
%             \EndIf
%         \EndFor
%         \State $threshold \gets (\Delta_k+\Delta_j*\alpha$
%         \If{$min <= threshold$}
%         \State $\Call{Merge}{min\_couple}$  
%         % Doublon avec la condition de terminaison du until ? 
%         \EndIf
%     \Until{ $min > threshold$}
%     % \EndWhile 
% \end{algorithmic}
% \end{algorithm}
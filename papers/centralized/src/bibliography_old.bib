
@misc{verbraeken_bristle_2021,
	title = {Bristle: {Decentralized} {Federated} {Learning} in {Byzantine}, {Non}-i.i.d. {Environments}},
	shorttitle = {Bristle},
	url = {http://arxiv.org/abs/2110.11006},
	doi = {10.48550/arXiv.2110.11006},
	abstract = {Federated learning (FL) is a privacy-friendly type of machine learning where devices locally train a model on their private data and typically communicate model updates with a server. In decentralized FL (DFL), peers communicate model updates with each other instead. However, DFL is challenging since (1) the training data possessed by different peers is often non-i.i.d. (i.e., distributed differently between the peers) and (2) malicious, or Byzantine, attackers can share arbitrary model updates with other peers to subvert the training process. We address these two challenges and present Bristle, middleware between the learning application and the decentralized network layer. Bristle leverages transfer learning to predetermine and freeze the non-output layers of a neural network, significantly speeding up model training and lowering communication costs. To securely update the output layer with model updates from other peers, we design a fast distance-based prioritizer and a novel performance-based integrator. Their combined effect results in high resilience to Byzantine attackers and the ability to handle non-i.i.d. classes. We empirically show that Bristle converges to a consistent 95\% accuracy in Byzantine environments, outperforming all evaluated baselines. In non-Byzantine environments, Bristle requires 83\% fewer iterations to achieve 90\% accuracy compared to state-of-the-art methods. We show that when the training classes are non-i.i.d., Bristle significantly outperforms the accuracy of the most Byzantine-resilient baselines by 2.3x while reducing communication costs by 90\%.},
	urldate = {2023-01-16},
	publisher = {arXiv},
	author = {Verbraeken, Joost and de Vos, Martijn and Pouwelse, Johan},
	month = oct,
	year = {2021},
	note = {arXiv:2110.11006 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@misc{baruch_little_2019,
	title = {A {Little} {Is} {Enough}: {Circumventing} {Defenses} {For} {Distributed} {Learning}},
	shorttitle = {A {Little} {Is} {Enough}},
	url = {http://arxiv.org/abs/1902.06156},
	doi = {10.48550/arXiv.1902.06156},
	abstract = {Distributed learning is central for large-scale training of deep-learning models. However, they are exposed to a security threat in which Byzantine participants can interrupt or control the learning process. Previous attack models and their corresponding defenses assume that the rogue participants are (a) omniscient (know the data of all other participants), and (b) introduce large change to the parameters. We show that small but well-crafted changes are sufficient, leading to a novel non-omniscient attack on distributed learning that go undetected by all existing defenses. We demonstrate our attack method works not only for preventing convergence but also for repurposing of the model behavior (backdooring). We show that 20\% of corrupt workers are sufficient to degrade a CIFAR10 model accuracy by 50\%, as well as to introduce backdoors into MNIST and CIFAR10 models without hurting their accuracy},
	urldate = {2023-01-11},
	publisher = {arXiv},
	author = {Baruch, Moran and Baruch, Gilad and Goldberg, Yoav},
	month = feb,
	year = {2019},
	note = {arXiv:1902.06156 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{karimireddy_learning_2021,
	title = {Learning from {History} for {Byzantine} {Robust} {Optimization}},
	url = {https://proceedings.mlr.press/v139/karimireddy21a.html},
	abstract = {Byzantine robustness has received significant attention recently given its importance for distributed and federated learning. In spite of this, we identify severe flaws in existing algorithms even when the data across the participants is identically distributed. First, we show realistic examples where current state of the art robust aggregation rules fail to converge even in the absence of any Byzantine attackers. Secondly, we prove that even if the aggregation rules may succeed in limiting the influence of the attackers in a single round, the attackers can couple their attacks across time eventually leading to divergence. To address these issues, we present two surprisingly simple strategies: a new robust iterative clipping procedure, and incorporating worker momentum to overcome time-coupled attacks. This is the first provably robust method for the standard stochastic optimization setting.},
	language = {en},
	urldate = {2022-10-21},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Karimireddy, Sai Praneeth and He, Lie and Jaggi, Martin},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {5311--5319},
}

@misc{chu_securing_2022,
	title = {Securing {Federated} {Sensitive} {Topic} {Classification} against {Poisoning} {Attacks}},
	url = {http://arxiv.org/abs/2201.13086},
	doi = {10.48550/arXiv.2201.13086},
	abstract = {We present a Federated Learning (FL) based solution for building a distributed classifier capable of detecting URLs containing GDPR-sensitive content related to categories such as health, sexual preference, political beliefs, etc. Although such a classifier addresses the limitations of previous offline/centralised classifiers,it is still vulnerable to poisoning attacks from malicious users that may attempt to reduce the accuracy for benign users by disseminating faulty model updates. To guard against this, we develop a robust aggregation scheme based on subjective logic and residual-based attack detection. Employing a combination of theoretical analysis, trace-driven simulation, as well as experimental validation with a prototype and real users, we show that our classifier can detect sensitive content with high accuracy, learn new labels fast, and remain robust in view of poisoning attacks from malicious users, as well as imperfect input from non-malicious ones.},
	urldate = {2022-10-20},
	publisher = {arXiv},
	author = {Chu, Tianyue and Garcia-Recuero, Alvaro and Iordanou, Costas and Smaragdakis, Georgios and Laoutaris, Nikolaos},
	month = jan,
	year = {2022},
	note = {arXiv:2201.13086 [cs]},
	keywords = {68M25, Computer Science - Cryptography and Security, Computer Science - Distributed, Parallel, and Cluster Computing, I.2.11, K.4.1},
}

@article{huang_eefed_2022,
	title = {{EEFED}: {Personalized} federated learning of {Execution}\&{Evaluation} dual network for {CPS} intrusion detection},
	issn = {1556-6021},
	shorttitle = {{EEFED}},
	doi = {10.1109/TIFS.2022.3214723},
	abstract = {In the modern interconnected world, intelligent networks and computing technologies are increasingly being incorporated in industrial systems. However, this adoption of advanced technology has resulted in increased cyber threats to cyber-physical systems. Existing intrusion detection systems are continually challenged by constantly evolving cyber threats. Machine learning algorithms have been applied for intrusion detection. In these techniques, a classification model is trained by learning cyber behavior patterns. However, these models typically require considerable high-quality datasets. Limited attack samples are available because of the unpredictability and constant evolution of cyber threats. To address these problems, we propose a novel federated Execution\&Evaluation dual network framework (EEFED), which allows multiple federal participants to personalize their local detection models undermining the original purpose of Federated Learning. Thus, a general global detection model was developed for collaboratively improving the performance of a single local model against cyberattacks. The proposed personalized update algorithm and the optimizing backtracking parameters replacement policy effectively reduced the negative influence of federated learning in imbalanced and non-i.i.d distribution of data. The proposed method improved model stability. Furthermore, extensive experiments conducted on a network dataset in various cyber scenarios revealed that the proposed method outperformed single model and state-of-the-art methods.},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Huang, Xianting and Liu, Jing and Lai, Yingxu and Mao, Beifeng and Lyu, Hongshuo},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Information Forensics and Security},
	keywords = {Computational modeling, Computer crime, Data models, Federated learning, Intrusion detection, Security, Training, cyber security, cyber-physical system (CPS), intrusion detection, personalized model},
	pages = {1--1},
}

@inproceedings{rajput_detox_2019,
	title = {{DETOX}: {A} {Redundancy}-based {Framework} for {Faster} and {More} {Robust} {Gradient} {Aggregation}},
	volume = {32},
	shorttitle = {{DETOX}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/415185ea244ea2b2bedeb0449b926802-Abstract.html},
	abstract = {To improve the resilience of distributed  training to worst-case, or Byzantine node failures, several recent methods have replaced gradient averaging with robust aggregation methods. Such techniques can have high computational costs, often quadratic in the number of compute nodes, and only have limited robustness guarantees. Other methods have instead used redundancy to guarantee robustness, but can only tolerate limited numbers of Byzantine failures. In this work, we present DETOX, a Byzantine-resilient distributed training framework that combines algorithmic redundancy with robust aggregation. DETOX operates in two steps, a filtering step that uses limited redundancy to significantly reduce the effect of Byzantine nodes, and a hierarchical aggregation step that can be used in tandem with any state-of-the-art robust aggregation method. We show theoretically that this leads to a substantial increase in robustness, and has a per iteration runtime that can be nearly linear in the number of compute nodes. We provide extensive experiments over real distributed setups across a variety of large-scale machine learning tasks, showing that DETOX leads to orders of magnitude accuracy and speedup improvements over many state-of-the-art Byzantine-resilient approaches.},
	urldate = {2022-10-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rajput, Shashank and Wang, Hongyi and Charles, Zachary and Papailiopoulos, Dimitris},
	year = {2019},
	keywords = {⛔ No DOI found},
}

@inproceedings{cao_understanding_2019,
	title = {Understanding {Distributed} {Poisoning} {Attack} in {Federated} {Learning}},
	doi = {10.1109/ICPADS47876.2019.00042},
	abstract = {Federated learning is inherently vulnerable to poisoning attacks, since no training samples will be released to and checked by trustworthy authority. Poisoning attacks are widely investigated in centralized learning paradigm, however distributed poisoning attacks, in which more than one attacker colludes with each other, and injects malicious training samples into local models of their own, may result in a greater catastrophe in federated learning intuitively. In this paper, through real implementation of a federated learning system and distributed poisoning attacks, we obtain several observations about the relations between the number of poisoned training samples, attackers, and attack success rate. Moreover, we propose a scheme, Sniper, to eliminate poisoned local models from malicious participants during training. Sniper identifies benign local models by solving a maximum clique problem, and suspected (poisoned) local models will be ignored during global model updating. Experimental results demonstrate the efficacy of Sniper. The attack success rates are reduced to around 2\% even a third of participants are attackers.},
	booktitle = {2019 {IEEE} 25th {International} {Conference} on {Parallel} and {Distributed} {Systems} ({ICPADS})},
	author = {Cao, Di and Chang, Shan and Lin, Zhijian and Liu, Guohua and Sun, Donghong},
	month = dec,
	year = {2019},
	note = {ISSN: 1521-9097},
	keywords = {federated learning, distributed poisoning attack, defense, attack success rate, label-flipping},
	pages = {233--239},
}

@misc{bertoli_generalizing_2022,
	title = {Generalizing intrusion detection for heterogeneous networks: {A} stacked-unsupervised federated learning approach},
	shorttitle = {Generalizing intrusion detection for heterogeneous networks},
	url = {http://arxiv.org/abs/2209.00721},
	abstract = {The constantly evolving digital transformation imposes new requirements on our society. Aspects relating to reliance on the networking domain and the difficulty of achieving security by design pose a challenge today. As a result, data-centric and machine-learning approaches arose as feasible solutions for securing large networks. Although, in the network security domain, ML-based solutions face a challenge regarding the capability to generalize between different contexts. In other words, solutions based on specific network data usually do not perform satisfactorily on other networks. This paper describes the stacked-unsupervised federated learning (FL) approach to generalize on a cross-silo configuration for a flow-based network intrusion detection system (NIDS). The proposed approach we have examined comprises a deep autoencoder in conjunction with an energy flow classifier in an ensemble learning task. Our approach performs better than traditional local learning and naive cross-evaluation (training in one context and testing on another network data). Remarkably, the proposed approach demonstrates a sound performance in the case of non-iid data silos. In conjunction with an informative feature in an ensemble architecture for unsupervised learning, we advise that the proposed FL-based NIDS results in a feasible approach for generalization between heterogeneous networks. To the best of our knowledge, our proposal is the first successful approach to applying unsupervised FL on the problem of network intrusion detection generalization using flow-based data.},
	language = {en},
	urldate = {2022-09-12},
	publisher = {arXiv},
	author = {Bertoli, Gustavo de Carvalho and Junior, Lourenço Alves Pereira and Santos, Aldri Luiz dos and Saotome, Osamu},
	month = sep,
	year = {2022},
	note = {arXiv:2209.00721 [cs]},
	keywords = {Computer Science - Cryptography and Security, \_read\_urgently},
}

@inproceedings{zhang_blockchain_2021,
	title = {Blockchain {Empowered} {Reliable} {Federated} {Learning} by {Worker} {Selection}: {A} {Trustworthy} {Reputation} {Evaluation} {Method}},
	shorttitle = {Blockchain {Empowered} {Reliable} {Federated} {Learning} by {Worker} {Selection}},
	doi = {10.1109/WCNCW49093.2021.9420026},
	abstract = {Federated learning is a distributed machine learning framework that enables distributed model training with local datasets, which can effectively protect the data privacy of workers (i.e., intelligent edge nodes). The majority of federated learning algorithms assume that the workers are trusted and voluntarily participate in the cooperative model training process. However, the situation in practical application is not consistent with this. There are many challenges such as worker selection schemes for participating workers, which hamper the widespread adoption of federated learning. The existing research about worker selection scheme focused on multi-weight subjective logic model to calculate reputation value and adopted contract theory to motivate workers, which may exist subjective judgmental factors and unfair profit distribution. To address above challenges, we calculate the reputation value by model quality parameters to evaluate the reliability of workers. Blockchain is designed to store historical reputation value that realized tamperresistance and non-repudiation. Numerical results indicate that the worker selection scheme can improve the accuracy of the model and accelerate the model convergence.},
	booktitle = {2021 {IEEE} {Wireless} {Communications} and {Networking} {Conference} {Workshops} ({WCNCW})},
	author = {Zhang, Qinnan and Ding, Qingyang and Zhu, Jianming and Li, Dandan},
	month = mar,
	year = {2021},
	keywords = {Analytical models, Blockchain, Conferences, Data privacy, Machine learning, Predictive models, Training, blockchain, consensus algorithm, federated learning, reputation evaluation},
	pages = {1--6},
}

@article{song_reputation-based_2022,
	title = {Reputation-{Based} {Federated} {Learning} for {Secure} {Wireless} {Networks}},
	volume = {9},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2021.3079104},
	abstract = {The dilemma between the ever-increasing demands for data processing, and the limited capabilities of mobile devices in a wireless communication system calls for the appearance of federated learning (FL). As a distributed machine learning (ML) method, FL executes in an iterative manner by distributing the global model parameters and aggregating the local model parameters, which avoids the transmission of huge raw data and preserves data privacy during the training process. However, since FL cannot control the local training and transmission process, this gives malicious users the opportunity to deteriorate the global aggregation. We adopt a reputation model based on beta distribution function to measure the credibility of local users, and propose a reputation-based scheduling policy with user fairness constraint. By taking into account the impact of wireless channel conditions and malicious attack features, we derive tractable expressions for the convergence rate of FL in a wireless setting. Moreover, we validate the superiority of the proposed reputation-based scheduling policy via numerical analysis and empirical simulations. The results show that the proposed secure wireless FL framework can not only distinguish malicious users from normal users but also effectively defend against several typical attack types featured in attack intensity and attack frequency. The analysis also reveals that the effect of average attack intensity on the convergence performance of FL is dominated by the percentage of malicious user equipments (UEs), and imposes even greater negative effect on the convergence performance of FL as the percentage of malicious UEs increases.},
	number = {2},
	journal = {IEEE Internet of Things Journal},
	author = {Song, Zhendong and Sun, Hongguang and Yang, Howard H. and Wang, Xijun and Zhang, Yan and Quek, Tony Q. S.},
	month = jan,
	year = {2022},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Communication system security, Convergence, Convergence analysis, Data models, Reliability, Scheduling, Training, Wireless networks, federated learning (FL), malicious users, reputation-based scheduling policy, secure wireless networks},
	pages = {1212--1226},
}

@article{huang_personalized_2021,
	title = {Personalized {Cross}-{Silo} {Federated} {Learning} on {Non}-{IID} {Data}},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16960},
	doi = {10.1609/aaai.v35i9.16960},
	abstract = {Non-IID data present a tough challenge for federated learning. In this paper, we explore a novel idea of facilitating pairwise collaborations between clients with similar data. We propose FedAMP, a new method employing federated attentive message passing to facilitate similar clients to collaborate more. We establish the convergence of FedAMP for both convex and non-convex models, and propose a heuristic method to further improve the performance of FedAMP when clients adopt deep neural networks as personalized models. Our extensive experiments on benchmark data sets demonstrate the superior performance of the proposed methods.},
	language = {en},
	number = {9},
	urldate = {2022-09-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Huang, Yutao and Chu, Lingyang and Zhou, Zirui and Wang, Lanjun and Liu, Jiangchuan and Pei, Jian and Zhang, Yong},
	month = may,
	year = {2021},
	pages = {7865--7873},
}

@inproceedings{vasilomanolakis_towards_2017,
	address = {Cham},
	series = {{IFIP} {Advances} in {Information} and {Communication} {Technology}},
	title = {Towards {Trust}-{Aware} {Collaborative} {Intrusion} {Detection}: {Challenges} and {Solutions}},
	isbn = {978-3-319-59171-1},
	shorttitle = {Towards {Trust}-{Aware} {Collaborative} {Intrusion} {Detection}},
	doi = {10.1007/978-3-319-59171-1_8},
	abstract = {Collaborative Intrusion Detection Systems (CIDSs) are an emerging field in cyber-security. In such an approach, multiple sensors collaborate by exchanging alert data with the goal of generating a complete picture of the monitored network. This can provide significant improvements in intrusion detection and especially in the identification of sophisticated attacks. However, the challenge of deciding to which extend a sensor can trust others, has not yet been holistically addressed in related work. In this paper, we firstly propose a set of requirements for reliable trust management in CIDSs. Afterwards, we carefully investigate the most dominant CIDS trust schemes. The main contribution of the paper is mapping the results of the analysis to the aforementioned requirements, along with a comparison of the state of the art. Furthermore, this paper identifies and discusses the research gaps and challenges with regard to trust and CIDSs.},
	language = {en},
	booktitle = {Trust {Management} {XI}},
	publisher = {Springer International Publishing},
	author = {Vasilomanolakis, Emmanouil and Habib, Sheikh Mahbub and Milaszewicz, Pavlos and Malik, Rabee Sohail and Mühlhäuser, Max},
	editor = {Steghöfer, Jan-Philipp and Esfandiari, Babak},
	year = {2017},
	keywords = {Computational Trust, Initial Trust, Intrusion Detection System, Trust Level, Trust Management, decay},
	pages = {94--109},
}

@inproceedings{fung_trust_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Trust {Management} for {Host}-{Based} {Collaborative} {Intrusion} {Detection}},
	isbn = {978-3-540-87353-2},
	doi = {10.1007/978-3-540-87353-2_9},
	abstract = {The accuracy of detecting an intrusion within a network of intrusion detection systems (IDSes) depends on the efficiency of collaboration between member IDSes. The security itself within this network is an additional concern that needs to be addressed. In this paper, we present a trust-based framework for secure and effective collaboration within an intrusion detection network (IDN). In particular, we define a trust model that allows each IDS to evaluate the trustworthiness of others based on personal experience. We prove the correctness of our approach in protecting the IDN. Additionally, experimental results demonstrate that our system yields a significant improvement in detecting intrusions. The trust model further improves the robustness of the collaborative system against malicious attacks.},
	language = {en},
	booktitle = {Managing {Large}-{Scale} {Service} {Deployment}},
	publisher = {Springer},
	author = {Fung, Carol J. and Baysal, Olga and Zhang, Jie and Aib, Issam and Boutaba, Raouf},
	editor = {De Turck, Filip and Kellerer, Wolfgang and Kormentzas, George},
	year = {2008},
	keywords = {Collaboration, Decay, Intrusion detection Network, Peer-to-Peer, Security, Trust Management},
	pages = {109--122},
}

@misc{severi_network-level_2022,
	title = {Network-{Level} {Adversaries} in {Federated} {Learning}},
	url = {http://arxiv.org/abs/2208.12911},
	abstract = {Federated learning is a popular strategy for training models on distributed, sensitive data, while preserving data privacy. Prior work identiﬁed a range of security threats on federated learning protocols that poison the data or the model. However, federated learning is a networked system where the communication between clients and server plays a critical role for the learning task performance. We highlight how communication introduces another vulnerability surface in federated learning and study the impact of network-level adversaries on training federated learning models. We show that attackers dropping the network trafﬁc from carefully selected clients can signiﬁcantly decrease model accuracy on a target population. Moreover, we show that a coordinated poisoning campaign from a few clients can amplify the dropping attacks. Finally, we develop a server-side defense which mitigates the impact of our attacks by identifying and up-sampling clients likely to positively contribute towards target accuracy. We comprehensively evaluate our attacks and defenses on three datasets, assuming encrypted communication channels and attackers with partial visibility of the network.},
	language = {en},
	urldate = {2022-09-05},
	publisher = {arXiv},
	author = {Severi, Giorgio and Jagielski, Matthew and Yar, Gökberk and Wang, Yuxuan and Oprea, Alina and Nita-Rotaru, Cristina},
	month = aug,
	year = {2022},
	note = {arXiv:2208.12911 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture},
}

@misc{sun_dpauc_2022,
	title = {{DPAUC}: {Differentially} {Private} {AUC} {Computation} in {Federated} {Learning}},
	shorttitle = {{DPAUC}},
	url = {http://arxiv.org/abs/2208.12294},
	abstract = {Federated learning (FL) has gained signiﬁcant attention recently as a privacyenhancing tool to jointly train a machine learning model by multiple participants. The prior work on FL has mostly studied how to protect label privacy during model training. However, model evaluation in FL might also lead to potential leakage of private label information. In this work, we propose an evaluation algorithm that can accurately compute the widely used AUC (area under the curve) metric when using the label differential privacy (DP) in FL. Through extensive experiments, we show our algorithms can compute accurate AUCs compared to the ground truth.},
	language = {en},
	urldate = {2022-09-05},
	publisher = {arXiv},
	author = {Sun, Jiankai and Yang, Xin and Yao, Yuanshun and Xie, Junyuan and Wu, Di and Wang, Chong},
	month = aug,
	year = {2022},
	note = {arXiv:2208.12294 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{blanchard_machine_2017,
	title = {Machine learning with adversaries: {Byzantine} tolerant gradient descent},
	volume = {30},
	shorttitle = {Machine learning with adversaries},
	journal = {Advances in Neural Information Processing Systems},
	author = {Blanchard, Peva and El Mhamdi, El Mahdi and Guerraoui, Rachid and Stainer, Julien},
	year = {2017},
}

@inproceedings{you_poisoning_2022,
	title = {Poisoning attack detection using client historical similarity in non-iid environments},
	doi = {10.1109/Confluence52989.2022.9734158},
	abstract = {Federated learning has drawn widespread attention as privacy-preserving solution, which has a protective effect on data security and privacy. It has unique distributed machine learning mechanism, namely model sharing instead of data sharing. However, the mechanism also leads to the fact that malicious clients can easily train local model based on poisoned data and upload it to the server for contaminating the global model, thus severely hampering the development of federated learning. In this paper, we build a federated learning system and simulate heterogeneous data on each client for training. Although we cannot directly differentiate malicious customers by the uploaded model in a heterogeneous data environment, by experiments we found some features that are used to distinguish malicious customers from benign customers during training. Given above, we propose a federated learning poisoning attack detection method for detecting malicious clients and ensuring aggregation quality. The method can filter out anomaly models by comparing the similarity of the historical changes of clients and gradually identifying attacker clients through reputation mechanism. We experimentally demonstrate that the method significantly improves the performance of the global model even when the proportion of malicious clients is as high as one-third.},
	booktitle = {2022 12th {International} {Conference} on {Cloud} {Computing}, {Data} {Science} \& {Engineering} ({Confluence})},
	author = {You, XinTong and Liu, Zhengqi and Yang, Xu and Ding, Xuyang},
	month = jan,
	year = {2022},
	keywords = {Collaborative work, Distributed Machine Learning, Distributed databases, Euclidean distance, Federated Learning, Heterogeneous Data, Machine learning, Market research, Poisoning Attack Detection, Resists, Training},
	pages = {439--447},
}

@article{zhou_differentially_2022,
	title = {A {Differentially} {Private} {Federated} {Learning} {Model} against {Poisoning} {Attacks} in {Edge} {Computing}},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2022.3168556},
	abstract = {Federated learning is increasingly popular, as it allows us to circumvent challenges due to data islands, by training a global model using data from one or more data owners/sources. However, in edge computing, resource-constrained end devices are vulnerable to be compromised and abused to facilitate poisoning attacks. Privacy-preserving is another important property to consider when dealing with sensitive user data on end devices. Most existing approaches only consider either defending against poisoning attacks or supporting privacy, but not both properties simultaneously. In this paper, we propose a differentially private federated learning model against poisoning attacks, designed for edge computing deployment. First, we design a weight-based algorithm to perform anomaly detection on the parameters uploaded by end devices in edge nodes, which improves detection rate using only small-size validation datasets and minimizes the communication cost. Then, differential privacy technology is leveraged to protect the privacy of both data and model in an edge computing setting. We also evaluate and compare the detection performance in the presence of random and customized malicious end devices with the state-of-the-art, in terms of attack resiliency, communication and computation costs. Experimental results demonstrate that our scheme can achieve an optimal tradeoff between security, efficiency and accuracy.},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Zhou, Jun and Wu, Nan and Wang, Yisong and Gu, Shouzhen and Cao, Zhenfu and Dong, Xiaolei and Choo, Kim-Kwang Raymond},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Collaborative work, Computational modeling, Data models, Differential privacy, Edge computing, Federated learning, High-practicability, Image edge detection, Poisoning attack, Privacy, Training},
	pages = {1--1},
}

@inproceedings{hsieh_non-iid_2020,
	title = {The {Non}-{IID} {Data} {Quagmire} of {Decentralized} {Machine} {Learning}},
	url = {https://proceedings.mlr.press/v119/hsieh20a.html},
	abstract = {Many large-scale machine learning (ML) applications need to perform decentralized learning over datasets generated at different devices and locations. Such datasets pose a significant challenge to decentralized learning because their different contexts result in significant data distribution skew across devices/locations. In this paper, we take a step toward better understanding this challenge by presenting a detailed experimental study of decentralized DNN training on a common type of data skew: skewed distribution of data labels across devices/locations. Our study shows that: (i) skewed data labels are a fundamental and pervasive problem for decentralized learning, causing significant accuracy loss across many ML applications, DNN models, training datasets, and decentralized learning algorithms; (ii) the problem is particularly challenging for DNN models with batch normalization; and (iii) the degree of data skew is a key determinant of the difficulty of the problem. Based on these findings, we present SkewScout, a system-level approach that adapts the communication frequency of decentralized learning algorithms to the (skew-induced) accuracy loss between data partitions. We also show that group normalization can recover much of the accuracy loss of batch normalization.},
	language = {en},
	urldate = {2022-09-01},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hsieh, Kevin and Phanishayee, Amar and Mutlu, Onur and Gibbons, Phillip},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {4387--4398},
}

@article{tian_comprehensive_2022,
	title = {A {Comprehensive} {Survey} on {Poisoning} {Attacks} and {Countermeasures} in {Machine} {Learning}},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3551636},
	doi = {10.1145/3551636},
	abstract = {The prosperity of machine learning has been accompanied by increasing attacks on the training process. Among them, poisoning attacks have become an emerging threat during model training. Poisoning attacks have profound impacts on the target models, e.g., making them unable to converge or manipulating their prediction results. Moreover, the rapid development of recent distributed learning frameworks, especially federated learning, has further stimulated the development of poisoning attacks. Defending against poisoning attacks is challenging and urgent. However, the systematic review from a unified perspective remains blank. This survey provides an in-depth and up-to-date overview of poisoning attacks and corresponding countermeasures in both centralized and federated learning. We firstly categorize attack methods based on their goals. Secondly, we offer detailed analysis of the differences and connections among the attack techniques. Furthermore, we present countermeasures in different learning framework and highlight their advantages and disadvantages. Finally, we discuss the reasons for the feasibility of poisoning attacks and address the potential research directions from attacks and defenses perspectives, separately.},
	urldate = {2022-09-01},
	journal = {ACM Computing Surveys},
	author = {Tian, Zhiyi and Cui, Lei and Liang, Jie and Yu, Shui},
	year = {2022},
	note = {Just Accepted},
	keywords = {Deep learning, backdoor attack, federated learning, poisoning attack},
}

@misc{guo_robust_2021,
	title = {Robust and {Privacy}-{Preserving} {Collaborative} {Learning}: {A} {Comprehensive} {Survey}},
	shorttitle = {Robust and {Privacy}-{Preserving} {Collaborative} {Learning}},
	url = {http://arxiv.org/abs/2112.10183},
	doi = {10.48550/arXiv.2112.10183},
	abstract = {With the rapid demand of data and computational resources in deep learning systems, a growing number of algorithms to utilize collaborative machine learning techniques, for example, federated learning, to train a shared deep model across multiple participants. It could effectively take advantage of the resources of each participant and obtain a more powerful learning system. However, integrity and privacy threats in such systems have greatly obstructed the applications of collaborative learning. And a large amount of works have been proposed to maintain the model integrity and mitigate the privacy leakage of training data during the training phase for different collaborative learning systems. Compared with existing surveys that mainly focus on one specific collaborative learning system, this survey aims to provide a systematic and comprehensive review of security and privacy researches in collaborative learning. Our survey first provides the system overview of collaborative learning, followed by a brief introduction of integrity and privacy threats. In an organized way, we then detail the existing integrity and privacy attacks as well as their defenses. We also list some open problems in this area and opensource the related papers on GitHub: https://github.com/csl-cqu/awesome-secure-collebrative-learning-papers.},
	urldate = {2022-09-01},
	publisher = {arXiv},
	author = {Guo, Shangwei and Zhang, Xu and Yang, Fei and Zhang, Tianwei and Gan, Yan and Xiang, Tao and Liu, Yang},
	month = dec,
	year = {2021},
	note = {arXiv:2112.10183 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@misc{ramirez_poisoning_2022,
	title = {Poisoning {Attacks} and {Defenses} on {Artificial} {Intelligence}: {A} {Survey}},
	shorttitle = {Poisoning {Attacks} and {Defenses} on {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2202.10276},
	doi = {10.48550/arXiv.2202.10276},
	abstract = {Machine learning models have been widely adopted in several fields. However, most recent studies have shown several vulnerabilities from attacks with a potential to jeopardize the integrity of the model, presenting a new window of research opportunity in terms of cyber-security. This survey is conducted with a main intention of highlighting the most relevant information related to security vulnerabilities in the context of machine learning (ML) classifiers; more specifically, directed towards training procedures against data poisoning attacks, representing a type of attack that consists of tampering the data samples fed to the model during the training phase, leading to a degradation in the models accuracy during the inference phase. This work compiles the most relevant insights and findings found in the latest existing literatures addressing this type of attacks. Moreover, this paper also covers several defense techniques that promise feasible detection and mitigation mechanisms, capable of conferring a certain level of robustness to a target model against an attacker. A thorough assessment is performed on the reviewed works, comparing the effects of data poisoning on a wide range of ML models in real-world conditions, performing quantitative and qualitative analyses. This paper analyzes the main characteristics for each approach including performance success metrics, required hyperparameters, and deployment complexity. Moreover, this paper emphasizes the underlying assumptions and limitations considered by both attackers and defenders along with their intrinsic properties such as: availability, reliability, privacy, accountability, interpretability, etc. Finally, this paper concludes by making references of some of main existing research trends that provide pathways towards future research directions in the field of cyber-security.},
	urldate = {2022-09-01},
	publisher = {arXiv},
	author = {Ramirez, Miguel A. and Kim, Song-Kyoo and Hamadi, Hussam Al and Damiani, Ernesto and Byon, Young-Ji and Kim, Tae-Yeon and Cho, Chung-Suk and Yeun, Chan Yeob},
	month = feb,
	year = {2022},
	note = {arXiv:2202.10276 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@inproceedings{peri_deep_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Deep k-{NN} {Defense} {Against} {Clean}-{Label} {Data} {Poisoning} {Attacks}},
	isbn = {978-3-030-66415-2},
	doi = {10.1007/978-3-030-66415-2_4},
	abstract = {Targeted clean-label data poisoning is a type of adversarial attack on machine learning systems in which an adversary injects a few correctly-labeled, minimally-perturbed samples into the training data, causing a model to misclassify a particular test sample during inference. Although defenses have been proposed for general poisoning attacks, no reliable defense for clean-label attacks has been demonstrated, despite the attacks’ effectiveness and realistic applications. In this work, we propose a simple, yet highly-effective Deep k-NN defense against both feature collision and convex polytope clean-label attacks on the CIFAR-10 dataset. We demonstrate that our proposed strategy is able to detect over 99\% of poisoned examples in both attacks and remove them without compromising model performance. Additionally, through ablation studies, we discover simple guidelines for selecting the value of k as well as for implementing the Deep k-NN defense on real-world datasets with class imbalance. Our proposed defense shows that current clean-label poisoning attack strategies can be annulled, and serves as a strong yet simple-to-implement baseline defense to test future clean-label poisoning attacks. Our code is available on GitHub.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Peri, Neehar and Gupta, Neal and Huang, W. Ronny and Fowl, Liam and Zhu, Chen and Feizi, Soheil and Goldstein, Tom and Dickerson, John P.},
	editor = {Bartoli, Adrien and Fusiello, Andrea},
	year = {2020},
	keywords = {Adversarial attacks, Clean label poisoning, Deep k-NN, Machine learning},
	pages = {55--70},
}

@article{wang_threats_2022,
	title = {Threats to {Training}: {A} {Survey} of {Poisoning} {Attacks} and {Defenses} on {Machine} {Learning} {Systems}},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Threats to {Training}},
	url = {https://dl.acm.org/doi/10.1145/3538707},
	doi = {10.1145/3538707},
	abstract = {Machine learning (ML) has been universally adopted for automated decisions in a variety of ields, including recognition and classiication applications, recommendation systems, natural language processing, etc. However, in the light of high expenses on training data and computing resources, recent years have witnessed a rapid increase in outsourced ML training, either partially or completely, which provides vulnerabilities for adversaries to exploit. A prime threat in training phase is called poisoning attack, where adversaries strive to subvert the behavior of machine learning systems by poisoning training data or other means of interference. Although a growing number of relevant studies have been proposed, the research among poisoning attack is still overly scattered, with each paper focusing on a particular task in a speciic domain. In this survey, we summarize and categorize existing attack methods and corresponding defenses, as well as demonstrate compelling application scenarios, thus providing a uniied framework to analyze poisoning attacks. Besides, we also discuss the main limitations of current works, along with the corresponding future directions to facilitate further researches. Our ultimate motivation is to provide a comprehensive and self-contained survey of this growing ield of research and lay the foundation for a more standardized approach to reproducible studies. CCS Concepts: · Theory of computation → Adversarial learning; · Security and privacy → Systems security.},
	language = {en},
	urldate = {2022-09-01},
	journal = {ACM Computing Surveys},
	author = {Wang, Zhibo and Ma, Jingjing and Wang, Xue and Hu, Jiahui and Qin, Zhan and Ren, Kui},
	month = may,
	year = {2022},
	pages = {3538707},
}

@inproceedings{liu_federated_2022,
	title = {Federated {Learning} with {Anomaly} {Client} {Detection} and {Decentralized} {Parameter} {Aggregation}},
	doi = {10.1109/DSN-W54100.2022.00016},
	abstract = {Federated learning is a framework for machine learning that is dedicated to data privacy protection. In federated learning, system cannot fully control the behavior of clients which can be faulty. These behaviors include sharing arbitrary faulty gradients and delaying the process of sharing due to Byzantine attacks or clients’ own software and hardware failures. In federated learning, the parameter server may also be faulty during gradient collection and aggregation, mainly including gradient-based training data inference and model parameter faulty update. The above problems may lead to reduced accuracy of federated learning model training, leakage of client privacy, etc. Existing research enhances the robustness of federated learning by exploiting the decentralization and immutability of Blockchain. For untrusted clients, most research is based on Byzantine fault tolerance to defend against clients indiscriminately, and may cause model accuracy reduction. In addition, most of the research focus on unencrypted gradients, and there is insufficient research on dealing with client anomalies in the case of gradient encryption. For untrusted parameter servers, existing research has problems in energy overhead and scalability. Aiming at the problems above, this paper studies the robustness of federated learning, and proposes a blockchain-based federated learning parameter update architecture PUS-FL. Through experiments simulating distributed machine learning on neural networks, we demonstrate that the anomaly detection algorithm of PUS-FL outperforms conventional gradient filters including geometric median, Multi-Krum and trimmed mean. In addition, our experiments also verify that the scalability-enhanced parameter aggregation consensus algorithm proposed in this paper(SE-PBFT) improves consensus scalability by reducing communication complexity.},
	booktitle = {2022 52nd {Annual} {IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} {Workshops} ({DSN}-{W})},
	author = {Liu, Shu and Shang, Yanlei},
	month = jun,
	year = {2022},
	note = {ISSN: 2325-6664},
	keywords = {Blockchain, Byzantine Attack, Collaborative work, Consensus Algorithm, Fault tolerant systems, Federated learning, Inference algorithms, Machine learning, Privacy, Robustness, Scalability, Trusted Computing},
	pages = {37--43},
}

@article{kumar_fedclean_nodate,
	title = {{FEDCLEAN}: {A} {DEFENSE} {MECHANISM} {AGAINST} {PARAMETER} {POISONING} {ATTACKS} {IN} {FEDERATED} {LEARNING}},
	doi = {10.1109/ICASSP43922.2022.9747497},
	abstract = {In Federated learning (FL) systems, a centralized entity (server), instead of access to the training data, has access to model parameter updates computed by each participant independently and based solely on their samples. Unfortunately, FL is susceptible to model poisoning attacks, in which malicious or malfunctioning entities share polluted updates that can compromise the model’s accuracy. In this study, we propose FedClean, an FL mechanism that is robust to model poisoning attacks. The accuracy of the models trained with the assistance of FedClean is close to the one where malicious entities do not participate.},
	language = {en},
	author = {Kumar, Abhishek and Khimani, Vivek and Chatzopoulos, Dimitris and Hui, Pan},
	keywords = {agent selection, cosin similarity},
	pages = {5},
}

@misc{dunnett_trusted_2022,
	title = {A {Trusted}, {Verifiable} and {Differential} {Cyber} {Threat} {Intelligence} {Sharing} {Framework} using {Blockchain}},
	url = {http://arxiv.org/abs/2208.12031},
	abstract = {Cyber Threat Intelligence (CTI) is the knowledge of cyber and physical threats that help mitigate potential cyber attacks. The rapid evolution of the current threat landscape has seen many organisations share CTI to strengthen their security posture for mutual beneﬁt. However, in many cases, CTI data contains attributes (e.g., software versions) that have the potential to leak sensitive information or cause reputational damage to the sharing organisation. While current approaches allow restricting CTI sharing to trusted organisations, they lack solutions where the shared data can be veriﬁed and disseminated ‘differentially’ (i.e., selective information sharing) with policies and metrics ﬂexibly deﬁned by an organisation. In this paper, we propose a blockchain-based CTI sharing framework that allows organisations to share sensitive CTI data in a trusted, veriﬁable and differential manner. We discuss the limitations associated with existing approaches and highlight the advantages of the proposed CTI sharing framework. We further present a detailed proof of concept using the Ethereum blockchain network. Our experimental results show that the proposed framework can facilitate the exchange of CTI without creating signiﬁcant additional overheads.},
	language = {en},
	urldate = {2022-08-30},
	publisher = {arXiv},
	author = {Dunnett, Kealan and Pal, Shantanu and Putra, Guntur Dharma and Jadidi, Zahra and Jurdak, Raja},
	month = aug,
	year = {2022},
	note = {arXiv:2208.12031 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Distributed, Parallel, and Cluster Computing},
}

@article{miao_privacy-preserving_2022,
	title = {Privacy-{Preserving} {Byzantine}-{Robust} {Federated} {Learning} via {Blockchain} {Systems}},
	volume = {17},
	issn = {1556-6021},
	doi = {10.1109/TIFS.2022.3196274},
	abstract = {Federated learning enables clients to train a machine learning model jointly without sharing their local data. However, due to the centrality of federated learning framework and the untrustworthiness of clients, traditional federated learning solutions are vulnerable to poisoning attacks from malicious clients and servers. In this paper, we aim to mitigate the impact of the central server and malicious clients by designing a Privacy-preserving Byzantine-robust Federated Learning (PBFL) scheme based on blockchain. Specifically, we use cosine similarity to judge the malicious gradients uploaded by malicious clients. Then, we adopt fully homomorphic encryption to provide secure aggregation. Finally, we use blockchain system to facilitate transparent processes and implementation of regulations. Our formal analysis proves that our scheme achieves convergence and provides privacy protection. Our extensive experiments on different datasets demonstrate that our scheme is robust and efficient. Even if the root dataset is small, our scheme can achieve the same efficiency as FedSGD.},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Miao, Yinbin and Liu, Ziteng and Li, Hongwei and Choo, Kim-Kwang Raymond and Deng, Robert H.},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Information Forensics and Security},
	keywords = {Blockchains, Collaborative work, Computational modeling, Federated learning, Privacy, Resists, Servers, Training, blockchain, fully homomorphic encryption, poisoning attacks},
	pages = {2848--2861},
}

@article{yang_personalized_2022,
	title = {Personalized {Federated} {Learning} on {Non}-{IID} {Data} via {Group}-{Based} {Meta}-{Learning}},
	issn = {1556-4681},
	url = {https://doi.org/10.1145/3558005},
	doi = {10.1145/3558005},
	abstract = {Personalized federated learning (PFL) has emerged as a paradigm to provide a personalized model that can fit the local data distribution of each client. One natural choice for PFL is to leverage the fast adaptation capability of meta-learning, where it first obtains a single global model, and each client achieves a personalized model by fine-tuning the global one with its local data. However, existing meta-learning-based approaches implicitly assume that the data distribution among different clients is similar, which may not be applicable due to the property of data heterogeneity in federated learning. In this work, we propose a Group-based Federated Meta-Learning framework, called G-FML, which adaptively divides the clients into groups based on the similarity of their data distribution, and the personalized models are obtained with meta-learning within each group. In particular, we develop a simple yet effective grouping mechanism to adaptively partition the clients into multiple groups. Our mechanism ensures that each group is formed by the clients with similar data distribution such that the group-wise meta-model can achieve “personalization” at large. By doing so, our framework can be generalized to a highly heterogeneous environment. We evaluate the effectiveness of our proposed G-FML framework on three heterogeneous benchmarking datasets. The experimental results show that our framework improves the model accuracy by up to 13.15\% relative to the state-of-the-art federated meta-learning.},
	urldate = {2022-08-29},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Yang, Lei and Huang, Jiaming and Lin, Wanyu and Cao, Jiannong},
	year = {2022},
	note = {Just Accepted},
	keywords = {Federated learning, clustering methods, meta learning, neural networks},
}

@inproceedings{Shen2016,
	address = {New York, NY, USA},
	title = {Auror: defending against poisoning attacks in collaborative deep learning systems},
	isbn = {978-1-4503-4771-6},
	url = {https://dl.acm.org/doi/10.1145/2991079.2991125},
	doi = {10.1145/2991079.2991125},
	booktitle = {Proceedings of the 32nd {Annual} {Conference} on {Computer} {Security} {Applications}},
	publisher = {ACM},
	author = {Shen, Shiqi and Tople, Shruti and Saxena, Prateek},
	month = dec,
	year = {2016},
	pages = {508--519},
}

@misc{zhao_shielding_2020,
	title = {Shielding {Collaborative} {Learning}: {Mitigating} {Poisoning} {Attacks} through {Client}-{Side} {Detection}},
	shorttitle = {Shielding {Collaborative} {Learning}},
	url = {http://arxiv.org/abs/1910.13111},
	abstract = {Collaborative learning allows multiple clients to train a joint model without sharing their data with each other. Each client performs training locally and then submits the model updates to a central server for aggregation. Since the server has no visibility into the process of generating the updates, collaborative learning is vulnerable to poisoning attacks where a malicious client can generate a poisoned update to introduce backdoor functionality to the joint model. The existing solutions for detecting poisoned updates, however, fail to defend against the recently proposed attacks, especially in the non-IID setting. In this paper, we present a novel defense scheme to detect anomalous updates in both IID and non-IID settings. Our key idea is to realize client-side cross-validation, where each update is evaluated over other clients' local data. The server will adjust the weights of the updates based on the evaluation results when performing aggregation. To adapt to the unbalanced distribution of data in the non-IID setting, a dynamic client allocation mechanism is designed to assign detection tasks to the most suitable clients. During the detection process, we also protect the client-level privacy to prevent malicious clients from stealing the training data of other clients, by integrating differential privacy with our design without degrading the detection performance. Our experimental evaluations on two real-world datasets show that our scheme is significantly robust to two representative poisoning attacks.},
	urldate = {2022-08-28},
	publisher = {arXiv},
	author = {Zhao, Lingchen and Hu, Shengshan and Wang, Qian and Jiang, Jianlin and Shen, Chao and Luo, Xiangyang and Hu, Pengfei},
	month = mar,
	year = {2020},
	note = {arXiv:1910.13111 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{Chismon2015,
	title = {Threat {Intelligence}: {Collecting}, {Analysing}, {Evaluating}},
	abstract = {Threat intelligence is rapidly becoming an ever-higher business priority. There is a general awareness of the need to ‘do’ threat intelligence, and vendors are falling over themselves to offer a confusingly diverse array of threat intelligence products. Figure},
	journal = {Cert-Uk},
	author = {Chismon, David and Ruks, Martyn},
	year = {2015},
	keywords = {⛔ No DOI found},
	pages = {36},
}

@misc{NIS2016,
	title = {Directive ({EU}) 2016/1148 of 6 {July} 2016 concerning measures for a high common level of security of network and information systems across the {Union}},
	url = {https://eur-lex.europa.eu/eli/dir/2016/1148/oj},
	abstract = {It proposes a wide-ranging set of measures to boost the level of security of network and information systems (cybersecurity*) to secure services vital to the EU economy and society. It aims to ensure that EU countries are well-prepared and are ready to handle and respond to cyberattacks through: the designation of competent authorities, the set-up of computer-security incident response teams (CSIRTs), and the adoption of national cybersecurity strategies. It also establishes EU-level cooperation both at strategic and technical level. Lastly, it introduces the obligation on essential-services providers and digital service providers to take the appropriate security measures and to notify the relevant national authorities about serious incidents.},
	author = {{The European Parliament and The Counsil}},
	year = {2016},
}

@article{Vasilomanolakis2015,
	title = {Taxonomy and {Survey} of {Collaborative} {Intrusion} {Detection}},
	volume = {47},
	doi = {10.1145/2716260},
	language = {en},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Vasilomanolakis, Emmanouil and Karuppayah, Shankar and Fischer, Mathias},
	month = may,
	year = {2015},
	keywords = {+survey, \_processed},
	pages = {33},
}

@inproceedings{jia_towards_2019,
	title = {Towards {Efficient} {Data} {Valuation} {Based} on the {Shapley} {Value}},
	url = {https://proceedings.mlr.press/v89/jia19a.html},
	abstract = {\{{\textbackslash}em “How much is my data worth?”\} is an increasingly common question posed by organizations and individuals alike. An answer to this question could allow, for instance, fairly distributing profits among multiple data contributors and determining prospective compensation when data breaches happen. In this paper, we study the problem of {\textbackslash}emph\{data valuation\} by utilizing the Shapley value, a popular notion of value which originated in coopoerative game theory. The Shapley value defines a unique payoff scheme that satisfies many desiderata for the notion of data value. However, the Shapley value often requires {\textbackslash}emph\{exponential\} time to compute. To meet this challenge, we propose a repertoire of efficient algorithms for approximating the Shapley value. We also demonstrate the value of each training instance for various benchmark datasets.},
	language = {en},
	urldate = {2022-08-25},
	booktitle = {Proceedings of the {Twenty}-{Second} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Jia, Ruoxi and Dao, David and Wang, Boxin and Hubis, Frances Ann and Hynes, Nick and Gürel, Nezihe Merve and Li, Bo and Zhang, Ce and Song, Dawn and Spanos, Costas J.},
	month = apr,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1167--1176},
}

@inproceedings{song_profit_2019,
	title = {Profit {Allocation} for {Federated} {Learning}},
	doi = {10.1109/BigData47090.2019.9006327},
	abstract = {Due to stricter data management regulations such as General Data Protection Regulation (GDPR), traditional production mode of machine learning services is shifting to federated learning, a paradigm that allows multiple data providers to train a joint model collaboratively with their data kept locally. A key enabler for practical adoption of federated learning is how to allocate the prolit earned by the joint model to each data provider. For fair prolit allocation, a metric to quantify the contribution of each data provider to the joint model is essential. Shapley value is a classical concept in cooperative game theory which assigns a unique distribution (among the players) of a total surplus generated by the coalition of all players and has been used for data valuation in machine learning services. However, prior Shapley value based data valuation schemes either do not apply to federated learning or involve extra model training which leads to high cost. In this paper, given n data providers with data sets D1, D2, ⋯, Dn, a federated learning algorithm A and a standard test set T, we propose the contribution index, a new Shapley value based metric lit for assessing the contribution of each data provider for the joint model trained by federated learning. The contribution index shares the same properties as Shapley value. However, direct calculation of the contribution index is time consuming, since a large number of joint models with different combinations of data sets are required to be trained and evaluated. To solve this problem, we propose two gradient based methods. The idea is to reconstruct approximately the models on different combinations of the data sets through the intermediate results of the training process of federated learning so as to avoid extra training. The lirst method reconstructs models by updating the initial global model in federated learning with the gradients in different rounds. Then it calculates the contribution index by the performance of these reconstructed models. The second method calculates contribution index in each round by updating the global model in the previous round with the gradients in the current round. Contribution indexes of multiple rounds are then added with elaborated weights to get the linal result. We conduct extensive experiments on the MNIST data set in different settings. The results demonstrate that the proposed methods can approximate the exact contribution index effectively and achieve a time speed up of up to 2x-100x compared with the exact calculation and other baselines extended from existing work.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Song, Tianshu and Tong, Yongxin and Wei, Shuyue},
	month = dec,
	year = {2019},
	keywords = {Data models, Federated Learning, Google, Incentive Mechanism, Indexes, Machine learning, Servers, Shapley Value, Task analysis, Training},
	pages = {2577--2586},
}

@article{Wagner2019,
	title = {Cyber threat intelligence sharing: {Survey} and research directions},
	volume = {87},
	issn = {01674048},
	doi = {10.1016/j.cose.2019.101589},
	abstract = {Cyber Threat Intelligence (CTI) sharing has become a novel weapon in the arsenal of cyber defenders to proactively mitigate increasing cyber attacks. Automating the process of CTI sharing, and even the basic consumption, has raised new challenges for researchers and practitioners. This extensive literature survey explores the current state-of-the-art and approaches different problem areas of interest pertaining to the larger field of sharing cyber threat intelligence. The motivation for this research stems from the recent emergence of sharing cyber threat intelligence and the involved challenges of automating its processes. This work comprises a considerable amount of articles from academic and gray literature, and focuses on technical and non-technical challenges. Moreover, the findings reveal which topics were widely discussed, and hence considered relevant by the authors and cyber threat intelligence sharing communities.},
	journal = {Computers \& Security},
	author = {Wagner, Thomas D. and Mahbub, Khaled and Palomar, Esther and Abdallah, Ali E.},
	year = {2019},
	note = {Publisher: Elsevier Ltd},
	pages = {101589},
}

@article{Chaabouni2019,
	title = {Network {Intrusion} {Detection} for {IoT} {Security} {Based} on {Learning} {Techniques}},
	volume = {21},
	issn = {1553-877X},
	url = {https://ieeexplore.ieee.org/document/8629941/},
	doi = {10.1109/COMST.2019.2896380},
	abstract = {Pervasive growth of Internet of Things (IoT) is visible across the globe. The 2016 Dyn cyberattack exposed the critical fault-lines among smart networks. Security of IoT has become a critical concern. The danger exposed by infested Internet-connected Things not only affects the security of IoT but also threatens the complete Internet eco-system which can possibly exploit the vulnerable Things (smart devices) deployed as botnets. Mirai malware compromised the video surveillance devices and paralyzed Internet via distributed denial of service attacks. In the recent past, security attack vectors have evolved bothways, in terms of complexity and diversity. Hence, to identify and prevent or detect novel attacks, it is important to analyze techniques in IoT context. This survey classifies the IoT security threats and challenges for IoT networks by evaluating existing defense techniques. Our main focus is on network intrusion detection systems (NIDSs); hence, this paper reviews existing NIDS implementation tools and datasets as well as free and open-source network sniffing software. Then, it surveys, analyzes, and compares state-of-the-art NIDS proposals in the IoT context in terms of architecture, detection methodologies, validation strategies, treated threats, and algorithm deployments. The review deals with both traditional and machine learning (ML) NIDS techniques and discusses future directions. In this survey, our focus is on IoT NIDS deployed via ML since learning algorithms have a good success rate in security and privacy. The survey provides a comprehensive review of NIDSs deploying different aspects of learning techniques for IoT, unlike other top surveys targeting the traditional systems. We believe that, this paper will be useful for academia and industry research, first, to identify IoT threats and challenges, second, to implement their own NIDS and finally to propose new smart techniques in IoT context considering IoT limitations. Moreover, the survey will enable security individuals differentiate IoT NIDS from traditional ones.},
	number = {3},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Chaabouni, Nadia and Mosbah, Mohamed and Zemmari, Akka and Sauvignac, Cyrille and Faruki, Parvez},
	year = {2019},
	note = {Publisher: IEEE},
	pages = {2671--2701},
}

@article{alkhalidy_new_2022,
	title = {A {New} {Scheme} for {Detecting} {Malicious} {Nodes} in {Vehicular} {Ad} {Hoc} {Networks} {Based} on {Monitoring} {Node} {Behavior}},
	doi = {10.3390/fi14080223},
	abstract = {Vehicular ad hoc networks have played a key role in intelligent transportation systems that considerably improve road safety and management. This new technology allows vehicles to communicate and share road information. However, malicious users may inject false emergency alerts into vehicular ad hoc networks, preventing nodes from accessing accurate road information. In order to assure the reliability and trustworthiness of information through the networks, assessing the credibility of nodes has become a critical task in vehicular ad hoc networks. A new scheme for malicious node detection is proposed in this work. Multiple factors are fed into a fuzzy logic model for evaluating the trust for each node. Vehicles are divided into clusters in our approach, and a road side unit manages each cluster. The road side unit assesses the credibility of nodes before accessing vehicular ad hoc networks. The road side unit evicts a malicious node based on trust value. Simulations are used to validate our technique. We demonstrate that our scheme can detect and evict all malicious nodes in the vehicular ad hoc network over time, lowering the ratio of malicious nodes. Furthermore, it has a positive impact on selﬁsh node participation. The scheme increases the success rate of delivered data to the same level as the ideal cases when no selﬁsh node is present.},
	language = {en},
	journal = {Future Internet},
	author = {Alkhalidy, Muhsen and Al-Serhan, Atalla Fahed and Alsarhan, Ayoub and Igried, Bashar},
	year = {2022},
	pages = {11},
}

@article{deng_improving_2022,
	title = {Improving {Federated} {Learning} {With} {Quality}-{Aware} {User} {Incentive} and {Auto}-{Weighted} {Model} {Aggregation}},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2022.3195207},
	abstract = {Federated learning enables distributed model training over various computing nodes, e.g., mobile devices, where instead of sharing raw user data, computing nodes can solely commit model updates without compromising data privacy. The quality of federated learning relies on the model updates contributed by computing nodes training with their local data. However, with various factors (e.g., training data size, mislabeled data samples, skewed data distributions), the model update qualities of computing nodes can vary dramatically, while inclusively aggregating low-quality model updates can deteriorate the global model quality. To achieve efficient federated learning, in this paper, we propose a novel framework named FAIR, i.e., Federated leArning with qualIty awaReness. Particularly, FAIR integrates three major components: 1) learning quality estimation: we adopt the model aggregation weight (learned in the third component) to reversely quantify the individual learning quality of nodes in a privacy-preserving manner, and leverage the historical learning records to infer the next-round learning quality; 2) quality-aware incentive mechanism: within the recruiting budget, we model a reverse auction problem to stimulate the participation of high-quality and low-cost computing nodes, and the method is proved to be truthful, individually rational, and computationally efficient; and 3) auto-weighted model aggregation: based on the gradient descent method, we devise an auto-weighted model aggregation algorithm to automatically learn the optimal aggregation weights to further enhance the global model quality. Based on real-world datasets and learning tasks, extensive experiments are conducted to demonstrate the efficacy of FAIR.},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Deng, Yongheng and Lyu, Feng and Ren, Ju and Chen, Yi-Chao and Yang, Peng and Zhou, Yuezhi and Zhang, Yaoxue},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {Collaborative work, Computational modeling, Data models, Edge computing, Resource management, Task analysis, Training, Training data, federated learning, incentive mechanism, learning quality, mobile computing, model aggregation},
	pages = {1--15},
}

@article{tan_reputation-aware_2022,
	title = {Reputation-{Aware} {Federated} {Learning} {Client} {Selection} based on {Stochastic} {Integer} {Programming}},
	issn = {2332-7790},
	doi = {10.1109/TBDATA.2022.3191332},
	abstract = {Federated Learning(FL) has attracted wide research interest due to its potential in building machine learning models while preserving users' data privacy. However, due to the distributive nature of FL, it is vulnerable to misbehavior from participating worker nodes. Thus, it is important to select clients to participate in FL. Recent studies on FL client selection focus on the perspective of improving model training efficiency and performance, without holistically considering potential misbehavior and the cost of hiring. To bridge this gap, we propose a first-of-its-kind reputation-aware Stochastic integer programming-based FL Client Selection method (SCS). It can optimally select and compensate clients with different reputation profiles. Extensive experiments show that SCS achieves the most advantageous performance-cost trade-off compared to other existing state-of-the-art approaches.},
	journal = {IEEE Transactions on Big Data},
	author = {Tan, Xavier and Ng, Wei Chong and Lim, Wei Yang Bryan and Xiong, Zehui and Niyato, Dusit and Yu, Han},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Big Data},
	keywords = {Biological system modeling, Computational modeling, Costs, Data models, Federated learning, Stochastic processes, Training, Uncertainty, client selection, reputation, stochastic integer programming},
	pages = {1--12},
}

@inproceedings{manyadza_fl-finder_2022,
	title = {{FL}-finder: {Detecting} {Unknown} {Network} {Anomaly} in {Federated} {Learning}},
	shorttitle = {{FL}-finder},
	doi = {10.1109/ICAIBD55127.2022.9820480},
	abstract = {The emergence of federated learning has ensured data and privacy security in deep learning models while enabling models to train more efficiently. However, the transmission of network parameters in federated learning may be subject to attacks by unknown anomalies. In this paper, we attempted to detect unknown anomalies in transmitted parameters in federated learning. We designed and implemented F1-finder, an unknown network anomaly detection framework in federated learning, which detects anomalies based on incremental learning. It retains the unknown anomalies to its prior knowledge base using the network updater, and adopts an online mode that reports new anomalies in a real-time. Extensive experimental results show that our model increased the average accuracy of unknown anomaly detection by 10.4\% and the average F1-Score improved to 19\%.},
	booktitle = {2022 5th {International} {Conference} on {Artificial} {Intelligence} and {Big} {Data} ({ICAIBD})},
	author = {Manyadza, Tinashe Justice and Du, Haizhou and Wang, Shiwei and Yang, Wenbin and Chen, Cheng and Tian, Fei},
	month = may,
	year = {2022},
	keywords = {Collaborative work, Data models, Detectors, Energy consumption, Federated Learning, Incremental learning, Knowledge based systems, Learning (artificial intelligence), Prior Knowledge, Real-time systems, Unknown Anomaly Detection},
	pages = {593--597},
}

@inproceedings{briggs_federated_2020,
	title = {Federated learning with hierarchical clustering of local updates to improve training on non-{IID} data},
	doi = {10.1109/IJCNN48605.2020.9207469},
	abstract = {Federated learning (FL) is a well established method for performing machine learning tasks over massively distributed data. However in settings where data is distributed in a non-iid (not independent and identically distributed) fashion - as is typical in real world situations - the joint model produced by FL suffers in terms of test set accuracy and/or communication costs compared to training on iid data. We show that learning a single joint model is often not optimal in the presence of certain types of non-iid data. In this work we present a modification to FL by introducing a hierarchical clustering step (FL+HC) to separate clusters of clients by the similarity of their local updates to the global joint model. Once separated, the clusters are trained independently and in parallel on specialised models. We present a robust empirical analysis of the hyperparameters for FL+HC for several iid and non-iid settings. We show how FL+HC allows model training to converge in fewer communication rounds (significantly so under some non-iid settings) compared to FL without clustering. Additionally, FL+HC allows for a greater percentage of clients to reach a target accuracy compared to standard FL. Finally we make suggestions for good default hyperparameters to promote superior performing specialised models without modifying the the underlying federated learning communication protocol.},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Briggs, Christopher and Fan, Zhong and Andras, Peter},
	month = jul,
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {Cats, Clustering algorithms, Data models, Distributed databases, Merging, Task analysis, Training, clustering applications, distributed machine learning, federated learning},
	pages = {1--9},
}

@inproceedings{wang_novel_2020,
	title = {A {Novel} {Reputation}-aware {Client} {Selection} {Scheme} for {Federated} {Learning} within {Mobile} {Environments}},
	doi = {10.1109/CAMAD50429.2020.9209263},
	abstract = {This paper studies the problem of training federated deep learning models over a mobile environment. Stemming from the federated learning (FL) concept, deep learning models on mobile devices can be trained for various use cases including but not limited to image sorting and prediction of upcoming words. Mobile devices have access to rich data sets through embedded sensors and as well as installed software, and these feature rich data can facilitate solid training models, including personal images and other behaviometric features. However, utilizing the data through conventional approaches can potentially lead to privacy leakages. In this paper, we propose an alternate strategy that builds on the Federated Learning (FL) concept, to keep the training data on distributed mobile devices, and train a shared model by aggregating updated local models. The contribution of this study is an optimal user selection method for the federated learning environment based on reputation scores. Through extensive validation experiments considering two different model architectures and three datasets, our experiments show that the proposed approach is stable over data that is not independent nor identically distributed (i.e., non-IID) and under imbalanced distribution. Experimental results show that the proposed reputation-aware FL scheme can achieve improvements in the test accuracy up to 9.30\% under different data sets.},
	booktitle = {2020 {IEEE} 25th {International} {Workshop} on {Computer} {Aided} {Modeling} and {Design} of {Communication} {Links} and {Networks} ({CAMAD})},
	author = {Wang, Yuwei and Kantarci, Burak},
	month = sep,
	year = {2020},
	note = {ISSN: 2378-4873},
	keywords = {Computational modeling, Data models, Data privacy, Federated learning, Machine learning, Mobile handsets, Servers, Training, client selection, data sharing, deep learning models, mobile networks},
	pages = {1--6},
}

@article{chen_zero_2021,
	title = {Zero {Knowledge} {Clustering} {Based} {Adversarial} {Mitigation} in {Heterogeneous} {Federated} {Learning}},
	volume = {8},
	issn = {2327-4697},
	doi = {10.1109/TNSE.2020.3002796},
	abstract = {The simultaneous development of deep learning techniques and Internet of Things (IoT)/Cyber-physical Systems (CPS) technologies has afforded untold possibilities for improving distributed computing, sensing, and data analysis. Among these technologies, federated learning has received increased attention as a privacy-preserving collaborative learning paradigm, and has shown significant potential in IoT/CPS-driven large-scale smart-world systems. At the same time, the vulnerabilities of deep neural networks, especially to adversarial attacks, cannot be overstated and should not be minimized. Moreover, the distributed nature of federated learning makes defense against such adversarial attacks a more challenging problem due to the unavailability of local data and resource heterogeneity. To tackle these challenges, in this paper, we propose ZeKoC, a Zero Knowledge Clustering approach to mitigating adversarial attacks. Particularly, we first formulate the problem of resource-constrained adversarial mitigation. Specifically, noting that a global server has no access to training samples, we reformulate the unsupervised weight clustering problem. Our proposed ZeKoC approach allows the server to automatically split and merge weight clusters for weight selection and aggregation. Theoretical analysis demonstrates that convergence is guaranteed. Further, our experimental results illustrate that, in a non-i.i.d. (i.e., independent and identically distributed) data setting, the proposed ZeKoC approach successfully mitigates general attacks while outperforming state-of-art schemes.},
	number = {2},
	journal = {IEEE Transactions on Network Science and Engineering},
	author = {Chen, Zheyi and Tian, Pu and Liao, Weixian and Yu, Wei},
	month = apr,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Network Science and Engineering},
	keywords = {Data models, Distributed databases, Machine learning, Non-i.i.d. data, Peer-to-peer computing, Security, Servers, Training, adversarial mitigation, federated learning},
	pages = {1070--1083},
}

@article{ng_reputation-aware_2022,
	title = {Reputation-{Aware} {Hedonic} {Coalition} {Formation} for {Efficient} {Serverless} {Hierarchical} {Federated} {Learning}},
	volume = {33},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2021.3139039},
	abstract = {Amid growing concerns on data privacy, Federated Learning (FL) has emerged as a promising privacy preserving distributed machine learning paradigm. Given that the FL network is expected to be implemented at scale, several studies have proposed system architectures towards improving the network scalability and efficiency. Specifically, the Hierarchical FL (HFL) network utilizes cluster heads, e.g., base stations, for the intermediate aggregation and relay of model parameters. Serverless FL is also proposed recently, in which the data owners, i.e., workers, exchange the local model parameters among a neighborhood of workers. This decentralized approach reduces the risk of a single point of failure but inevitably incurs significant communication overheads. To achieve the best of both worlds, we propose the Serverless Hierarchical Federated Learning (SHFL) framework in this article. The SHFL framework adopts a two-layer system architecture. In the lower layer, the FL workers are grouped into clusters under cluster heads. In the upper layer, the cluster heads exchange the intermediate parameters with their one-hop neighbors without the aid of a central server. To improve the sustainable efficiency of the FL system while taking into account the incentive design for workers’ marginal contributions in the system, we propose the reputation-aware hedonic coalition formation game in this article. Specifically, the workers are rewarded for their marginal contribution to the cluster, whereas the reputation opinions of each cluster head is updated in a decentralized manner, thereby deterring malicious behaviors by the cluster head. This improves the performance of the network since cluster heads with higher reputation scores are more reliable in relaying the intermediate model parameters. The simulation results show that our proposed hedonic coalition formation algorithm converges to a Nash-stable partition and improves the network efficiency.},
	number = {11},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Ng, Jer Shyuan and Lim, Wei Yang Bryan and Xiong, Zehui and Cao, Xianbin and Jin, Jiangming and Niyato, Dusit and Leung, Cyril and Miao, Chunyan},
	month = nov,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {Base stations, Collaborative work, Computational modeling, Costs, Federated learning, Magnetic heads, Servers, Training, decentralized edge intelligence, hedonic coalition formation, serverless federated learning},
	pages = {2675--2686},
}

@inproceedings{xia_tofi_2021,
	address = {Cham},
	series = {Lecture {Notes} of the {Institute} for {Computer} {Sciences}, {Social} {Informatics} and {Telecommunications} {Engineering}},
	title = {{ToFi}: {An} {Algorithm} to {Defend} {Against} {Byzantine} {Attacks} in {Federated} {Learning}},
	isbn = {978-3-030-90019-9},
	shorttitle = {{ToFi}},
	doi = {10.1007/978-3-030-90019-9_12},
	abstract = {In distributed gradient descent based machine learning model training, workers periodically upload locally computed gradients or weights to the parameter server (PS). Byzantine attacks take place when some workers upload wrong gradients or weights, i.e., the information received by the PS is not always the true values computed by workers. Approaches such as score-based, median-based, and distance-based defense algorithms were proposed previously, but all of them made the asumptions: (1) the dataset on each worker is independent and identically distributed (i.i.d.), and (2) the majority of all participating workers are honest. These assumptions are not realistic in federated learning where each worker may keep its non-i.i.d. private dataset and malicious workers may take over the majority in some iterations. In this paper, we propose a novel reference dataset based algorithm along with a practical Two-Filter algorithm (ToFi) to defend against Byzantine attacks in federated learning. Our experiments highlight the effectiveness of our algorithm compared with previous algorithms in different settings.},
	language = {en},
	booktitle = {Security and {Privacy} in {Communication} {Networks}},
	publisher = {Springer International Publishing},
	author = {Xia, Qi and Tao, Zeyi and Li, Qun},
	editor = {Garcia-Alfaro, Joaquin and Li, Shujun and Poovendran, Radha and Debar, Hervé and Yung, Moti},
	year = {2021},
	keywords = {Byzantine attacks, Federated learning},
	pages = {229--248},
}

@misc{cao_fltrust_2022,
	title = {{FLTrust}: {Byzantine}-robust {Federated} {Learning} via {Trust} {Bootstrapping}},
	shorttitle = {{FLTrust}},
	url = {http://arxiv.org/abs/2012.13995},
	doi = {10.48550/arXiv.2012.13995},
	abstract = {Byzantine-robust federated learning aims to enable a service provider to learn an accurate global model when a bounded number of clients are malicious. The key idea of existing Byzantine-robust federated learning methods is that the service provider performs statistical analysis among the clients' local model updates and removes suspicious ones, before aggregating them to update the global model. However, malicious clients can still corrupt the global models in these methods via sending carefully crafted local model updates to the service provider. The fundamental reason is that there is no root of trust in existing federated learning methods. In this work, we bridge the gap via proposing FLTrust, a new federated learning method in which the service provider itself bootstraps trust. In particular, the service provider itself collects a clean small training dataset (called root dataset) for the learning task and the service provider maintains a model (called server model) based on it to bootstrap trust. In each iteration, the service provider first assigns a trust score to each local model update from the clients, where a local model update has a lower trust score if its direction deviates more from the direction of the server model update. Then, the service provider normalizes the magnitudes of the local model updates such that they lie in the same hyper-sphere as the server model update in the vector space. Our normalization limits the impact of malicious local model updates with large magnitudes. Finally, the service provider computes the average of the normalized local model updates weighted by their trust scores as a global model update, which is used to update the global model. Our extensive evaluations on six datasets from different domains show that our FLTrust is secure against both existing attacks and strong adaptive attacks.},
	urldate = {2022-08-09},
	publisher = {arXiv},
	author = {Cao, Xiaoyu and Fang, Minghong and Liu, Jia and Gong, Neil Zhenqiang},
	month = apr,
	year = {2022},
	note = {arXiv:2012.13995 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Distributed, Parallel, and Cluster Computing},
}

@inproceedings{wang_reputation-enabled_2021,
	title = {Reputation-enabled {Federated} {Learning} {Model} {Aggregation} in {Mobile} {Platforms}},
	doi = {10.1109/ICC42927.2021.9500928},
	abstract = {Federated Learning (FL) builds on a mobile network of participating nodes that train local models and contribute to the learning model parameters at a central server without being obliged to share their raw data. The server aggregates the uploaded model parameters to generate a global model. Common practice for the uploaded local models is an evenly weighted aggregation, assuming that each node of the network contributes to advancing the global model equally. Due to the heterogeneous nature of the devices and collected data, it is inevitable to have variations between the contributions of the users to the global model. Therefore, users (i.e., devices) with higher contributions should be weighted higher during aggregation. With this in mind, this paper proposes a reputation-enabled aggregation methodology that scales the aggregation weights of users by their reputation scores. Reputation score of a user is computed according to the performance metrics of their trained local models during each training round, therefore it can be a metric to evaluate the direct contributions of their trained local model. Numerical comparison of the proposed aggregation methodology to a baseline that utilizes standard averaging as well as a second baseline that is scoped to a reputation-based client selection shows an improvement of 17.175\% over the standard baseline for not independent and identically distributed (non-IID) scenarios for an FL network of 100 participants. Consistent improvements over the first and second baselines under smaller FL networks with users ranging from 20 to 100 are also shown.},
	booktitle = {{ICC} 2021 - {IEEE} {International} {Conference} on {Communications}},
	author = {Wang, Yuwei and Kantarci, Burak},
	month = jun,
	year = {2021},
	note = {ISSN: 1938-1883},
	keywords = {Collaborative work, Computational modeling, Data aggregation, Data models, Deep Learning, Deep Neural Networks, Distance measurement, Distributed Learning, Federated Learning, Mobile Networks, Neural networks, Reputation systems, Training},
	pages = {1--6},
}

@article{hu_contribution-_2022,
	title = {Contribution- and {Participation}-based {Federated} {Learning} on non-{IID} {Data}},
	issn = {1541-1672, 1941-1294},
	url = {https://ieeexplore.ieee.org/document/9760086/},
	doi = {10.1109/MIS.2022.3168298},
	abstract = {The learning process takes place inside clients in federated learning (FL). How to effectively motivate clients and avoid the impact of statistical heterogeneity are challenges in FL. This paper proposes contribution- and participationbased federated learning (CPFL) to address these challenges. CPFL can effectively allocate client incentives and aggregate models according to client contribution ratios, by which it can reduce the impact of heterogeneous data. To get effective and approximately fair client contributions faster, we propose an extended Raiffa solution (ERS). Compared to the conventional solution Shapley Value, the time complexity of ERS goes from O(2n) down to O(n). We perform extensive experiments with the MNIST/EMNIST datasets, heterogeneous datasets, and with different ratios of participation reward. Experimental results demonstrate that CPFL generally has a better learning effect in the heterogeneous case.},
	language = {en},
	urldate = {2022-07-05},
	journal = {IEEE Intelligent Systems},
	author = {Hu, Fei and Zhou, Wuneng and Liao, Kaili and Li, Hongliang},
	year = {2022},
	pages = {1--1},
}

@inproceedings{wang_flare_2022,
	address = {Nagasaki Japan},
	title = {{FLARE}: {Defending} {Federated} {Learning} against {Model} {Poisoning} {Attacks} via {Latent} {Space} {Representations}},
	isbn = {978-1-4503-9140-5},
	shorttitle = {{FLARE}},
	url = {https://dl.acm.org/doi/10.1145/3488932.3517395},
	doi = {10.1145/3488932.3517395},
	abstract = {Federated learning (FL) has been shown vulnerable to a new class of adversarial attacks, known as model poisoning attacks (MPA), where one or more malicious clients try to poison the global model by sending carefully crafted local model updates to the central parameter server. Existing defenses that have been fixated on analyzing model parameters show limited effectiveness in detecting such carefully crafted poisonous models. In this work, we propose FLARE, a robust model aggregation mechanism for FL, which is resilient against state-of-the-art MPAs. Instead of solely depending on model parameters, FLARE leverages the penultimate layer representations (PLRs) of the model for characterizing the adversarial influence on each local model update. PLRs demonstrate a better capability to differentiate malicious models from benign ones than model parameter-based solutions. We further propose a trust evaluation method that estimates a trust score for each model update based on pairwise PLR discrepancies among all model updates. Under the assumption that honest clients make up the majority, FLARE assigns a trust score to each model update in a way that those far from the benign cluster are assigned low scores. FLARE then aggregates the model updates weighted by their trust scores and finally updates the global model. Extensive experimental results demonstrate the effectiveness of FLARE in defending FL against various MPAs, including semantic backdoor attacks, trojan backdoor attacks, and untargeted attacks, and safeguarding the accuracy of FL.},
	language = {en},
	urldate = {2022-07-05},
	booktitle = {Proceedings of the 2022 {ACM} on {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Wang, Ning and Xiao, Yang and Chen, Yimin and Hu, Yang and Lou, Wenjing and Hou, Y. Thomas},
	month = may,
	year = {2022},
	pages = {946--958},
}

@article{kang_incentive_2019,
	title = {Incentive {Mechanism} for {Reliable} {Federated} {Learning}: {A} {Joint} {Optimization} {Approach} to {Combining} {Reputation} and {Contract} {Theory}},
	volume = {6},
	issn = {2327-4662},
	shorttitle = {Incentive {Mechanism} for {Reliable} {Federated} {Learning}},
	doi = {10.1109/JIOT.2019.2940820},
	abstract = {Federated learning is an emerging machine learning technique that enables distributed model training using local datasets from large-scale nodes, e.g., mobile devices, but shares only model updates without uploading the raw training data. This technique provides a promising privacy preservation for mobile devices while simultaneously ensuring high learning performance. The majority of existing work has focused on designing advanced learning algorithms with an aim to achieve better learning performance. However, the challenges, such as incentive mechanisms for participating in training and worker (i.e., mobile devices) selection schemes for reliable federated learning, have not been explored yet. These challenges have hindered the widespread adoption of federated learning. To address the above challenges, in this article, we first introduce reputation as the metric to measure the reliability and trustworthiness of the mobile devices. We then design a reputation-based worker selection scheme for reliable federated learning by using a multiweight subjective logic model. We also leverage the blockchain to achieve secure reputation management for workers with nonrepudiation and tamper-resistance properties in a decentralized manner. Moreover, we propose an effective incentive mechanism combining reputation with contract theory to motivate high-reputation mobile devices with high-quality data to participate in model learning. Numerical results clearly indicate that the proposed schemes are efficient for reliable federated learning in terms of significantly improving the learning accuracy.},
	number = {6},
	journal = {IEEE Internet of Things Journal},
	author = {Kang, Jiawen and Xiong, Zehui and Niyato, Dusit and Xie, Shengli and Zhang, Junshan},
	month = dec,
	year = {2019},
	keywords = {Blockchain, Contracts, Data models, Mobile handsets, Reliability, Task analysis, Training, contract theory, federated learning, mobile networks, reputation, security and privacy},
	pages = {10700--10714},
}

@article{kang_reliable_2020,
	title = {Reliable {Federated} {Learning} for {Mobile} {Networks}},
	volume = {27},
	issn = {1558-0687},
	doi = {10.1109/MWC.001.1900119},
	abstract = {Federated learning, as a promising machine learning approach, has emerged to leverage a distributed personalized dataset from a number of nodes, for example, mobile devices, to improve performance while simultaneously providing privacy preservation for mobile users. In federated learning, training data is widely distributed and maintained on the mobile devices as workers. A central aggregator updates a global model by collecting local updates from mobile devices using their local training data to train the global model in each iteration. However, unreliable data may be uploaded by the mobile devices (i.e., workers), leading to frauds in tasks of federated learning. The workers may perform unreliable updates intentionally, for example, the data poisoning attack, or unintentionally, for example, low-quality data caused by energy constraints or high-speed mobility. Therefore, finding out trusted and reliable workers in federated learning tasks becomes critical. In this article, the concept of reputation is introduced as a metric. Based on this metric, a reliable worker selection scheme is proposed for federated learning tasks. Consortium blockchain is leveraged as a decentralized approach for achieving efficient reputation management of the workers without repudiation and tampering. By numerical analysis, the proposed approach is demonstrated to improve the reliability of federated learning tasks in mobile networks.},
	number = {2},
	journal = {IEEE Wireless Communications},
	author = {Kang, Jiawen and Xiong, Zehui and Niyato, Dusit and Zou, Yuze and Zhang, Yang and Guizani, Mohsen},
	month = apr,
	year = {2020},
	note = {Conference Name: IEEE Wireless Communications},
	keywords = {Data models, Data privacy, Machine learning, Metasearch, Mobile handsets, Task analysis, Training data, \_obsidian},
	pages = {72--80},
}

@article{skopik_problem_2016,
	title = {A problem shared is a problem halved: {A} survey on the dimensions of collective cyber defense through security information sharing},
	volume = {60},
	issn = {01674048},
	doi = {10.1016/j.cose.2016.04.003},
	abstract = {The Internet threat landscape is fundamentally changing. A major shift away from hobby hacking toward well-organized cyber crime can be observed. These attacks are typically carried out for commercial reasons in a sophisticated and targeted manner, and specifically in a way to circumvent common security measures. Additionally, networks have grown to a scale and complexity, and have reached a degree of interconnectedness, that their protection can often only be guaranteed and financed as shared efforts. Consequently, new paradigms are required for detecting contemporary attacks and mitigating their effects. Today, many attack detection tasks are performed within individual organizations, and there is little cross-organizational information sharing. However, information sharing is a crucial step to acquiring a thorough understanding of large-scale cyber-attack situations, and is therefore seen as one of the key concepts to protect future networks. Discovering covert cyber attacks and new malware, issuing early warnings, advice about how to secure networks, and selectively distribute threat intelligence data are just some of the many use cases. In this survey article we provide a structured overview about the dimensions of cyber security information sharing. First, we motivate the need in more detail and work out the requirements for an information sharing system. Second, we highlight legal aspects and efforts from standardization bodies such as ISO and the National Institute of Standards and Technology (NIST). Third, we survey implementations in terms of both organizational and technological matters. In this regard, we study the structures of Computer Emergency Response Teams (CERTs) and Computer Security Incident Response Teams (CSIRTs), and evaluate what we could learn from them in terms of applied processes, available protocols and implemented tools. We conclude with a critical review of the state of the art and highlight important considerations when building effective security information sharing platforms for the future.},
	journal = {Computers \& Security},
	author = {Skopik, Florian and Settanni, Giuseppe and Fiedler, Roman},
	year = {2016},
	note = {Publisher: Elsevier Ltd},
	pages = {154--176},
}

@inproceedings{ur_rehman_towards_2020,
	address = {Toronto, ON, Canada},
	title = {Towards {Blockchain}-{Based} {Reputation}-{Aware} {Federated} {Learning}},
	isbn = {978-1-72818-695-5},
	url = {https://ieeexplore.ieee.org/document/9163027/},
	doi = {10.1109/INFOCOMWKSHPS50562.2020.9163027},
	abstract = {Federated learning (FL) is the collaborative machine learning (ML) technique whereby the devices collectively train and update a shared ML model while preserving their personal datasets. FL systems solve the problems of communicationefﬁciency, bandwidth-optimization, and privacy-preservation. Despite the potential beneﬁts of FL, one centralized shared ML model across all the devices produce coarse-grained predictions which, in essence, are not required in many application areas involving personalized prediction services. In this paper, we present a novel concept of ﬁne-grained FL to decentralize the shared ML models on the edge servers. We then present a formal extended deﬁnition of ﬁne-grained FL process in mobile edge computing systems. In addition, we deﬁne the core requirements of ﬁnegrained FL systems including personalization, decentralization, ﬁne-grained FL, incentive mechanisms, trust, activity monitoring, heterogeneity and context-awareness, model synchronization, and communication and bandwidth-efﬁciency. Moreover, we present the concept of blockchain-based reputation-aware ﬁne-grained FL in order to ensure trustworthy collaborative training in mobile edge computing systems. Finally, we perform the qualitative comparison of proposed approach with state-of-the-art related work and found some promising initial results.},
	language = {en},
	urldate = {2021-10-29},
	booktitle = {{IEEE} {INFOCOM} 2020 - {IEEE} {Conference} on {Computer} {Communications} {Workshops} ({INFOCOM} {WKSHPS})},
	publisher = {IEEE},
	author = {ur Rehman, Muhammad Habib and Salah, Khaled and Damiani, Ernesto and Svetinovic, Davor},
	month = jul,
	year = {2020},
	keywords = {\_read},
	pages = {183--188},
}

@techreport{enisa_incentives_2010,
	title = {Incentives and {Challenges} for {Information} {Sharing} in the {Context} of {Network} and {Information} {Security}},
	url = {http://www.google.com/#sclient=psy&hl=en&safe=off&q=literature+review+information+sharing+law+enforcement&aq=f&aqi=&aql=&oq=&gs_rfai=&pbx=1&fp=9bef8cda26d1a6ec},
	abstract = {The importance of information sharing to ensuring network and information security is widely acknowledged by both policy-makers and by the technical and practitioner community – for example, in the European Programme on Critical Infrastructure Protection (EPCIP) and in the 2004 Availability and Robustness of Electronic Communications Infrastructures (ARECI) study, which noted that formal means for sharing information should be set up in order to ―improve the protection and rapid restoration of infrastructure critical to the reliability of communications within and throughout Europe‖. A 2009 gap analysis conducted by ENISA of good practice in respect of telecommunication network operators identified information sharing as a set of useful best practice. Given the acknowledged importance of information sharing, this report sets out findings from a research project into the barriers to and incentives for information sharing in the field of network and information security, in the context of peer-to-peer groups such as Information Exchanges (IE) and Information Sharing Analysis Centres (ISACs).{\textbackslash}nMethods and approach The information in this report is drawn from three sources: {\textbackslash}n A review of available literature – both academic and non-academic publications,  Interviews with key informants working in the field of network and information {\textbackslash}nsecurity and in IEs,  A two-round Delphi exercise with network and information security professionals. {\textbackslash}n The aim of this project is to identify those barriers and incentives which are most important in day-to-day practice in IEs and ISACs. This research differs from other work in this field in being firmly grounded in the experiences of practitioners and those involved in IE and Information Sharing activities. Nonetheless we only managed to speak to a limited number of experts from a handful of countries. Therefore, the findings of this research are a first step to developing an evidence base in this field, but we do not claim they are generalisable to all kinds of IEs. {\textbackslash}nIncentives and challenges for information sharing Our findings indicate that many of the barriers and incentives commonly identified in the {\textbackslash}navailable literature are of relatively low importance to practitioners and security officials currently working in IEs. As part of this research we asked practitioners to rank a list of barriers and incentives in terms of their relative importance. Our findings indicate that the incentives which are most important are: {\textbackslash}n Economic incentives stemming from cost savings;  Incentives stemming from the quality, value, and use of information shared. {\textbackslash}n While the barriers which are the most important are: {\textbackslash}n Poor quality information;  Misaligned economic incentives stemming from reputational risks;  Poor management.},
	author = {{ENISA}},
	year = {2010},
	note = {Volume: 10},
	pages = {52},
}

@inproceedings{putra_decentralised_2021,
	title = {Decentralised {Trustworthy} {Collaborative} {Intrusion} {Detection} {System} for {IoT}},
	doi = {10.1109/Blockchain53845.2021.00048},
	abstract = {Intrusion Detection Systems (IDS) have been the industry standard for securing IoT networks against known attacks. To increase the capability of an IDS, researchers proposed the concept of blockchain-based Collaborative-IDS (CIDS), wherein blockchain acts as a decentralised platform allowing collaboration between CIDS nodes to share intrusion related information, such as intrusion alarms and detection rules. However, proposals in blockchain-based CIDS overlook the importance of continuous evaluation of the trustworthiness of each node and generally work based on the assumption that the nodes are always honest. In this paper, we propose a decentralised CIDS that emphasises the importance of building trust between CIDS nodes. In our proposed solution, each CIDS node exchanges detection rules to help other nodes detect new types of intrusion. Our architecture offloads the trust computation to the blockchain and utilises a decentralised storage to host the shared trustworthy detection rules, ensuring scalability. Our implementation in a lab-scale testbed shows that the our solution is feasible and performs within the expected benchmarks of the Ethereum platform.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Blockchain} ({Blockchain})},
	author = {Putra, Guntur Dharma and Dedeoglu, Volkan and Pathak, Abhinav and Kanhere, Salil S. and Jurdak, Raja},
	month = dec,
	year = {2021},
	keywords = {Benchmark testing, Blockchains, Collaboration, Computer architecture, Economics, Intrusion detection, IoT, Scalability, blockchain, collaborative, intrusion detection system, scalability, trust management},
	pages = {306--313},
}

@article{cordero_sphinx_2018,
	title = {Sphinx: a {Colluder}-{Resistant} {Trust} {Mechanism} for {Collaborative} {Intrusion} {Detection}},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Sphinx},
	doi = {10.1109/ACCESS.2018.2880297},
	abstract = {The destructive effects of cyber-attacks demand more proactive security approaches. One such promising approach is the idea of collaborative intrusion detection systems (CIDSs). These systems combine the knowledge of multiple sensors (e.g., intrusion detection systems, honeypots, or firewalls) to create a holistic picture of a monitored network. Sensors monitor parts of a network and exchange alert data to learn from each other, improve their detection capabilities and ultimately identify sophisticated attacks. Nevertheless, if one or a group of sensors is unreliable (due to incompetence or malice), the system might miss important information needed to detect attacks. In this paper, we propose Sphinx, an evidence-based trust mechanism capable of detecting unreliable sensors within a CIDS. The Sphinx detects, both, single sensors or coalitions of dishonest sensors that lie about the reliability of others to boost or worsen their trust score. Our evaluation shows that, given an honest majority of sensors, dishonesty is punished in a timely manner. Moreover, if several coalitions exist, even when more than 50\% of all sensors are dishonest, dishonesty is punished.},
	journal = {IEEE Access},
	author = {Cordero, Carlos Garcia and Traverso, Giulia and Nojoumian, Mehrdad and Habib, Sheikh Mahbub and Mühlhäuser, Max and Buchmann, Johannes and Vasilomanolakis, Emmanouil},
	year = {2018},
	note = {Conference Name: IEEE Access},
	keywords = {Clustering, Collaboration, Electronic mail, Intrusion detection, Machine learning, Monitoring, Reliability, Sensors, collaborative intrusion detection, machine learning, mixture models, sensor reliability, trust management},
	pages = {72427--72438},
}

@article{gil_perez_repcidn_2013,
	title = {{RepCIDN}: {A} {Reputation}-based {Collaborative} {Intrusion} {Detection} {Network} to {Lessen} the {Impact} of {Malicious} {Alarms}},
	volume = {21},
	issn = {1573-7705},
	shorttitle = {{RepCIDN}},
	url = {https://doi.org/10.1007/s10922-012-9230-8},
	doi = {10.1007/s10922-012-9230-8},
	abstract = {Distributed and coordinated attacks in computer networks are causing considerable economic losses worldwide in recent years. This is mainly due to the transition of attackers’ operational patterns towards a more sophisticated and more global behavior. This fact is leading current intrusion detection systems to be more likely to generate false alarms. In this context, this paper describes the design of a collaborative intrusion detection network (CIDN) that is capable of building and sharing collective knowledge about isolated alarms in order to efficiently and accurately detect distributed attacks. It has been also strengthened with a reputation mechanism aimed to improve the detection coverage by dropping false or bogus alarms that arise from malicious or misbehaving nodes. This model will enable a CIDN to detect malicious behaviors according to the trustworthiness of the alarm issuers, calculated from previous interactions with the system. Experimental results will finally demonstrate how entities are gradually isolated as their behavior worsens throughout the time.},
	language = {en},
	number = {1},
	urldate = {2022-07-07},
	journal = {Journal of Network and Systems Management},
	author = {Gil Pérez, Manuel and Gómez Mármol, Félix and Martínez Pérez, Gregorio and Skarmeta Gómez, Antonio F.},
	month = mar,
	year = {2013},
	keywords = {Collaboration networks, Group reputation, Intrusion detection systems, Reputation systems, Security, Trust management},
	pages = {128--167},
}

@article{fung_facid_2016,
	title = {{FACID}: {A} trust-based collaborative decision framework for intrusion detection networks},
	volume = {53},
	issn = {1570-8705},
	shorttitle = {{FACID}},
	url = {https://www.sciencedirect.com/science/article/pii/S1570870516302062},
	doi = {10.1016/j.adhoc.2016.08.014},
	abstract = {Computer systems evolve to be more complex and vulnerable. Cyber attacks have also grown to be more sophisticated and harder to detect. Intrusion detection is the process of monitoring and identifying unauthorized system access or manipulation. It becomes increasingly difficult for a single intrusion detection system (IDS) to detect all attacks due to limited knowledge about attacks. Collaboration among intrusion detection devices can be used to gain higher detection accuracy and cost efficiency as compared to its traditional single host-based counterpart. Through cooperation, a local IDS can detect new attacks that may be known to other IDSs, which may be from different vendors. However, how to utilize the diagnosis from different IDSs to perform intrusion detection is the key challenge. This paper proposes a system architecture of a collaborative intrusion detection network (CIDN), in which trustworthy and efficient feedback aggregation is a key component. To achieve a reliable and trustworthy CIDN, we present a framework called FACID, which leverages data analytical models and hypothesis testing methods for efficient, distributed and sequential feedback aggregations. FACID provides an inherent trust evaluation mechanism and reduces communication overhead needed for IDSs as well as the computational resources and memory needed to achieve satisfactory feedback aggregation results when the number of collaborators of an IDS is large. Our simulation results corroborate our theoretical results and demonstrate the properties of cost efficiency and accuracy compared to other heuristic methods. The analytical result on the lower-bound of the average number of acquaintances for consultation is essential for the design and configuration of IDSs in a collaborative environment.},
	language = {en},
	urldate = {2022-07-07},
	journal = {Ad Hoc Networks},
	author = {Fung, Carol J. and Zhu, Quanyan},
	month = dec,
	year = {2016},
	keywords = {Cooperative networks, Distributed algorithms, Intrusion detection networks, Resource allocations},
	pages = {17--31},
}

@inproceedings{alexopoulos_towards_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Towards {Blockchain}-{Based} {Collaborative} {Intrusion} {Detection} {Systems}},
	isbn = {978-3-319-99843-5},
	doi = {10.1007/978-3-319-99843-5_10},
	abstract = {In an attempt to cope with the increased number of cyber-attacks, research in Intrusion Detection System IDSs is moving towards more collaborative mechanisms. Collaborative IDSs (CIDSs) are such an approach; they combine the knowledge of a plethora of monitors to generate a holistic picture of the monitored network. Despite the research done in this field, CIDSs still face a number of fundamental challenges, especially regarding maintaining trust among the collaborating parties. Recent advances in distributed ledger technologies, e.g. various implementations of blockchain protocols, are a good fit to the problem of enhancing trust in collaborative environments. This paper touches the intersection of CIDSs and blockchains. Particularly, it introduces the idea of utilizing blockchain technologies as a mechanism for improving CIDSs. We argue that certain properties of blockchains can be of significant benefit for CIDSs; namely for the improvement of trust between monitors, and for providing accountability and consensus. For this, we study the related work and highlight the research gaps and challenges towards such a task. Finally, we propose a generic architecture for the incorporation of blockchains into the field of CIDSs and an analysis of the design decisions that need to be made to implement such an architecture.},
	language = {en},
	booktitle = {Critical {Information} {Infrastructures} {Security}},
	publisher = {Springer International Publishing},
	author = {Alexopoulos, Nikolaos and Vasilomanolakis, Emmanouil and Ivánkó, Natália Réka and Mühlhäuser, Max},
	editor = {D'Agostino, Gregorio and Scala, Antonio},
	year = {2018},
	pages = {107--118},
}

@book{haji_mirzaee_chfl_2022,
	title = {{CHFL}: {A} {Collaborative} {Hierarchical} {Federated} {Intrusion} {Detection} {System} for {Vehicular} {Networks}},
	shorttitle = {{CHFL}},
	abstract = {Wireless interfaces, remote control schemes, and increased autonomy have raised the attacks surface of vehicular networks. As powerful monitoring entities, intrusion detection systems (IDS) must be updated and customised to respond to emerging networks' requirements. As server-based monitoring schemes were prone to significant privacy concerns, new privacy constrained learning methods such as federated learning (FL) have received considerable attention in designing IDS. However, to alleviate the efficiency and enhance the scalability of the original FL, this paper proposes a novel collaborative hierarchical federated IDS, named CHFL for the vehicular network. In the CHFL model, a group of vehicles assisted by vehicle-to-everything (V2X) communication technologies can exchange intrusion detection information collaboratively in a private format. Each group nominates a leader, and the leading vehicle serves as the intermediate in the second level detection system of the hierarchical federated model. The leader communicates directly with the server to transmit and receive model updates of its nearby end vehicles. By reducing the number of direct communications to the server, our proposed system reduces network uplink traffic and queuing-processing latency. In addition, CHFL improved the prediction loss and the accuracy of the whole system. We are achieving an accuracy of 99.10\% compared with 97.01\% accuracy of the original FL.},
	author = {Haji Mirzaee, Parya and Shojafar, Mohammad and Cruickshank, Haitham and Tafazolli, Rahim},
	month = apr,
	year = {2022},
}

@inproceedings{short_using_2020,
	title = {Using {Blockchain} {Technologies} to {Improve} {Security} in {Federated} {Learning} {Systems}},
	doi = {10.1109/COMPSAC48688.2020.00-96},
	abstract = {The potential of Federated Learning (FL) deployment increases rapidly as the number of connected devices increases, the value of artificial intelligence is recognized and networking technologies and edge computing evolves. However, as in any distributed system, a set of security issues arise in FL systems. In this paper, we discuss the use of blockchain technology to address diverse security aspects of FL systems and focus on the model poisoning attack for which we propose a novel Blockchain-based defense scheme. An assessment using data from the MNIST database has shown that the proposed approach, which has been designed to be implemented on blockchain technology, offers significant protection against adversaries attempting model poisoning attacks. The approach adopts a novel algorithm for evaluating the model updates, by verifying each model update separately against a verification dataset, without requiring information about the training dataset size, which is often unavailable or easily falsified.},
	booktitle = {2020 {IEEE} 44th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Short, Andrew Ronald and Leligou, Helen C. and Papoutsidakis, Michael and Theocharis, Efstathios},
	month = jul,
	year = {2020},
	note = {ISSN: 0730-3157},
	keywords = {Blockchain, Federated Learning, Security attacks, Computers, Conferences, Software},
	pages = {1183--1188},
}

@article{fung_dirichlet-based_2011,
	title = {Dirichlet-{Based} {Trust} {Management} for {Effective} {Collaborative} {Intrusion} {Detection} {Networks}},
	volume = {8},
	issn = {1932-4537},
	doi = {10.1109/TNSM.2011.050311.100028},
	abstract = {The accuracy of detecting intrusions within a Collaborative Intrusion Detection Network (CIDN) depends on the efficiency of collaboration between peer Intrusion Detection Systems (IDSes) as well as the security itself of the CIDN. In this paper, we propose Dirichlet-based trust management to measure the level of trust among IDSes according to their mutual experience. An acquaintance management algorithm is also proposed to allow each IDS to manage its acquaintances according to their trustworthiness. Our approach achieves strong scalability properties and is robust against common insider threats, resulting in an effective CIDN. We evaluate our approach based on a simulated CIDN, demonstrating its improved robustness, efficiency and scalability for collaborative intrusion detection in comparison with other existing models.},
	number = {2},
	journal = {IEEE Transactions on Network and Service Management},
	author = {Fung, Carol J and Zhang, Jie and Aib, Issam and Boutaba, Raouf},
	month = jun,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Network and Service Management},
	keywords = {Collaboration, Collaborative intrusion detection system, Equations, Intrusion detection, Mathematical model, Peer to peer computing, Robustness, Scalability, admission control, computer security, security management, trust management},
	pages = {79--91},
}

@article{bougueroua_survey_2021,
	title = {A {Survey} on {Multi}-{Agent} {Based} {Collaborative} {Intrusion} {Detection} {Systems}},
	volume = {11},
	doi = {10.2478/jaiscr-2021-0008},
	abstract = {Multi-Agent Systems (MAS) have been widely used in many areas like modeling and simulation of complex phenomena, and distributed problem solving. Likewise, MAS have been used in cyber-security, to build more efficient Intrusion Detection Systems (IDS), namely Collaborative Intrusion Detection Systems (CIDS). This work presents a taxonomy for classifying the methods used to design intrusion detection systems, and how such methods were used alongside with MAS in order to build IDS that are deployed in distributed environments, resulting in the emergence of CIDS. The proposed taxonomy, consists of three parts: 1) general architecture of CIDS, 2) the used agent technology, and 3) decision techniques, in which used technologies are presented. The proposed taxonomy reviews and classifies the most relevant works in this topic and highlights open research issues in view of recent and emerging threats. Thus, this work provides a good insight regarding past, current, and future solutions for CIDS, and helps both researchers and professionals design more effective solutions.},
	journal = {Journal of Artificial Intelligence and Soft Computing Research},
	author = {Bougueroua, Nassima and Mazouzi, Smaine and Belaoued, Mohamed and Seddari, Noureddine and Derhab, Abdelouahid and Bouras, Abdelghani},
	month = apr,
	year = {2021},
	pages = {111--142},
}

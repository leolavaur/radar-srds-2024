
@misc{layeghy_generalisability_2022,
	title = {On {Generalisability} of {Machine} {Learning}-based {Network} {Intrusion} {Detection} {Systems}},
	url = {http://arxiv.org/abs/2205.04112},
	abstract = {Many of the proposed machine learning (ML) based network intrusion detection systems (NIDSs) achieve near perfect detection performance when evaluated on synthetic benchmark datasets. Though, there is no record of if and how these results generalise to other network scenarios, in particular to real-world networks. In this paper, we investigate the generalisability property of ML-based NIDSs by extensively evaluating seven supervised and unsupervised learning models on four recently published benchmark NIDS datasets. Our investigation indicates that none of the considered models is able to generalise over all studied datasets. Interestingly, our results also indicate that the generalisability has a high degree of asymmetry, i.e., swapping the source and target domains can signiﬁcantly change the classiﬁcation performance. Our investigation also indicates that overall, unsupervised learning methods generalise better than supervised learning models in our considered scenarios. Using SHAP values to explain these results indicates that the lack of generalisability is mainly due to the presence of strong correspondence between the values of one or more features and Attack/Benign classes in one datasetmodel combination and its absence in other datasets that have different feature distributions.},
	language = {en},
	urldate = {2023-03-23},
	publisher = {arXiv},
	author = {Layeghy, Siamak and Portmann, Marius},
	month = may,
	year = {2022},
	note = {arXiv:2205.04112 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture},
}

@misc{CSE-CIC-IDS-2018_url,
	title = {{IDS} 2018},
	url = {https://www.unb.ca/cic/datasets/ids-2018.html},
	abstract = {A collaboration between CSE and CIC. The dataset includes 7 attack scenarios: Brute-force, Heartbleed, Botnet, DoS, DDoS, Web attacks and inside infiltration.},
	language = {en},
	urldate = {2023-03-22},
}

@misc{ToN_IoT_url,
	title = {The {TON}\_IoT {Datasets}},
	url = {https://research.unsw.edu.au/projects/toniot-datasets},
	urldate = {2023-03-22},
}

@misc{Bot-IoT_url,
	title = {The {Bot}-{IoT} {Dataset}},
	url = {https://research.unsw.edu.au/projects/bot-iot-dataset},
	urldate = {2023-03-22},
}

@misc{UNSW-NB15_url,
	title = {The {UNSW}-{NB15} {Dataset}},
	url = {https://research.unsw.edu.au/projects/unsw-nb15-dataset},
	urldate = {2023-03-22},
}

@article{koroniotis_towards_2019,
	title = {Towards the development of realistic botnet dataset in the {Internet} of {Things} for network forensic analytics: {Bot}-{IoT} dataset},
	volume = {100},
	issn = {0167-739X},
	shorttitle = {Towards the development of realistic botnet dataset in the {Internet} of {Things} for network forensic analytics},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X18327687},
	doi = {10.1016/j.future.2019.05.041},
	abstract = {The proliferation of IoT systems, has seen them targeted by malicious third parties. To address this challenge, realistic protection and investigation countermeasures, such as network intrusion detection and network forensic systems, need to be effectively developed. For this purpose, a well-structured and representative dataset is paramount for training and validating the credibility of the systems. Although there are several network datasets, in most cases, not much information is given about the Botnet scenarios that were used. This paper proposes a new dataset, so-called Bot-IoT, which incorporates legitimate and simulated IoT network traffic, along with various types of attacks. We also present a realistic testbed environment for addressing the existing dataset drawbacks of capturing complete network information, accurate labeling, as well as recent and complex attack diversity. Finally, we evaluate the reliability of the BoT-IoT dataset using different statistical and machine learning methods for forensics purposes compared with the benchmark datasets. This work provides the baseline for allowing botnet identification across IoT-specific networks. The Bot-IoT dataset can be accessed at Bot-iot (2018) [1].},
	language = {en},
	urldate = {2023-03-22},
	journal = {Future Generation Computer Systems},
	author = {Koroniotis, Nickolaos and Moustafa, Nour and Sitnikova, Elena and Turnbull, Benjamin},
	month = nov,
	year = {2019},
	keywords = {Bot-IoT dataset, Forensics analytics, Network flow, Network forensics},
	pages = {779--796},
}

@article{shafiq_selection_2020,
	title = {Selection of effective machine learning algorithm and {Bot}-{IoT} attacks traffic identification for internet of things in smart city},
	volume = {107},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X19334880},
	doi = {10.1016/j.future.2020.02.017},
	abstract = {Identifying cyber attacks traffic is very important for the Internet of things (IoT) security in smart city. Recently, the research community in the field of IoT Security endeavor hard to build anomaly, intrusion and cyber attacks traffic identification model using Machine Learning (ML) algorithms for IoT security analysis. However, the critical and significant problem still not studied in depth that is how to select an effective ML algorithm when there are numbers of ML algorithms for cyber attacks detection system for IoT security. In this paper, we proposed a new framework model and a hybrid algorithm to solve this problem. Firstly BoT-IoT identification dataset is applied and its 44 effective features are selected from a number of features for the machine learning algorithm. Then five effective machine learning algorithm is selected for the identification of malicious and anomaly traffic identification and also select the most widely ML algorithm performance evaluation metrics. To find out which ML algorithm is effective and should be used to select for IoT anomaly and intrusion traffic identification, a bijective soft set approach and its algorithm is applied. Then we applied the proposed algorithm based on bijective soft set approach. Our experimental results show that the proposed model with the algorithm is effective for the selection ML algorithm out of numbers of ML algorithms.},
	language = {en},
	urldate = {2023-03-22},
	journal = {Future Generation Computer Systems},
	author = {Shafiq, Muhammad and Tian, Zhihong and Sun, Yanbin and Du, Xiaojiang and Guizani, Mohsen},
	month = jun,
	year = {2020},
	keywords = {Bot-IoT attacks, Identification, IoT, Machine learning, Selection, Smart city},
	pages = {433--442},
}

@inproceedings{sharafaldin_toward_2018,
	address = {Funchal, Madeira, Portugal},
	title = {Toward {Generating} a {New} {Intrusion} {Detection} {Dataset} and {Intrusion} {Traffic} {Characterization}},
	isbn = {978-989-758-282-0},
	shorttitle = {Toward {Generating} a {New} {Intrusion} {Detection} {Dataset} and {Intrusion} {Traffic} {Characterization}},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006639801080116},
	doi = {10.5220/0006639801080116},
	abstract = {Intrusion Detection, IDS Dataset, DoS, Web Attack, Inﬁltration, Brute Force.},
	language = {en},
	urldate = {2023-03-22},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Information} {Systems} {Security} and {Privacy}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Sharafaldin, Iman and Habibi Lashkari, Arash and Ghorbani, Ali A.},
	year = {2018},
	pages = {108--116},
}

@inproceedings{moustafa_federatedtoniot_2020,
	title = {Federated {TON}\_IoT {Windows} {Datasets} for {Evaluating} {AI}-{Based} {Security} {Applications}},
	doi = {10.1109/TrustCom50675.2020.00114},
	abstract = {Existing cyber security solutions have been basically developed using knowledge-based models that often cannot trigger new cyber-attack families. With the boom of Artificial Intelligence (AI), especially Deep Learning (DL) algorithms, those security solutions have been plugged-in with AI models to discover, trace, mitigate or respond to incidents of new security events. The algorithms demand a large number of heterogeneous data sources to train and validate new security systems. This paper presents the description of new datasets, the so-called ToN\_IoT, which involve federated data sources collected from Telemetry datasets of IoT services, Operating system datasets of Windows and Linux, and datasets of Network traffic. The paper introduces the testbed and description of TON\_IoT datasets for Windows operating systems. The testbed was implemented in three layers: edge, fog and cloud. The edge layer involves IoT and network devices, the fog layer contains virtual machines and gateways, and the cloud layer involves cloud services, such as data analytics, linked to the other two layers. These layers were dynamically managed using the platforms of software-Defined Network (SDN) and Network-Function Virtualization (NFV) using the VMware NSX and vCloud NFV platform. The Windows datasets were collected from audit traces of memories, processors, networks, processes and hard disks. The datasets would be used to evaluate various AI-based cyber security solutions, including intrusion detection, threat intelligence and hunting, privacy preservation and digital forensics. This is because the datasets have a wide range of recent normal and attack features and observations, as well as authentic ground truth events. The datasets can be publicly accessed from this link [1].},
	booktitle = {2020 {IEEE} 19th {International} {Conference} on {Trust}, {Security} and {Privacy} in {Computing} and {Communications} ({TrustCom})},
	author = {Moustafa, Nour and Keshky, Marwa and Debiez, Essam and Janicke, Helge},
	month = dec,
	year = {2020},
	note = {ISSN: 2324-9013},
	keywords = {Cloud computing, Computer crime, Data privacy, Federated datasets, AI-based security applications, testbed, Windows operating systems, intrusion detection, Internet of Things, Operating systems, Security, Virtual machining},
	pages = {848--855},
}

@article{Jatti2019,
	title = {{UNSW}-{NB15}: {A} {Comprehensive} {Data} set for {Network} {Intrusion} {Detection} {Systems}},
	volume = {8},
	issn = {2277-3878},
	doi = {10.35940/ijrte.B1540.0982S1119},
	abstract = {One of the major research challenges in this field is the unavailability of a comprehensive network based data set which can reflect modern network traffic scenarios, vast varieties of low footprint intrusions and depth structured information about the network traffic. Evaluating network intrusion detection systems research efforts, KDD98, KDDCUP99 and NSLKDD benchmark data sets were generated a decade ago. However, numerous current studies showed that for the current network threat environment, these data sets do not inclusively reflect network traffic and modern low footprint attacks. Countering the unavailability of network benchmark data set challenges, this paper examines a UNSW-NB15 data set creation. This data set has a hybrid of the real modern normal and the contemporary synthesized attack activities of the network traffic. Existing and novel methods are utilised to generate the features of the UNSW- NB15 data set.},
	number = {2S11},
	journal = {International Journal of Recent Technology and Engineering},
	author = {Jatti, S. Ashwini V. and Kishor Sontif, V. J.K.},
	month = nov,
	year = {2019},
	note = {Publisher: IEEE},
	keywords = {\_obsidian},
	pages = {3976--3983},
}

@inproceedings{sharafaldin_toward_2023,
	title = {Toward {Generating} a {New} {Intrusion} {Detection} {Dataset} and {Intrusion} {Traffic} {Characterization}},
	isbn = {978-989-758-282-0},
	url = {https://www.scitepress.org/Link.aspx?doi=10.5220/0006639801080116},
	abstract = {Digital Library},
	urldate = {2023-03-22},
	author = {Sharafaldin, Iman and Lashkari, Arash Habibi and Ghorbani, Ali A.},
	month = mar,
	year = {2023},
	pages = {108--116},
}

@inproceedings{popoola_federated_2021,
	title = {Federated {Deep} {Learning} for {Collaborative} {Intrusion} {Detection} in {Heterogeneous} {Networks}},
	doi = {10.1109/VTC2021-Fall52928.2021.9625505},
	abstract = {In this paper, we propose Federated Deep Learning (FDL) for intrusion detection in heterogeneous networks. Local Deep Neural Network (DNN) models are used to learn the hierarchical representations of the private network traffic data in multiple edge nodes. A dedicated central server receives the parameters of the local DNN models from the edge nodes, and it aggregates them to produce an FDL model using the Fed+ fusion algorithm. Simulation results show that the FDL model achieved an accuracy of 99.27 ± 0.79\%, a precision of 97.03 ± 4.22\%, a recall of 98.06 ± 1.72\%, an F1 score of 97.50 ± 2.55\%, and a False Positive Rate (FPR) of 2.40 ± 2.47\%. The classification performance and the generalisation ability of the FDL model are better than those of the local DNN models. The Fed+ algorithm outperformed two state-of-the-art fusion algorithms, namely federated averaging (FedAvg) and Coordinate Median (CM). Therefore, the DNN-Fed+ model is preferable for intrusion detection in heterogeneous wireless networks.},
	booktitle = {2021 {IEEE} 94th {Vehicular} {Technology} {Conference} ({VTC2021}-{Fall})},
	author = {Popoola, Segun I. and Gui, Guan and Adebisi, Bamidele and Hammoudeh, Mohammad and Gacanin, Haris},
	month = sep,
	year = {2021},
	note = {ISSN: 2577-2465},
	keywords = {Deep learning, Heterogeneous networks, Image edge detection, Intrusion detection, Simulation, Telecommunication traffic, Wireless networks, deep learning, federated learning, heterogeneous wireless networks, intrusion detection, smart city},
	pages = {1--6},
}

@inproceedings{popoola_federated_2021-1,
	title = {Federated {Deep} {Learning} for {Collaborative} {Intrusion} {Detection} in {Heterogeneous} {Networks}},
	doi = {10.1109/VTC2021-Fall52928.2021.9625505},
	abstract = {In this paper, we propose Federated Deep Learning (FDL) for intrusion detection in heterogeneous networks. Local Deep Neural Network (DNN) models are used to learn the hierarchical representations of the private network traffic data in multiple edge nodes. A dedicated central server receives the parameters of the local DNN models from the edge nodes, and it aggregates them to produce an FDL model using the Fed+ fusion algorithm. Simulation results show that the FDL model achieved an accuracy of 99.27 ± 0.79\%, a precision of 97.03 ± 4.22\%, a recall of 98.06 ± 1.72\%, an F1 score of 97.50 ± 2.55\%, and a False Positive Rate (FPR) of 2.40 ± 2.47\%. The classification performance and the generalisation ability of the FDL model are better than those of the local DNN models. The Fed+ algorithm outperformed two state-of-the-art fusion algorithms, namely federated averaging (FedAvg) and Coordinate Median (CM). Therefore, the DNN-Fed+ model is preferable for intrusion detection in heterogeneous wireless networks.},
	booktitle = {2021 {IEEE} 94th {Vehicular} {Technology} {Conference} ({VTC2021}-{Fall})},
	author = {Popoola, Segun I. and Gui, Guan and Adebisi, Bamidele and Hammoudeh, Mohammad and Gacanin, Haris},
	month = sep,
	year = {2021},
	note = {ISSN: 2577-2465},
	keywords = {Deep learning, Heterogeneous networks, Image edge detection, Intrusion detection, Simulation, Telecommunication traffic, Wireless networks, deep learning, federated learning, heterogeneous wireless networks, intrusion detection, smart city},
	pages = {1--6},
}

@misc{sarhan_standardfeatureset_2021,
	title = {Towards a {Standard} {Feature} {Set} for {Network} {Intrusion} {Detection} {System} {Datasets}},
	url = {http://arxiv.org/abs/2101.11315},
	abstract = {Network Intrusion Detection Systems (NIDSs) are important tools for the protection of computer networks against increasingly frequent and sophisticated cyber attacks. Recently, a lot of research eﬀort has been dedicated to the development of Machine Learning (ML) based NIDSs. As in any ML-based application, the availability of high-quality datasets is critical for the training and evaluation of ML-based NIDS. One of the key problems with the currently available NIDS datasets is the lack of a standard feature set. The use of a unique and proprietary set of features for each of the publicly available datasets makes it virtually impossible to compare the performance of ML-based traﬃc classiﬁers on diﬀerent datasets, and hence to evaluate the ability of these systems to generalise across diﬀerent network scenarios. To address that limitation, this paper proposes and evaluates standard NIDS feature sets based on the NetFlow network meta-data collection protocol and system. We evaluate and compare two NetFlow-based feature set variants, a version with 12 features, and another one with 43 features. For our evaluation, we converted four widely used NIDS datasets (UNSW-NB15, BoT-IoT, ToN-IoT, CSE-CIC-IDS2018) into new variants with our proposed NetFlow based feature sets. Based on an Extra Tree classiﬁer, we compared the classiﬁcation performance of the NetFlow-based feature sets with the proprietary feature sets provided with the original datasets. While the smaller feature set cannot match the classiﬁcation performance of the proprietary feature sets, the larger set with 43 NetFlow features, surprisingly achieves a consistently higher classiﬁcation performance compared to the original feature set, which was tailored to each of the considered NIDS datasets. The proposed NetFlow-based standard NIDS feature set, together with four benchmark datasets, made available to the research community, allow a fair comparison of ML-based network traﬃc classiﬁers across diﬀerent NIDS datasets. We believe that having a standard feature set is critical for allowing a more rigorous and thorough evaluation of ML-based NIDSs and that it can help bridge the gap between academic research and the practical deployment of such systems.},
	language = {en},
	urldate = {2022-09-12},
	publisher = {arXiv},
	author = {Sarhan, Mohanad and Layeghy, Siamak and Portmann, Marius},
	month = may,
	year = {2021},
	note = {arXiv:2101.11315 [cs]},
	keywords = {Computer Science - Networking and Internet Architecture, \_read\_urgently},
}

@article{pontes_new_2021,
	title = {A {New} {Method} for {Flow}-{Based} {Network} {Intrusion} {Detection} {Using} the {Inverse} {Potts} {Model}},
	volume = {18},
	issn = {1932-4537},
	doi = {10.1109/TNSM.2021.3075503},
	abstract = {Network Intrusion Detection Systems (NIDS) play an important role as tools for identifying potential network threats. In the context of ever-increasing traffic volume on computer networks, flow-based NIDS arise as good solutions for real-time traffic classification. In recent years, different flow-based classifiers have been proposed using Machine Learning (ML) algorithms. Nevertheless, classical ML-based classifiers have some limitations. For instance, they require large amounts of labeled data for training, which might be difficult to obtain. Additionally, most ML-based classifiers are not capable of domain adaptation, i.e., after being trained on an specific data distribution, they are not general enough to be applied to other related data distributions. And, finally, many of the models inferred by these algorithms are black boxes, which do not provide explainable results. To overcome these limitations, we propose a new algorithm, called Energy-based Flow Classifier (EFC). This anomaly-based classifier uses inverse statistics to infer a statistical model based on labeled benign examples. We show that EFC is capable of accurately performing binary flow classification and is more adaptable to different data distributions than classical ML-based classifiers. Given the positive results obtained on three different datasets (CIDDS-001, CICIDS17 and CICDDoS19), we consider EFC to be a promising algorithm to perform robust flow-based traffic classification.},
	number = {2},
	journal = {IEEE Transactions on Network and Service Management},
	author = {Pontes, Camila F. T. and de Souza, Manuela M. C. and Gondim, João J. C. and Bishop, Matt and Marotta, Marcelo Antonio},
	month = jun,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Network and Service Management},
	keywords = {Adaptation models, Data models, Flow-based network intrusion detection, Machine learning algorithms, Network intrusion detection, Real-time systems, Security, Training, anomaly-based network intrusion detection, domain adaptation, energy-based flow classifier, inverse Potts model, network flow classification, network intrusion detection systems},
	pages = {1125--1136},
}

@inproceedings{fung_limitations_2020,
	address = {San Sebastian},
	title = {The limitations of federated learning in sybil settings},
	isbn = {978-1-939133-18-2},
	url = {https://www.usenix.org/conference/raid2020/presentation/fung},
	booktitle = {23rd international symposium on research in attacks, intrusions and defenses ({RAID} 2020)},
	publisher = {USENIX Association},
	author = {Fung, Clement and Yoon, Chris J. M. and Beschastnikh, Ivan},
	month = oct,
	year = {2020},
	pages = {301--316},
}

@inproceedings{de_melo_generalizing_2022,
	title = {Generalizing {Flow} {Classification} for {Distributed} {Denial}-of-{Service} over {Different} {Networks}},
	doi = {10.1109/GLOBECOM48099.2022.10001530},
	abstract = {With the growth in connected devices and network traffic, these systems require automated and fast approaches to achieve secure operations. Hence, machine learning-based network intrusion detection has become the state-of-the-art approach to tackle uncertainties and new attacks. However, the generalization of the models when exposed to different domains and workloads remains an open issue. In this paper, we propose using federated learning (FL) with sampling methods and feature selection to improve the generalization of the trained global model when evaluated in different network contexts. We evaluate this approach to classify network flows representing benign traffic and distributed denial-of-service attacks. Our proposed approach results in an 85\% improvement compared with the naive evaluation of training in one context and evaluating others. Moreover, it presented a similar performance to a statistical algorithm with the reported generalization capability on flow-based network traffic classification. Additionally, this FL-based approach brings data privacy and distributed learning capability to the table.},
	booktitle = {{GLOBECOM} 2022 - 2022 {IEEE} {Global} {Communications} {Conference}},
	author = {de Melo, Leonardo H and de C Bertoli, Gustavo and Pereira, Lourenco A and Saotome, Osamu and Domingues, Marcelo F and dos Santos, Aldri Luiz},
	month = dec,
	year = {2022},
	keywords = {DDoS, Distance learning, Distributed denial-of-service attack, Federated learning, Network intrusion detection, Telecommunication traffic, Training, Uncertainty, federated learning, network intrusion detection, security},
	pages = {879--884},
}

@article{de_carvalho_bertoli_generalizing_2023,
	title = {Generalizing intrusion detection for heterogeneous networks: {A} stacked-unsupervised federated learning approach},
	volume = {127},
	issn = {0167-4048},
	shorttitle = {Generalizing intrusion detection for heterogeneous networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0167404823000160},
	doi = {10.1016/j.cose.2023.103106},
	abstract = {The constantly evolving digital transformation imposes new requirements on our society. Aspects relating to reliance on the networking domain and the difficulty of achieving security by design pose a challenge today. As a result, data-centric and machine-learning approaches arose as feasible solutions for securing large networks. Although, in the network security domain, ML-based solutions face a challenge regarding the capability to generalize between different contexts. In other words, solutions based on specific network data usually do not perform satisfactorily on other networks. This paper describes the stacked-unsupervised federated learning (FL) approach to generalize on a cross-silo configuration for a flow-based network intrusion detection system (NIDS). The proposed approach we have examined comprises a deep autoencoder in conjunction with an energy flow classifier in an ensemble learning task. Our approach performs better than traditional local learning and naive cross-evaluation (training in one context and testing on another network data). Remarkably, the proposed approach demonstrates a sound performance in the case of non-IID data silos. In conjunction with an informative feature in an ensemble architecture for unsupervised learning, we advise that the proposed FL-based NIDS results in a feasible approach for generalization between heterogeneous networks.},
	language = {en},
	urldate = {2023-03-14},
	journal = {Computers \& Security},
	author = {de Carvalho Bertoli, Gustavo and Alves Pereira Junior, Lourenço and Saotome, Osamu and dos Santos, Aldri Luiz},
	month = apr,
	year = {2023},
	keywords = {Federated learning, Generalization, Network flows, Network intrusion detection, Unsupervised learning},
	pages = {103106},
}

@inproceedings{awan_contra_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{CONTRA}: {Defending} {Against} {Poisoning} {Attacks} in {Federated} {Learning}},
	isbn = {978-3-030-88418-5},
	shorttitle = {{CONTRA}},
	doi = {10.1007/978-3-030-88418-5_22},
	abstract = {Federated learning (FL) is an emerging machine learning paradigm. With FL, distributed data owners aggregate their model updates to train a shared deep neural network collaboratively, while keeping the training data locally. However, FL has little control over the local data and the training process. Therefore, it is susceptible to poisoning attacks, in which malicious or compromised clients use malicious training data or local updates as the attack vector to poison the trained global model. Moreover, the performance of existing detection and defense mechanisms drops significantly in a scaled-up FL system with non-iid data distributions. In this paper, we propose a defense scheme named CONTRA to defend against poisoning attacks, e.g., label-flipping and backdoor attacks, in FL systems. CONTRA implements a cosine-similarity-based measure to determine the credibility of local model parameters in each round and a reputation scheme to dynamically promote or penalize individual clients based on their per-round and historical contributions to the global model. With extensive experiments, we show that CONTRA significantly reduces the attack success rate while achieving high accuracy with the global model. Compared with a state-of-the-art (SOTA) defense, CONTRA reduces the attack success rate by 70\% and reduces the global model performance degradation by 50\%.},
	language = {en},
	booktitle = {Computer {Security} – {ESORICS} 2021},
	publisher = {Springer International Publishing},
	author = {Awan, Sana and Luo, Bo and Li, Fengjun},
	editor = {Bertino, Elisa and Shulman, Haya and Waidner, Michael},
	year = {2021},
	keywords = {Adversarial machine learning, Backdoor attacks, Data poisoning, Federated learning, Label-flipping attacks},
	pages = {455--475},
}

@inproceedings{chen_fedequal_2021,
	title = {{FedEqual}: {Defending} {Model} {Poisoning} {Attacks} in {Heterogeneous} {Federated} {Learning}},
	shorttitle = {{FedEqual}},
	doi = {10.1109/GLOBECOM46510.2021.9685082},
	abstract = {With the upcoming edge AI, federated learning (FL) is a privacy-preserving framework to meet the General Data Protection Regulation (GDPR). Unfortunately, FL is vulnerable to an up-to-date security threat, model poisoning attacks. By successfully replacing the global model with the targeted poisoned model, malicious end devices can trigger backdoor attacks and manipulate the whole learning process. The traditional researches under a homogeneous environment can ideally exclude the outliers with scarce side-effects on model performance. However, in privacy-preserving FL, each end device possibly owns a few data classes and different amounts of data, forming into a substantial heterogeneous environment where outliers could be malicious or benign. To achieve the system performance and robustness of FL's framework, we should not assertively remove any local model from the global model updating procedure. Therefore, in this paper, we propose a defending strategy called FedEqual to mitigate model poisoning attacks while preserving the learning task's performance without excluding any benign models. The results show that FedEqual outperforms other state-of-the-art baselines under different heterogeneous environments based on reproduced up-to-date model poisoning attacks.},
	booktitle = {2021 {IEEE} {Global} {Communications} {Conference} ({GLOBECOM})},
	author = {Chen, Ling-Yuan and Chiu, Te-Chuan and Pang, Ai-Chun and Cheng, Li-Chen},
	month = dec,
	year = {2021},
	keywords = {Collaborative work, Conferences, Edge AI, Federated Learning, Global communication, Model Poisoning Attacks, Model Security, Performance evaluation, Robustness, Security, System Robustness, System performance},
	pages = {1--6},
}

@inproceedings{mao_romoa_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Romoa: {Robust} {Model} {Aggregation} for the {Resistance} of {Federated} {Learning} to {Model} {Poisoning} {Attacks}},
	isbn = {978-3-030-88418-5},
	shorttitle = {Romoa},
	doi = {10.1007/978-3-030-88418-5_23},
	abstract = {Training a deep neural network requires substantial data and intensive computing resources. Unaffordable price holds back many potential applications of deep learning. Besides, it is risky to gather user’s private data for training centrally. Then federated learning appears as a promising solution to having users learned jointly while keeping training data local. However, security issues keep coming up in federated learning applications. One of the most threatening attacks is the model poisoning attack which can manipulate the inference result of a jointly learned model. Some recent studies show that elaborate model poisoning approaches can even breach the existing Byzantine-robust federated learning solutions. Hence, it is critical to discuss alternative solutions to secure federated learning. In this paper, we propose to protect federated learning against model poisoning attacks by introducing a robust model aggregation solution named Romoa. Unlike previous studies, Romoa can deal with targeted and untargeted poisoning attacks with a unified approach. Moreover, Romoa achieves more precise attack detection and better fairness for federated learning participants by constructing a new similarity measurement. We conclude that through a comprehensive evaluation of standard datasets, Romoa can provide a satisfying defense effect against model poisoning attacks, including those attacks breaching Byzantine-robust federated learning solutions.},
	language = {en},
	booktitle = {Computer {Security} – {ESORICS} 2021},
	publisher = {Springer International Publishing},
	author = {Mao, Yunlong and Yuan, Xinyu and Zhao, Xinyang and Zhong, Sheng},
	editor = {Bertino, Elisa and Shulman, Haya and Waidner, Michael},
	year = {2021},
	keywords = {Federated learning, Model poisoning attack, Robust model aggregation},
	pages = {476--496},
}

@article{wagner_cyber_2019,
	title = {Cyber threat intelligence sharing: {Survey} and research directions},
	volume = {87},
	issn = {01674048},
	doi = {10.1016/j.cose.2019.101589},
	abstract = {Cyber Threat Intelligence (CTI) sharing has become a novel weapon in the arsenal of cyber defenders to proactively mitigate increasing cyber attacks. Automating the process of CTI sharing, and even the basic consumption, has raised new challenges for researchers and practitioners. This extensive literature survey explores the current state-of-the-art and approaches different problem areas of interest pertaining to the larger field of sharing cyber threat intelligence. The motivation for this research stems from the recent emergence of sharing cyber threat intelligence and the involved challenges of automating its processes. This work comprises a considerable amount of articles from academic and gray literature, and focuses on technical and non-technical challenges. Moreover, the findings reveal which topics were widely discussed, and hence considered relevant by the authors and cyber threat intelligence sharing communities.},
	journal = {Computers \& Security},
	author = {Wagner, Thomas D. and Mahbub, Khaled and Palomar, Esther and Abdallah, Ali E.},
	year = {2019},
	pages = {101589},
}

@inproceedings{shen_auror_2016,
	address = {New York, NY, USA},
	title = {Auror: defending against poisoning attacks in collaborative deep learning systems},
	isbn = {978-1-4503-4771-6},
	url = {https://dl.acm.org/doi/10.1145/2991079.2991125},
	doi = {10.1145/2991079.2991125},
	booktitle = {Proceedings of the 32nd {Annual} {Conference} on {Computer} {Security} {Applications}},
	publisher = {ACM},
	author = {Shen, Shiqi and Tople, Shruti and Saxena, Prateek},
	month = dec,
	year = {2016},
	pages = {508--519},
}

@article{ma_shieldfl_2022,
	title = {{ShieldFL}: {Mitigating} {Model} {Poisoning} {Attacks} in {Privacy}-{Preserving} {Federated} {Learning}},
	volume = {17},
	issn = {1556-6013, 1556-6021},
	shorttitle = {{ShieldFL}},
	url = {https://ieeexplore.ieee.org/document/9762272/},
	doi = {10.1109/TIFS.2022.3169918},
	abstract = {Privacy-Preserving Federated Learning (PPFL) is an emerging secure distributed learning paradigm that aggregates user-trained local gradients into a federated model through a cryptographic protocol. Unfortunately, PPFL is vulnerable to model poisoning attacks launched by a Byzantine adversary, who crafts malicious local gradients to harm the accuracy of the federated model. To resist model poisoning attacks, existing defense strategies focus on identifying suspicious local gradients over plaintexts. However, the Byzantine adversary submits encrypted poisonous gradients to circumvent existing defense strategies in PPFL, resulting in encrypted model poisoning. To address the issue, in this paper we design a privacy-preserving defense strategy using two-trapdoor homomorphic encryption (referred to as ShieldFL), which can resist encrypted model poisoning without compromising privacy in PPFL. Specially, we ﬁrst present the secure cosine similarity method aiming to measure the distance between two encrypted gradients. Then, we propose the Byzantine-tolerance aggregation using cosine similarity, which can achieve robustness for both Independently Identically Distribution (IID) and non-IID data. Extensive evaluations on three benchmark datasets (i.e., MNIST, KDDCup99, and Amazon) show that ShieldFL outperforms existing defense strategies. Especially, ShieldFL can achieve 30\%−80\% accuracy improvement to defend two state-of-the-art model poisoning attacks in both non-IID and IID settings.},
	language = {en},
	urldate = {2022-07-05},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Ma, Zhuoran and Ma, Jianfeng and Miao, Yinbin and Li, Yingjiu and Deng, Robert H.},
	year = {2022},
	pages = {1639--1654},
}

@article{nguyen_federated_2022,
	title = {Federated {Learning} for {Smart} {Healthcare}: {A} {Survey}},
	volume = {55},
	issn = {0360-0300},
	shorttitle = {Federated {Learning} for {Smart} {Healthcare}},
	url = {https://doi.org/10.1145/3501296},
	doi = {10.1145/3501296},
	abstract = {Recent advances in communication technologies and the Internet-of-Medical-Things (IOMT) have transformed smart healthcare enabled by artificial intelligence (AI). Traditionally, AI techniques require centralized data collection and processing that may be infeasible in realistic healthcare scenarios due to the high scalability of modern healthcare networks and growing data privacy concerns. Federated Learning (FL), as an emerging distributed collaborative AI paradigm, is particularly attractive for smart healthcare, by coordinating multiple clients (e.g., hospitals) to perform AI training without sharing raw data. Accordingly, we provide a comprehensive survey on the use of FL in smart healthcare. First, we present the recent advances in FL, the motivations, and the requirements of using FL in smart healthcare. The recent FL designs for smart healthcare are then discussed, ranging from resource-aware FL, secure and privacy-aware FL to incentive FL and personalized FL. Subsequently, we provide a state-of-the-art review on the emerging applications of FL in key healthcare domains, including health data management, remote health monitoring, medical imaging, and COVID-19 detection. Several recent FL-based smart healthcare projects are analyzed, and the key lessons learned from the survey are also highlighted. Finally, we discuss interesting research challenges and possible directions for future FL research in smart healthcare.},
	number = {3},
	urldate = {2023-03-11},
	journal = {ACM Computing Surveys},
	author = {Nguyen, Dinh C. and Pham, Quoc-Viet and Pathirana, Pubudu N. and Ding, Ming and Seneviratne, Aruna and Lin, Zihuai and Dobre, Octavia and Hwang, Won-Joo},
	year = {2022},
	keywords = {Federated learning, privacy, smart healthcare},
	pages = {60:1--60:37},
}

@inproceedings{fong_interpretable_2017,
	title = {Interpretable {Explanations} of {Black} {Boxes} by {Meaningful} {Perturbation}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Fong_Interpretable_Explanations_of_ICCV_2017_paper.html},
	urldate = {2023-03-11},
	author = {Fong, Ruth C. and Vedaldi, Andrea},
	year = {2017},
	pages = {3429--3437},
}

@inproceedings{lundberg_unified_2017,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	urldate = {2023-03-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lundberg, Scott M and Lee, Su-In},
	year = {2017},
	keywords = {⛔ No DOI found},
}

@inproceedings{yin_byzantine-robust_2018,
	title = {Byzantine-{Robust} {Distributed} {Learning}: {Towards} {Optimal} {Statistical} {Rates}},
	shorttitle = {Byzantine-{Robust} {Distributed} {Learning}},
	url = {https://proceedings.mlr.press/v80/yin18a.html},
	abstract = {In this paper, we develop distributed optimization algorithms that are provably robust against Byzantine failures—arbitrary and potentially adversarial behavior, in distributed computing systems, with a focus on achieving optimal statistical performance. A main result of this work is a sharp analysis of two robust distributed gradient descent algorithms based on median and trimmed mean operations, respectively. We prove statistical error rates for all of strongly convex, non-strongly convex, and smooth non-convex population loss functions. In particular, these algorithms are shown to achieve order-optimal statistical error rates for strongly convex losses. To achieve better communication efficiency, we further propose a median-based distributed algorithm that is provably robust, and uses only one communication round. For strongly convex quadratic loss, we show that this algorithm achieves the same optimal error rate as the robust distributed gradient descent algorithms.},
	language = {en},
	urldate = {2023-03-07},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yin, Dong and Chen, Yudong and Kannan, Ramchandran and Bartlett, Peter},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {5650--5659},
}

@inproceedings{tolpegin_data_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Data {Poisoning} {Attacks} {Against} {Federated} {Learning} {Systems}},
	isbn = {978-3-030-58951-6},
	doi = {10.1007/978-3-030-58951-6_24},
	abstract = {Federated learning (FL) is an emerging paradigm for distributed training of large-scale deep neural networks in which participants’ data remains on their own devices with only model updates being shared with a central server. However, the distributed nature of FL gives rise to new threats caused by potentially malicious participants. In this paper, we study targeted data poisoning attacks against FL systems in which a malicious subset of the participants aim to poison the global model by sending model updates derived from mislabeled data. We first demonstrate that such data poisoning attacks can cause substantial drops in classification accuracy and recall, even with a small percentage of malicious participants. We additionally show that the attacks can be targeted, i.e., they have a large negative impact only on classes that are under attack. We also study attack longevity in early/late round training, the impact of malicious participant availability, and the relationships between the two. Finally, we propose a defense strategy that can help identify malicious participants in FL to circumvent poisoning attacks, and demonstrate its effectiveness.},
	language = {en},
	booktitle = {Computer {Security} – {ESORICS} 2020},
	publisher = {Springer International Publishing},
	author = {Tolpegin, Vale and Truex, Stacey and Gursoy, Mehmet Emre and Liu, Ling},
	editor = {Chen, Liqun and Li, Ninghui and Liang, Kaitai and Schneider, Steve},
	year = {2020},
	keywords = {Adversarial machine learning, Data poisoning, Deep learning, Federated learning, Label flipping},
	pages = {480--501},
}

@inproceedings{bhagoji_analyzing_2019,
	title = {Analyzing {Federated} {Learning} through an {Adversarial} {Lens}},
	url = {https://proceedings.mlr.press/v97/bhagoji19a.html},
	abstract = {Federated learning distributes model training among a multitude of agents, who, guided by privacy concerns, perform training using their local data but share only model parameter updates, for iterative aggregation at the server to train an overall global model. In this work, we explore how the federated learning setting gives rise to a new threat, namely model poisoning, which differs from traditional data poisoning. Model poisoning is carried out by an adversary controlling a small number of malicious agents (usually 1) with the aim of causing the global model to misclassify a set of chosen inputs with high conﬁdence. We explore a number of strategies to carry out this attack on deep neural networks, starting with targeted model poisoning using a simple boosting of the malicious agent’s update to overcome the effects of other agents. We also propose two critical notions of stealth to detect malicious updates. We bypass these by including them in the adversarial objective to carry out stealthy model poisoning. We improve its stealth with the use of an alternating minimization strategy which alternately optimizes for stealth and the adversarial objective. We also empirically demonstrate that Byzantine-resilient aggregation strategies are not robust to our attacks. Our results indicate that highly constrained adversaries can carry out model poisoning attacks while maintaining stealth, thus highlighting the vulnerability of the federated learning setting and the need to develop effective defense strategies.},
	language = {en},
	urldate = {2023-02-23},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bhagoji, Arjun Nitin and Chakraborty, Supriyo and Mittal, Prateek and Calo, Seraphin},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {634--643},
}

@inproceedings{fang_local_2020,
	title = {Local {Model} {Poisoning} {Attacks} to \{{Byzantine}-{Robust}\} {Federated} {Learning}},
	isbn = {978-1-939133-17-5},
	url = {https://www.usenix.org/conference/usenixsecurity20/presentation/fang},
	language = {en},
	urldate = {2023-02-23},
	author = {Fang, Minghong and Cao, Xiaoyu and Jia, Jinyuan and Gong, Neil},
	year = {2020},
	pages = {1605--1622},
}

@inproceedings{ouyang_clusterfl_2021,
	address = {Virtual Event Wisconsin},
	title = {{ClusterFL}: a similarity-aware federated learning system for human activity recognition},
	isbn = {978-1-4503-8443-8},
	shorttitle = {{ClusterFL}},
	url = {https://dl.acm.org/doi/10.1145/3458864.3467681},
	doi = {10.1145/3458864.3467681},
	abstract = {Federated Learning (FL) has recently received signiﬁcant interests thanks to its capability of protecting data privacy. However, existing FL paradigms yield unsatisfactory performance for a wide class of human activity recognition (HAR) applications since they are oblivious to the intrinsic relationship between data of diﬀerent users. We propose ClusterFL, a similarity-aware federated learning system that can provide high model accuracy and low communication overhead for HAR applications. ClusterFL features a novel clustered multi-task federated learning framework that maximizes the training accuracy of multiple learned models while automatically capturing the intrinsic clustering relationship among the data of diﬀerent nodes. Based on the learned cluster relationship, ClusterFL can eﬃciently drop out the nodes that converge slower or have little correlation with other nodes in each cluster, signiﬁcantly speeding up the convergence while maintaining the accuracy performance. We evaluate the performance of ClusterFL on an NVIDIA edge testbed using four new HAR datasets collected from total 145 users. The results show that, ClusterFL outperforms several state-of-the-art FL paradigms in terms of overall accuracy, and save more than 50\% communication overhead at the expense of negligible accuracy degradation.},
	language = {en},
	urldate = {2023-02-02},
	booktitle = {Proceedings of the 19th {Annual} {International} {Conference} on {Mobile} {Systems}, {Applications}, and {Services}},
	publisher = {ACM},
	author = {Ouyang, Xiaomin and Xie, Zhiyuan and Zhou, Jiayu and Huang, Jianwei and Xing, Guoliang},
	month = jun,
	year = {2021},
	pages = {54--66},
}

@article{resnick_reputation_2000,
	title = {Reputation systems},
	volume = {43},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/355112.355122},
	doi = {10.1145/355112.355122},
	number = {12},
	urldate = {2023-02-01},
	journal = {Communications of the ACM},
	author = {Resnick, Paul and Kuwabara, Ko and Zeckhauser, Richard and Friedman, Eric},
	year = {2000},
	pages = {45--48},
}

@inproceedings{uprety_mitigating_2021,
	title = {Mitigating {Poisoning} {Attack} in {Federated} {Learning}},
	doi = {10.1109/SSCI50451.2021.9659839},
	abstract = {Adversarial machine learning (AML) has emerged as one of the significant research areas in machine learning (ML) because models we train lack robustness and trustworthiness. Federated learning (FL) trains models over distributed devices and model parameters are shared instead of actual data in a privacy-preserving manner. Unfortunately, FL is also vulnerable to attacks including parameter/data poisoning attacks. In this paper, we first analyze the impact of the data poisoning attack on this training method with a label-flipping attack. We propose a poisoning attack mitigation technique based on the reputation of nodes' involved in the training process. The reputation score for each client is calculated using the beta probability distribution method. This is the first work to show the removal of malicious nodes with the poisoned dataset from the training environment based on the calculated reputation score. The improvement in model performance after filtering malicious nodes is validated using the benchmark MNIST dataset. At the same time, our work contributes to preventing denial of service attacks by considering a blockchain-based server network. Our results hold for two different attack settings with different proportions of poisoned data samples.},
	booktitle = {2021 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	author = {Uprety, Aashma and Rawat, Danda B.},
	month = dec,
	year = {2021},
	keywords = {Collaborative work, Computational modeling, Data models, Data poisoning attack, Data privacy, Distance learning, Filtering, Training, reputation model, secure federated learning},
	pages = {01--07},
}

@inproceedings{cai_cluster-based_2022,
	title = {Cluster-based {Federated} {Learning} {Framework} for {Intrusion} {Detection}},
	doi = {10.1109/PAAP56126.2022.10010553},
	abstract = {With the rapid development of Industrial Internet, the network intrusion detection has become particularly important. In the Industrial Internet, large-scale data is distributed in the edge nodes caused the joint analysis of network intrusion detection at each edge node has become necessary. Federated learning structure can avoid data out of local nodes to protect user privacy data. However, the data distribution is different for each edge nodes, which limits the effectiveness of federated learning models. We focus on the non-IID data features and propose a new cluster-based federated learning framework for network intrusion detection. In this method, we cluster clients into different communities by data labels, which the clients contain the similar proportion of data labels in the same community. Based on the clustering results, we decompose federated learning model aggregation into cluster aggregation and global aggregation by leveraging similarities both within and between clusters. We conduct extensive experiments based on UNSW\_NB15 dataset. The results show that our method has better performance than FedAvg and FedProx. It can work well in scenarios with different distributions of data samples while ensuring data security and privacy protection.},
	booktitle = {2022 {IEEE} 13th {International} {Symposium} on {Parallel} {Architectures}, {Algorithms} and {Programming} ({PAAP})},
	author = {Cai, Luxin and Chen, Naiyue and Wei, Yuanmeng and Chen, Huaping and Li, Yidong},
	month = nov,
	year = {2022},
	keywords = {Data privacy, Data security, Distributed databases, Federated learning, Image edge detection, Intrusion detection, Network intrusion detection, Programming, \_read\_urgently, cluster, federated learning, non-IID, similarity},
	pages = {1--6},
}

@article{blanchard_machine_2017,
	title = {Machine learning with adversaries: {Byzantine} tolerant gradient descent},
	volume = {30},
	shorttitle = {Machine learning with adversaries},
	journal = {Advances in Neural Information Processing Systems},
	author = {Blanchard, Peva and El Mhamdi, El Mahdi and Guerraoui, Rachid and Stainer, Julien},
	year = {2017},
	keywords = {⛔ No DOI found},
}

@article{Chismon2015,
	title = {Threat {Intelligence}: {Collecting}, {Analysing}, {Evaluating}},
	abstract = {Threat intelligence is rapidly becoming an ever-higher business priority. There is a general awareness of the need to ‘do’ threat intelligence, and vendors are falling over themselves to offer a confusingly diverse array of threat intelligence products. Figure},
	journal = {Cert-Uk},
	author = {Chismon, David and Ruks, Martyn},
	year = {2015},
	keywords = {⛔ No DOI found},
	pages = {36},
}

@inproceedings{rajput_detox_2019,
	title = {{DETOX}: {A} {Redundancy}-based {Framework} for {Faster} and {More} {Robust} {Gradient} {Aggregation}},
	volume = {32},
	shorttitle = {{DETOX}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/415185ea244ea2b2bedeb0449b926802-Abstract.html},
	abstract = {To improve the resilience of distributed  training to worst-case, or Byzantine node failures, several recent methods have replaced gradient averaging with robust aggregation methods. Such techniques can have high computational costs, often quadratic in the number of compute nodes, and only have limited robustness guarantees. Other methods have instead used redundancy to guarantee robustness, but can only tolerate limited numbers of Byzantine failures. In this work, we present DETOX, a Byzantine-resilient distributed training framework that combines algorithmic redundancy with robust aggregation. DETOX operates in two steps, a filtering step that uses limited redundancy to significantly reduce the effect of Byzantine nodes, and a hierarchical aggregation step that can be used in tandem with any state-of-the-art robust aggregation method. We show theoretically that this leads to a substantial increase in robustness, and has a per iteration runtime that can be nearly linear in the number of compute nodes. We provide extensive experiments over real distributed setups across a variety of large-scale machine learning tasks, showing that DETOX leads to orders of magnitude accuracy and speedup improvements over many state-of-the-art Byzantine-resilient approaches.},
	urldate = {2022-10-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rajput, Shashank and Wang, Hongyi and Charles, Zachary and Papailiopoulos, Dimitris},
	year = {2019},
	keywords = {⛔ No DOI found},
}

@misc{zhao_shielding_2020,
	title = {Shielding {Collaborative} {Learning}: {Mitigating} {Poisoning} {Attacks} through {Client}-{Side} {Detection}},
	shorttitle = {Shielding {Collaborative} {Learning}},
	url = {http://arxiv.org/abs/1910.13111},
	abstract = {Collaborative learning allows multiple clients to train a joint model without sharing their data with each other. Each client performs training locally and then submits the model updates to a central server for aggregation. Since the server has no visibility into the process of generating the updates, collaborative learning is vulnerable to poisoning attacks where a malicious client can generate a poisoned update to introduce backdoor functionality to the joint model. The existing solutions for detecting poisoned updates, however, fail to defend against the recently proposed attacks, especially in the non-IID setting. In this paper, we present a novel defense scheme to detect anomalous updates in both IID and non-IID settings. Our key idea is to realize client-side cross-validation, where each update is evaluated over other clients' local data. The server will adjust the weights of the updates based on the evaluation results when performing aggregation. To adapt to the unbalanced distribution of data in the non-IID setting, a dynamic client allocation mechanism is designed to assign detection tasks to the most suitable clients. During the detection process, we also protect the client-level privacy to prevent malicious clients from stealing the training data of other clients, by integrating differential privacy with our design without degrading the detection performance. Our experimental evaluations on two real-world datasets show that our scheme is significantly robust to two representative poisoning attacks.},
	urldate = {2022-08-28},
	publisher = {arXiv},
	author = {Zhao, Lingchen and Hu, Shengshan and Wang, Qian and Jiang, Jianlin and Shen, Chao and Luo, Xiangyang and Hu, Pengfei},
	month = mar,
	year = {2020},
	note = {arXiv:1910.13111 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{bougueroua_survey_2021,
	title = {A {Survey} on {Multi}-{Agent} {Based} {Collaborative} {Intrusion} {Detection} {Systems}},
	volume = {11},
	doi = {10.2478/jaiscr-2021-0008},
	abstract = {Multi-Agent Systems (MAS) have been widely used in many areas like modeling and simulation of complex phenomena, and distributed problem solving. Likewise, MAS have been used in cyber-security, to build more efficient Intrusion Detection Systems (IDS), namely Collaborative Intrusion Detection Systems (CIDS). This work presents a taxonomy for classifying the methods used to design intrusion detection systems, and how such methods were used alongside with MAS in order to build IDS that are deployed in distributed environments, resulting in the emergence of CIDS. The proposed taxonomy, consists of three parts: 1) general architecture of CIDS, 2) the used agent technology, and 3) decision techniques, in which used technologies are presented. The proposed taxonomy reviews and classifies the most relevant works in this topic and highlights open research issues in view of recent and emerging threats. Thus, this work provides a good insight regarding past, current, and future solutions for CIDS, and helps both researchers and professionals design more effective solutions.},
	journal = {Journal of Artificial Intelligence and Soft Computing Research},
	author = {Bougueroua, Nassima and Mazouzi, Smaine and Belaoued, Mohamed and Seddari, Noureddine and Derhab, Abdelouahid and Bouras, Abdelghani},
	month = apr,
	year = {2021},
	pages = {111--142},
}

@article{fung_dirichlet-based_2011,
	title = {Dirichlet-{Based} {Trust} {Management} for {Effective} {Collaborative} {Intrusion} {Detection} {Networks}},
	volume = {8},
	issn = {1932-4537},
	doi = {10.1109/TNSM.2011.050311.100028},
	abstract = {The accuracy of detecting intrusions within a Collaborative Intrusion Detection Network (CIDN) depends on the efficiency of collaboration between peer Intrusion Detection Systems (IDSes) as well as the security itself of the CIDN. In this paper, we propose Dirichlet-based trust management to measure the level of trust among IDSes according to their mutual experience. An acquaintance management algorithm is also proposed to allow each IDS to manage its acquaintances according to their trustworthiness. Our approach achieves strong scalability properties and is robust against common insider threats, resulting in an effective CIDN. We evaluate our approach based on a simulated CIDN, demonstrating its improved robustness, efficiency and scalability for collaborative intrusion detection in comparison with other existing models.},
	number = {2},
	journal = {IEEE Transactions on Network and Service Management},
	author = {Fung, Carol J and Zhang, Jie and Aib, Issam and Boutaba, Raouf},
	month = jun,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Network and Service Management},
	keywords = {Collaboration, Collaborative intrusion detection system, Equations, Intrusion detection, Mathematical model, Peer to peer computing, Robustness, Scalability, admission control, computer security, security management, trust management},
	pages = {79--91},
}

@inproceedings{short_using_2020,
	title = {Using {Blockchain} {Technologies} to {Improve} {Security} in {Federated} {Learning} {Systems}},
	doi = {10.1109/COMPSAC48688.2020.00-96},
	abstract = {The potential of Federated Learning (FL) deployment increases rapidly as the number of connected devices increases, the value of artificial intelligence is recognized and networking technologies and edge computing evolves. However, as in any distributed system, a set of security issues arise in FL systems. In this paper, we discuss the use of blockchain technology to address diverse security aspects of FL systems and focus on the model poisoning attack for which we propose a novel Blockchain-based defense scheme. An assessment using data from the MNIST database has shown that the proposed approach, which has been designed to be implemented on blockchain technology, offers significant protection against adversaries attempting model poisoning attacks. The approach adopts a novel algorithm for evaluating the model updates, by verifying each model update separately against a verification dataset, without requiring information about the training dataset size, which is often unavailable or easily falsified.},
	booktitle = {2020 {IEEE} 44th {Annual} {Computers}, {Software}, and {Applications} {Conference} ({COMPSAC})},
	author = {Short, Andrew Ronald and Leligou, Helen C. and Papoutsidakis, Michael and Theocharis, Efstathios},
	month = jul,
	year = {2020},
	note = {ISSN: 0730-3157},
	keywords = {Blockchain, Federated Learning, Security attacks, Computers, Conferences, Software},
	pages = {1183--1188},
}

@inproceedings{alexopoulos_towards_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Towards {Blockchain}-{Based} {Collaborative} {Intrusion} {Detection} {Systems}},
	isbn = {978-3-319-99843-5},
	doi = {10.1007/978-3-319-99843-5_10},
	abstract = {In an attempt to cope with the increased number of cyber-attacks, research in Intrusion Detection System IDSs is moving towards more collaborative mechanisms. Collaborative IDSs (CIDSs) are such an approach; they combine the knowledge of a plethora of monitors to generate a holistic picture of the monitored network. Despite the research done in this field, CIDSs still face a number of fundamental challenges, especially regarding maintaining trust among the collaborating parties. Recent advances in distributed ledger technologies, e.g. various implementations of blockchain protocols, are a good fit to the problem of enhancing trust in collaborative environments. This paper touches the intersection of CIDSs and blockchains. Particularly, it introduces the idea of utilizing blockchain technologies as a mechanism for improving CIDSs. We argue that certain properties of blockchains can be of significant benefit for CIDSs; namely for the improvement of trust between monitors, and for providing accountability and consensus. For this, we study the related work and highlight the research gaps and challenges towards such a task. Finally, we propose a generic architecture for the incorporation of blockchains into the field of CIDSs and an analysis of the design decisions that need to be made to implement such an architecture.},
	language = {en},
	booktitle = {Critical {Information} {Infrastructures} {Security}},
	publisher = {Springer International Publishing},
	author = {Alexopoulos, Nikolaos and Vasilomanolakis, Emmanouil and Ivánkó, Natália Réka and Mühlhäuser, Max},
	editor = {D'Agostino, Gregorio and Scala, Antonio},
	year = {2018},
	pages = {107--118},
}

@inproceedings{fung_trust_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Trust {Management} for {Host}-{Based} {Collaborative} {Intrusion} {Detection}},
	isbn = {978-3-540-87353-2},
	doi = {10.1007/978-3-540-87353-2_9},
	abstract = {The accuracy of detecting an intrusion within a network of intrusion detection systems (IDSes) depends on the efficiency of collaboration between member IDSes. The security itself within this network is an additional concern that needs to be addressed. In this paper, we present a trust-based framework for secure and effective collaboration within an intrusion detection network (IDN). In particular, we define a trust model that allows each IDS to evaluate the trustworthiness of others based on personal experience. We prove the correctness of our approach in protecting the IDN. Additionally, experimental results demonstrate that our system yields a significant improvement in detecting intrusions. The trust model further improves the robustness of the collaborative system against malicious attacks.},
	language = {en},
	booktitle = {Managing {Large}-{Scale} {Service} {Deployment}},
	publisher = {Springer},
	author = {Fung, Carol J. and Baysal, Olga and Zhang, Jie and Aib, Issam and Boutaba, Raouf},
	editor = {De Turck, Filip and Kellerer, Wolfgang and Kormentzas, George},
	year = {2008},
	keywords = {Collaboration, Decay, Intrusion detection Network, Peer-to-Peer, Security, Trust Management},
	pages = {109--122},
}

@book{haji_mirzaee_chfl_2022,
	title = {{CHFL}: {A} {Collaborative} {Hierarchical} {Federated} {Intrusion} {Detection} {System} for {Vehicular} {Networks}},
	shorttitle = {{CHFL}},
	abstract = {Wireless interfaces, remote control schemes, and increased autonomy have raised the attacks surface of vehicular networks. As powerful monitoring entities, intrusion detection systems (IDS) must be updated and customised to respond to emerging networks' requirements. As server-based monitoring schemes were prone to significant privacy concerns, new privacy constrained learning methods such as federated learning (FL) have received considerable attention in designing IDS. However, to alleviate the efficiency and enhance the scalability of the original FL, this paper proposes a novel collaborative hierarchical federated IDS, named CHFL for the vehicular network. In the CHFL model, a group of vehicles assisted by vehicle-to-everything (V2X) communication technologies can exchange intrusion detection information collaboratively in a private format. Each group nominates a leader, and the leading vehicle serves as the intermediate in the second level detection system of the hierarchical federated model. The leader communicates directly with the server to transmit and receive model updates of its nearby end vehicles. By reducing the number of direct communications to the server, our proposed system reduces network uplink traffic and queuing-processing latency. In addition, CHFL improved the prediction loss and the accuracy of the whole system. We are achieving an accuracy of 99.10\% compared with 97.01\% accuracy of the original FL.},
	author = {Haji Mirzaee, Parya and Shojafar, Mohammad and Cruickshank, Haitham and Tafazolli, Rahim},
	month = apr,
	year = {2022},
}

@article{fung_facid_2016,
	title = {{FACID}: {A} trust-based collaborative decision framework for intrusion detection networks},
	volume = {53},
	issn = {1570-8705},
	shorttitle = {{FACID}},
	url = {https://www.sciencedirect.com/science/article/pii/S1570870516302062},
	doi = {10.1016/j.adhoc.2016.08.014},
	abstract = {Computer systems evolve to be more complex and vulnerable. Cyber attacks have also grown to be more sophisticated and harder to detect. Intrusion detection is the process of monitoring and identifying unauthorized system access or manipulation. It becomes increasingly difficult for a single intrusion detection system (IDS) to detect all attacks due to limited knowledge about attacks. Collaboration among intrusion detection devices can be used to gain higher detection accuracy and cost efficiency as compared to its traditional single host-based counterpart. Through cooperation, a local IDS can detect new attacks that may be known to other IDSs, which may be from different vendors. However, how to utilize the diagnosis from different IDSs to perform intrusion detection is the key challenge. This paper proposes a system architecture of a collaborative intrusion detection network (CIDN), in which trustworthy and efficient feedback aggregation is a key component. To achieve a reliable and trustworthy CIDN, we present a framework called FACID, which leverages data analytical models and hypothesis testing methods for efficient, distributed and sequential feedback aggregations. FACID provides an inherent trust evaluation mechanism and reduces communication overhead needed for IDSs as well as the computational resources and memory needed to achieve satisfactory feedback aggregation results when the number of collaborators of an IDS is large. Our simulation results corroborate our theoretical results and demonstrate the properties of cost efficiency and accuracy compared to other heuristic methods. The analytical result on the lower-bound of the average number of acquaintances for consultation is essential for the design and configuration of IDSs in a collaborative environment.},
	language = {en},
	urldate = {2022-07-07},
	journal = {Ad Hoc Networks},
	author = {Fung, Carol J. and Zhu, Quanyan},
	month = dec,
	year = {2016},
	keywords = {Cooperative networks, Distributed algorithms, Intrusion detection networks, Resource allocations},
	pages = {17--31},
}

@inproceedings{vasilomanolakis_towards_2017,
	address = {Cham},
	series = {{IFIP} {Advances} in {Information} and {Communication} {Technology}},
	title = {Towards {Trust}-{Aware} {Collaborative} {Intrusion} {Detection}: {Challenges} and {Solutions}},
	isbn = {978-3-319-59171-1},
	shorttitle = {Towards {Trust}-{Aware} {Collaborative} {Intrusion} {Detection}},
	doi = {10.1007/978-3-319-59171-1_8},
	abstract = {Collaborative Intrusion Detection Systems (CIDSs) are an emerging field in cyber-security. In such an approach, multiple sensors collaborate by exchanging alert data with the goal of generating a complete picture of the monitored network. This can provide significant improvements in intrusion detection and especially in the identification of sophisticated attacks. However, the challenge of deciding to which extend a sensor can trust others, has not yet been holistically addressed in related work. In this paper, we firstly propose a set of requirements for reliable trust management in CIDSs. Afterwards, we carefully investigate the most dominant CIDS trust schemes. The main contribution of the paper is mapping the results of the analysis to the aforementioned requirements, along with a comparison of the state of the art. Furthermore, this paper identifies and discusses the research gaps and challenges with regard to trust and CIDSs.},
	language = {en},
	booktitle = {Trust {Management} {XI}},
	publisher = {Springer International Publishing},
	author = {Vasilomanolakis, Emmanouil and Habib, Sheikh Mahbub and Milaszewicz, Pavlos and Malik, Rabee Sohail and Mühlhäuser, Max},
	editor = {Steghöfer, Jan-Philipp and Esfandiari, Babak},
	year = {2017},
	keywords = {Computational Trust, Initial Trust, Intrusion Detection System, Trust Level, Trust Management, decay},
	pages = {94--109},
}

@inproceedings{putra_decentralised_2021,
	title = {Decentralised {Trustworthy} {Collaborative} {Intrusion} {Detection} {System} for {IoT}},
	doi = {10.1109/Blockchain53845.2021.00048},
	abstract = {Intrusion Detection Systems (IDS) have been the industry standard for securing IoT networks against known attacks. To increase the capability of an IDS, researchers proposed the concept of blockchain-based Collaborative-IDS (CIDS), wherein blockchain acts as a decentralised platform allowing collaboration between CIDS nodes to share intrusion related information, such as intrusion alarms and detection rules. However, proposals in blockchain-based CIDS overlook the importance of continuous evaluation of the trustworthiness of each node and generally work based on the assumption that the nodes are always honest. In this paper, we propose a decentralised CIDS that emphasises the importance of building trust between CIDS nodes. In our proposed solution, each CIDS node exchanges detection rules to help other nodes detect new types of intrusion. Our architecture offloads the trust computation to the blockchain and utilises a decentralised storage to host the shared trustworthy detection rules, ensuring scalability. Our implementation in a lab-scale testbed shows that the our solution is feasible and performs within the expected benchmarks of the Ethereum platform.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Blockchain} ({Blockchain})},
	author = {Putra, Guntur Dharma and Dedeoglu, Volkan and Pathak, Abhinav and Kanhere, Salil S. and Jurdak, Raja},
	month = dec,
	year = {2021},
	keywords = {Benchmark testing, Blockchains, Collaboration, Computer architecture, Economics, Intrusion detection, IoT, Scalability, blockchain, collaborative, intrusion detection system, scalability, trust management},
	pages = {306--313},
}

@article{gil_perez_repcidn_2013,
	title = {{RepCIDN}: {A} {Reputation}-based {Collaborative} {Intrusion} {Detection} {Network} to {Lessen} the {Impact} of {Malicious} {Alarms}},
	volume = {21},
	issn = {1573-7705},
	shorttitle = {{RepCIDN}},
	url = {https://doi.org/10.1007/s10922-012-9230-8},
	doi = {10.1007/s10922-012-9230-8},
	abstract = {Distributed and coordinated attacks in computer networks are causing considerable economic losses worldwide in recent years. This is mainly due to the transition of attackers’ operational patterns towards a more sophisticated and more global behavior. This fact is leading current intrusion detection systems to be more likely to generate false alarms. In this context, this paper describes the design of a collaborative intrusion detection network (CIDN) that is capable of building and sharing collective knowledge about isolated alarms in order to efficiently and accurately detect distributed attacks. It has been also strengthened with a reputation mechanism aimed to improve the detection coverage by dropping false or bogus alarms that arise from malicious or misbehaving nodes. This model will enable a CIDN to detect malicious behaviors according to the trustworthiness of the alarm issuers, calculated from previous interactions with the system. Experimental results will finally demonstrate how entities are gradually isolated as their behavior worsens throughout the time.},
	language = {en},
	number = {1},
	urldate = {2022-07-07},
	journal = {Journal of Network and Systems Management},
	author = {Gil Pérez, Manuel and Gómez Mármol, Félix and Martínez Pérez, Gregorio and Skarmeta Gómez, Antonio F.},
	month = mar,
	year = {2013},
	keywords = {Collaboration networks, Group reputation, Intrusion detection systems, Reputation systems, Security, Trust management},
	pages = {128--167},
}

@article{cordero_sphinx_2018,
	title = {Sphinx: a {Colluder}-{Resistant} {Trust} {Mechanism} for {Collaborative} {Intrusion} {Detection}},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Sphinx},
	doi = {10.1109/ACCESS.2018.2880297},
	abstract = {The destructive effects of cyber-attacks demand more proactive security approaches. One such promising approach is the idea of collaborative intrusion detection systems (CIDSs). These systems combine the knowledge of multiple sensors (e.g., intrusion detection systems, honeypots, or firewalls) to create a holistic picture of a monitored network. Sensors monitor parts of a network and exchange alert data to learn from each other, improve their detection capabilities and ultimately identify sophisticated attacks. Nevertheless, if one or a group of sensors is unreliable (due to incompetence or malice), the system might miss important information needed to detect attacks. In this paper, we propose Sphinx, an evidence-based trust mechanism capable of detecting unreliable sensors within a CIDS. The Sphinx detects, both, single sensors or coalitions of dishonest sensors that lie about the reliability of others to boost or worsen their trust score. Our evaluation shows that, given an honest majority of sensors, dishonesty is punished in a timely manner. Moreover, if several coalitions exist, even when more than 50\% of all sensors are dishonest, dishonesty is punished.},
	journal = {IEEE Access},
	author = {Cordero, Carlos Garcia and Traverso, Giulia and Nojoumian, Mehrdad and Habib, Sheikh Mahbub and Mühlhäuser, Max and Buchmann, Johannes and Vasilomanolakis, Emmanouil},
	year = {2018},
	note = {Conference Name: IEEE Access},
	keywords = {Clustering, Collaboration, Electronic mail, Intrusion detection, Machine learning, Monitoring, Reliability, Sensors, collaborative intrusion detection, machine learning, mixture models, sensor reliability, trust management},
	pages = {72427--72438},
}

@article{skopik_problem_2016,
	title = {A problem shared is a problem halved: {A} survey on the dimensions of collective cyber defense through security information sharing},
	volume = {60},
	issn = {01674048},
	doi = {10.1016/j.cose.2016.04.003},
	abstract = {The Internet threat landscape is fundamentally changing. A major shift away from hobby hacking toward well-organized cyber crime can be observed. These attacks are typically carried out for commercial reasons in a sophisticated and targeted manner, and specifically in a way to circumvent common security measures. Additionally, networks have grown to a scale and complexity, and have reached a degree of interconnectedness, that their protection can often only be guaranteed and financed as shared efforts. Consequently, new paradigms are required for detecting contemporary attacks and mitigating their effects. Today, many attack detection tasks are performed within individual organizations, and there is little cross-organizational information sharing. However, information sharing is a crucial step to acquiring a thorough understanding of large-scale cyber-attack situations, and is therefore seen as one of the key concepts to protect future networks. Discovering covert cyber attacks and new malware, issuing early warnings, advice about how to secure networks, and selectively distribute threat intelligence data are just some of the many use cases. In this survey article we provide a structured overview about the dimensions of cyber security information sharing. First, we motivate the need in more detail and work out the requirements for an information sharing system. Second, we highlight legal aspects and efforts from standardization bodies such as ISO and the National Institute of Standards and Technology (NIST). Third, we survey implementations in terms of both organizational and technological matters. In this regard, we study the structures of Computer Emergency Response Teams (CERTs) and Computer Security Incident Response Teams (CSIRTs), and evaluate what we could learn from them in terms of applied processes, available protocols and implemented tools. We conclude with a critical review of the state of the art and highlight important considerations when building effective security information sharing platforms for the future.},
	journal = {Computers \& Security},
	author = {Skopik, Florian and Settanni, Giuseppe and Fiedler, Roman},
	year = {2016},
	note = {Publisher: Elsevier Ltd},
	pages = {154--176},
}

@techreport{enisa_incentives_2010,
	title = {Incentives and {Challenges} for {Information} {Sharing} in the {Context} of {Network} and {Information} {Security}},
	url = {http://www.google.com/#sclient=psy&hl=en&safe=off&q=literature+review+information+sharing+law+enforcement&aq=f&aqi=&aql=&oq=&gs_rfai=&pbx=1&fp=9bef8cda26d1a6ec},
	abstract = {The importance of information sharing to ensuring network and information security is widely acknowledged by both policy-makers and by the technical and practitioner community – for example, in the European Programme on Critical Infrastructure Protection (EPCIP) and in the 2004 Availability and Robustness of Electronic Communications Infrastructures (ARECI) study, which noted that formal means for sharing information should be set up in order to ―improve the protection and rapid restoration of infrastructure critical to the reliability of communications within and throughout Europe‖. A 2009 gap analysis conducted by ENISA of good practice in respect of telecommunication network operators identified information sharing as a set of useful best practice. Given the acknowledged importance of information sharing, this report sets out findings from a research project into the barriers to and incentives for information sharing in the field of network and information security, in the context of peer-to-peer groups such as Information Exchanges (IE) and Information Sharing Analysis Centres (ISACs).{\textbackslash}nMethods and approach The information in this report is drawn from three sources: {\textbackslash}n A review of available literature – both academic and non-academic publications,  Interviews with key informants working in the field of network and information {\textbackslash}nsecurity and in IEs,  A two-round Delphi exercise with network and information security professionals. {\textbackslash}n The aim of this project is to identify those barriers and incentives which are most important in day-to-day practice in IEs and ISACs. This research differs from other work in this field in being firmly grounded in the experiences of practitioners and those involved in IE and Information Sharing activities. Nonetheless we only managed to speak to a limited number of experts from a handful of countries. Therefore, the findings of this research are a first step to developing an evidence base in this field, but we do not claim they are generalisable to all kinds of IEs. {\textbackslash}nIncentives and challenges for information sharing Our findings indicate that many of the barriers and incentives commonly identified in the {\textbackslash}navailable literature are of relatively low importance to practitioners and security officials currently working in IEs. As part of this research we asked practitioners to rank a list of barriers and incentives in terms of their relative importance. Our findings indicate that the incentives which are most important are: {\textbackslash}n Economic incentives stemming from cost savings;  Incentives stemming from the quality, value, and use of information shared. {\textbackslash}n While the barriers which are the most important are: {\textbackslash}n Poor quality information;  Misaligned economic incentives stemming from reputational risks;  Poor management.},
	author = {{ENISA}},
	year = {2010},
	note = {Volume: 10},
	pages = {52},
}

@inproceedings{ur_rehman_towards_2020,
	address = {Toronto, ON, Canada},
	title = {Towards {Blockchain}-{Based} {Reputation}-{Aware} {Federated} {Learning}},
	isbn = {978-1-72818-695-5},
	url = {https://ieeexplore.ieee.org/document/9163027/},
	doi = {10.1109/INFOCOMWKSHPS50562.2020.9163027},
	abstract = {Federated learning (FL) is the collaborative machine learning (ML) technique whereby the devices collectively train and update a shared ML model while preserving their personal datasets. FL systems solve the problems of communicationefﬁciency, bandwidth-optimization, and privacy-preservation. Despite the potential beneﬁts of FL, one centralized shared ML model across all the devices produce coarse-grained predictions which, in essence, are not required in many application areas involving personalized prediction services. In this paper, we present a novel concept of ﬁne-grained FL to decentralize the shared ML models on the edge servers. We then present a formal extended deﬁnition of ﬁne-grained FL process in mobile edge computing systems. In addition, we deﬁne the core requirements of ﬁnegrained FL systems including personalization, decentralization, ﬁne-grained FL, incentive mechanisms, trust, activity monitoring, heterogeneity and context-awareness, model synchronization, and communication and bandwidth-efﬁciency. Moreover, we present the concept of blockchain-based reputation-aware ﬁne-grained FL in order to ensure trustworthy collaborative training in mobile edge computing systems. Finally, we perform the qualitative comparison of proposed approach with state-of-the-art related work and found some promising initial results.},
	language = {en},
	urldate = {2021-10-29},
	booktitle = {{IEEE} {INFOCOM} 2020 - {IEEE} {Conference} on {Computer} {Communications} {Workshops} ({INFOCOM} {WKSHPS})},
	publisher = {IEEE},
	author = {ur Rehman, Muhammad Habib and Salah, Khaled and Damiani, Ernesto and Svetinovic, Davor},
	month = jul,
	year = {2020},
	keywords = {\_read},
	pages = {183--188},
}

@inproceedings{wang_flare_2022,
	address = {Nagasaki Japan},
	title = {{FLARE}: {Defending} {Federated} {Learning} against {Model} {Poisoning} {Attacks} via {Latent} {Space} {Representations}},
	isbn = {978-1-4503-9140-5},
	shorttitle = {{FLARE}},
	url = {https://dl.acm.org/doi/10.1145/3488932.3517395},
	doi = {10.1145/3488932.3517395},
	abstract = {Federated learning (FL) has been shown vulnerable to a new class of adversarial attacks, known as model poisoning attacks (MPA), where one or more malicious clients try to poison the global model by sending carefully crafted local model updates to the central parameter server. Existing defenses that have been fixated on analyzing model parameters show limited effectiveness in detecting such carefully crafted poisonous models. In this work, we propose FLARE, a robust model aggregation mechanism for FL, which is resilient against state-of-the-art MPAs. Instead of solely depending on model parameters, FLARE leverages the penultimate layer representations (PLRs) of the model for characterizing the adversarial influence on each local model update. PLRs demonstrate a better capability to differentiate malicious models from benign ones than model parameter-based solutions. We further propose a trust evaluation method that estimates a trust score for each model update based on pairwise PLR discrepancies among all model updates. Under the assumption that honest clients make up the majority, FLARE assigns a trust score to each model update in a way that those far from the benign cluster are assigned low scores. FLARE then aggregates the model updates weighted by their trust scores and finally updates the global model. Extensive experimental results demonstrate the effectiveness of FLARE in defending FL against various MPAs, including semantic backdoor attacks, trojan backdoor attacks, and untargeted attacks, and safeguarding the accuracy of FL.},
	language = {en},
	urldate = {2022-07-05},
	booktitle = {Proceedings of the 2022 {ACM} on {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Wang, Ning and Xiao, Yang and Chen, Yimin and Hu, Yang and Lou, Wenjing and Hou, Y. Thomas},
	month = may,
	year = {2022},
	pages = {946--958},
}

@article{kang_reliable_2020,
	title = {Reliable {Federated} {Learning} for {Mobile} {Networks}},
	volume = {27},
	issn = {1558-0687},
	doi = {10.1109/MWC.001.1900119},
	abstract = {Federated learning, as a promising machine learning approach, has emerged to leverage a distributed personalized dataset from a number of nodes, for example, mobile devices, to improve performance while simultaneously providing privacy preservation for mobile users. In federated learning, training data is widely distributed and maintained on the mobile devices as workers. A central aggregator updates a global model by collecting local updates from mobile devices using their local training data to train the global model in each iteration. However, unreliable data may be uploaded by the mobile devices (i.e., workers), leading to frauds in tasks of federated learning. The workers may perform unreliable updates intentionally, for example, the data poisoning attack, or unintentionally, for example, low-quality data caused by energy constraints or high-speed mobility. Therefore, finding out trusted and reliable workers in federated learning tasks becomes critical. In this article, the concept of reputation is introduced as a metric. Based on this metric, a reliable worker selection scheme is proposed for federated learning tasks. Consortium blockchain is leveraged as a decentralized approach for achieving efficient reputation management of the workers without repudiation and tampering. By numerical analysis, the proposed approach is demonstrated to improve the reliability of federated learning tasks in mobile networks.},
	number = {2},
	journal = {IEEE Wireless Communications},
	author = {Kang, Jiawen and Xiong, Zehui and Niyato, Dusit and Zou, Yuze and Zhang, Yang and Guizani, Mohsen},
	month = apr,
	year = {2020},
	note = {Conference Name: IEEE Wireless Communications},
	keywords = {Data models, Data privacy, Machine learning, Metasearch, Mobile handsets, Task analysis, Training data, \_obsidian},
	pages = {72--80},
}

@article{kang_incentive_2019,
	title = {Incentive {Mechanism} for {Reliable} {Federated} {Learning}: {A} {Joint} {Optimization} {Approach} to {Combining} {Reputation} and {Contract} {Theory}},
	volume = {6},
	issn = {2327-4662},
	shorttitle = {Incentive {Mechanism} for {Reliable} {Federated} {Learning}},
	doi = {10.1109/JIOT.2019.2940820},
	abstract = {Federated learning is an emerging machine learning technique that enables distributed model training using local datasets from large-scale nodes, e.g., mobile devices, but shares only model updates without uploading the raw training data. This technique provides a promising privacy preservation for mobile devices while simultaneously ensuring high learning performance. The majority of existing work has focused on designing advanced learning algorithms with an aim to achieve better learning performance. However, the challenges, such as incentive mechanisms for participating in training and worker (i.e., mobile devices) selection schemes for reliable federated learning, have not been explored yet. These challenges have hindered the widespread adoption of federated learning. To address the above challenges, in this article, we first introduce reputation as the metric to measure the reliability and trustworthiness of the mobile devices. We then design a reputation-based worker selection scheme for reliable federated learning by using a multiweight subjective logic model. We also leverage the blockchain to achieve secure reputation management for workers with nonrepudiation and tamper-resistance properties in a decentralized manner. Moreover, we propose an effective incentive mechanism combining reputation with contract theory to motivate high-reputation mobile devices with high-quality data to participate in model learning. Numerical results clearly indicate that the proposed schemes are efficient for reliable federated learning in terms of significantly improving the learning accuracy.},
	number = {6},
	journal = {IEEE Internet of Things Journal},
	author = {Kang, Jiawen and Xiong, Zehui and Niyato, Dusit and Xie, Shengli and Zhang, Junshan},
	month = dec,
	year = {2019},
	keywords = {Blockchain, Contracts, Data models, Mobile handsets, Reliability, Task analysis, Training, contract theory, federated learning, mobile networks, reputation, security and privacy},
	pages = {10700--10714},
}

@article{hu_contribution-_2022,
	title = {Contribution- and {Participation}-based {Federated} {Learning} on non-{IID} {Data}},
	issn = {1541-1672, 1941-1294},
	url = {https://ieeexplore.ieee.org/document/9760086/},
	doi = {10.1109/MIS.2022.3168298},
	abstract = {The learning process takes place inside clients in federated learning (FL). How to effectively motivate clients and avoid the impact of statistical heterogeneity are challenges in FL. This paper proposes contribution- and participationbased federated learning (CPFL) to address these challenges. CPFL can effectively allocate client incentives and aggregate models according to client contribution ratios, by which it can reduce the impact of heterogeneous data. To get effective and approximately fair client contributions faster, we propose an extended Raiffa solution (ERS). Compared to the conventional solution Shapley Value, the time complexity of ERS goes from O(2n) down to O(n). We perform extensive experiments with the MNIST/EMNIST datasets, heterogeneous datasets, and with different ratios of participation reward. Experimental results demonstrate that CPFL generally has a better learning effect in the heterogeneous case.},
	language = {en},
	urldate = {2022-07-05},
	journal = {IEEE Intelligent Systems},
	author = {Hu, Fei and Zhou, Wuneng and Liao, Kaili and Li, Hongliang},
	year = {2022},
	pages = {1--1},
}

@article{kumar_fedclean_nodate,
	title = {{FEDCLEAN}: {A} {DEFENSE} {MECHANISM} {AGAINST} {PARAMETER} {POISONING} {ATTACKS} {IN} {FEDERATED} {LEARNING}},
	doi = {10.1109/ICASSP43922.2022.9747497},
	abstract = {In Federated learning (FL) systems, a centralized entity (server), instead of access to the training data, has access to model parameter updates computed by each participant independently and based solely on their samples. Unfortunately, FL is susceptible to model poisoning attacks, in which malicious or malfunctioning entities share polluted updates that can compromise the model’s accuracy. In this study, we propose FedClean, an FL mechanism that is robust to model poisoning attacks. The accuracy of the models trained with the assistance of FedClean is close to the one where malicious entities do not participate.},
	language = {en},
	author = {Kumar, Abhishek and Khimani, Vivek and Chatzopoulos, Dimitris and Hui, Pan},
	keywords = {agent selection, cosin similarity},
	pages = {5},
}

@inproceedings{wang_reputation-enabled_2021,
	title = {Reputation-enabled {Federated} {Learning} {Model} {Aggregation} in {Mobile} {Platforms}},
	doi = {10.1109/ICC42927.2021.9500928},
	abstract = {Federated Learning (FL) builds on a mobile network of participating nodes that train local models and contribute to the learning model parameters at a central server without being obliged to share their raw data. The server aggregates the uploaded model parameters to generate a global model. Common practice for the uploaded local models is an evenly weighted aggregation, assuming that each node of the network contributes to advancing the global model equally. Due to the heterogeneous nature of the devices and collected data, it is inevitable to have variations between the contributions of the users to the global model. Therefore, users (i.e., devices) with higher contributions should be weighted higher during aggregation. With this in mind, this paper proposes a reputation-enabled aggregation methodology that scales the aggregation weights of users by their reputation scores. Reputation score of a user is computed according to the performance metrics of their trained local models during each training round, therefore it can be a metric to evaluate the direct contributions of their trained local model. Numerical comparison of the proposed aggregation methodology to a baseline that utilizes standard averaging as well as a second baseline that is scoped to a reputation-based client selection shows an improvement of 17.175\% over the standard baseline for not independent and identically distributed (non-IID) scenarios for an FL network of 100 participants. Consistent improvements over the first and second baselines under smaller FL networks with users ranging from 20 to 100 are also shown.},
	booktitle = {{ICC} 2021 - {IEEE} {International} {Conference} on {Communications}},
	author = {Wang, Yuwei and Kantarci, Burak},
	month = jun,
	year = {2021},
	note = {ISSN: 1938-1883},
	keywords = {Collaborative work, Computational modeling, Data aggregation, Data models, Deep Learning, Deep Neural Networks, Distance measurement, Distributed Learning, Federated Learning, Mobile Networks, Neural networks, Reputation systems, Training},
	pages = {1--6},
}

@misc{fung_mitigating_2020,
	title = {Mitigating {Sybils} in {Federated} {Learning} {Poisoning}},
	url = {http://arxiv.org/abs/1808.04866},
	doi = {10.48550/arXiv.1808.04866},
	abstract = {Machine learning (ML) over distributed multi-party data is required for a variety of domains. Existing approaches, such as federated learning, collect the outputs computed by a group of devices at a central aggregator and run iterative algorithms to train a globally shared model. Unfortunately, such approaches are susceptible to a variety of attacks, including model poisoning, which is made substantially worse in the presence of sybils. In this paper we first evaluate the vulnerability of federated learning to sybil-based poisoning attacks. We then describe {\textbackslash}emph\{FoolsGold\}, a novel defense to this problem that identifies poisoning sybils based on the diversity of client updates in the distributed learning process. Unlike prior work, our system does not bound the expected number of attackers, requires no auxiliary information outside of the learning process, and makes fewer assumptions about clients and their data. In our evaluation we show that FoolsGold exceeds the capabilities of existing state of the art approaches to countering sybil-based label-flipping and backdoor poisoning attacks. Our results hold for different distributions of client data, varying poisoning targets, and various sybil strategies. Code can be found at: https://github.com/DistributedML/FoolsGold},
	urldate = {2022-08-09},
	publisher = {arXiv},
	author = {Fung, Clement and Yoon, Chris J. M. and Beschastnikh, Ivan},
	month = jul,
	year = {2020},
	note = {arXiv:1808.04866 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{cao_fltrust_2022,
	title = {{FLTrust}: {Byzantine}-robust {Federated} {Learning} via {Trust} {Bootstrapping}},
	shorttitle = {{FLTrust}},
	url = {http://arxiv.org/abs/2012.13995},
	doi = {10.48550/arXiv.2012.13995},
	abstract = {Byzantine-robust federated learning aims to enable a service provider to learn an accurate global model when a bounded number of clients are malicious. The key idea of existing Byzantine-robust federated learning methods is that the service provider performs statistical analysis among the clients' local model updates and removes suspicious ones, before aggregating them to update the global model. However, malicious clients can still corrupt the global models in these methods via sending carefully crafted local model updates to the service provider. The fundamental reason is that there is no root of trust in existing federated learning methods. In this work, we bridge the gap via proposing FLTrust, a new federated learning method in which the service provider itself bootstraps trust. In particular, the service provider itself collects a clean small training dataset (called root dataset) for the learning task and the service provider maintains a model (called server model) based on it to bootstrap trust. In each iteration, the service provider first assigns a trust score to each local model update from the clients, where a local model update has a lower trust score if its direction deviates more from the direction of the server model update. Then, the service provider normalizes the magnitudes of the local model updates such that they lie in the same hyper-sphere as the server model update in the vector space. Our normalization limits the impact of malicious local model updates with large magnitudes. Finally, the service provider computes the average of the normalized local model updates weighted by their trust scores as a global model update, which is used to update the global model. Our extensive evaluations on six datasets from different domains show that our FLTrust is secure against both existing attacks and strong adaptive attacks.},
	urldate = {2022-08-09},
	publisher = {arXiv},
	author = {Cao, Xiaoyu and Fang, Minghong and Liu, Jia and Gong, Neil Zhenqiang},
	month = apr,
	year = {2022},
	note = {arXiv:2012.13995 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Distributed, Parallel, and Cluster Computing},
}

@inproceedings{xia_tofi_2021,
	address = {Cham},
	series = {Lecture {Notes} of the {Institute} for {Computer} {Sciences}, {Social} {Informatics} and {Telecommunications} {Engineering}},
	title = {{ToFi}: {An} {Algorithm} to {Defend} {Against} {Byzantine} {Attacks} in {Federated} {Learning}},
	isbn = {978-3-030-90019-9},
	shorttitle = {{ToFi}},
	doi = {10.1007/978-3-030-90019-9_12},
	abstract = {In distributed gradient descent based machine learning model training, workers periodically upload locally computed gradients or weights to the parameter server (PS). Byzantine attacks take place when some workers upload wrong gradients or weights, i.e., the information received by the PS is not always the true values computed by workers. Approaches such as score-based, median-based, and distance-based defense algorithms were proposed previously, but all of them made the asumptions: (1) the dataset on each worker is independent and identically distributed (i.i.d.), and (2) the majority of all participating workers are honest. These assumptions are not realistic in federated learning where each worker may keep its non-i.i.d. private dataset and malicious workers may take over the majority in some iterations. In this paper, we propose a novel reference dataset based algorithm along with a practical Two-Filter algorithm (ToFi) to defend against Byzantine attacks in federated learning. Our experiments highlight the effectiveness of our algorithm compared with previous algorithms in different settings.},
	language = {en},
	booktitle = {Security and {Privacy} in {Communication} {Networks}},
	publisher = {Springer International Publishing},
	author = {Xia, Qi and Tao, Zeyi and Li, Qun},
	editor = {Garcia-Alfaro, Joaquin and Li, Shujun and Poovendran, Radha and Debar, Hervé and Yung, Moti},
	year = {2021},
	keywords = {Byzantine attacks, Federated learning},
	pages = {229--248},
}

@article{ng_reputation-aware_2022,
	title = {Reputation-{Aware} {Hedonic} {Coalition} {Formation} for {Efficient} {Serverless} {Hierarchical} {Federated} {Learning}},
	volume = {33},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2021.3139039},
	abstract = {Amid growing concerns on data privacy, Federated Learning (FL) has emerged as a promising privacy preserving distributed machine learning paradigm. Given that the FL network is expected to be implemented at scale, several studies have proposed system architectures towards improving the network scalability and efficiency. Specifically, the Hierarchical FL (HFL) network utilizes cluster heads, e.g., base stations, for the intermediate aggregation and relay of model parameters. Serverless FL is also proposed recently, in which the data owners, i.e., workers, exchange the local model parameters among a neighborhood of workers. This decentralized approach reduces the risk of a single point of failure but inevitably incurs significant communication overheads. To achieve the best of both worlds, we propose the Serverless Hierarchical Federated Learning (SHFL) framework in this article. The SHFL framework adopts a two-layer system architecture. In the lower layer, the FL workers are grouped into clusters under cluster heads. In the upper layer, the cluster heads exchange the intermediate parameters with their one-hop neighbors without the aid of a central server. To improve the sustainable efficiency of the FL system while taking into account the incentive design for workers’ marginal contributions in the system, we propose the reputation-aware hedonic coalition formation game in this article. Specifically, the workers are rewarded for their marginal contribution to the cluster, whereas the reputation opinions of each cluster head is updated in a decentralized manner, thereby deterring malicious behaviors by the cluster head. This improves the performance of the network since cluster heads with higher reputation scores are more reliable in relaying the intermediate model parameters. The simulation results show that our proposed hedonic coalition formation algorithm converges to a Nash-stable partition and improves the network efficiency.},
	number = {11},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Ng, Jer Shyuan and Lim, Wei Yang Bryan and Xiong, Zehui and Cao, Xianbin and Jin, Jiangming and Niyato, Dusit and Leung, Cyril and Miao, Chunyan},
	month = nov,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {Base stations, Collaborative work, Computational modeling, Costs, Federated learning, Magnetic heads, Servers, Training, decentralized edge intelligence, hedonic coalition formation, serverless federated learning},
	pages = {2675--2686},
}

@inproceedings{wang_novel_2020,
	title = {A {Novel} {Reputation}-aware {Client} {Selection} {Scheme} for {Federated} {Learning} within {Mobile} {Environments}},
	doi = {10.1109/CAMAD50429.2020.9209263},
	abstract = {This paper studies the problem of training federated deep learning models over a mobile environment. Stemming from the federated learning (FL) concept, deep learning models on mobile devices can be trained for various use cases including but not limited to image sorting and prediction of upcoming words. Mobile devices have access to rich data sets through embedded sensors and as well as installed software, and these feature rich data can facilitate solid training models, including personal images and other behaviometric features. However, utilizing the data through conventional approaches can potentially lead to privacy leakages. In this paper, we propose an alternate strategy that builds on the Federated Learning (FL) concept, to keep the training data on distributed mobile devices, and train a shared model by aggregating updated local models. The contribution of this study is an optimal user selection method for the federated learning environment based on reputation scores. Through extensive validation experiments considering two different model architectures and three datasets, our experiments show that the proposed approach is stable over data that is not independent nor identically distributed (i.e., non-IID) and under imbalanced distribution. Experimental results show that the proposed reputation-aware FL scheme can achieve improvements in the test accuracy up to 9.30\% under different data sets.},
	booktitle = {2020 {IEEE} 25th {International} {Workshop} on {Computer} {Aided} {Modeling} and {Design} of {Communication} {Links} and {Networks} ({CAMAD})},
	author = {Wang, Yuwei and Kantarci, Burak},
	month = sep,
	year = {2020},
	note = {ISSN: 2378-4873},
	keywords = {Computational modeling, Data models, Data privacy, Federated learning, Machine learning, Mobile handsets, Servers, Training, client selection, data sharing, deep learning models, mobile networks},
	pages = {1--6},
}

@article{chen_zero_2021,
	title = {Zero {Knowledge} {Clustering} {Based} {Adversarial} {Mitigation} in {Heterogeneous} {Federated} {Learning}},
	volume = {8},
	issn = {2327-4697},
	doi = {10.1109/TNSE.2020.3002796},
	abstract = {The simultaneous development of deep learning techniques and Internet of Things (IoT)/Cyber-physical Systems (CPS) technologies has afforded untold possibilities for improving distributed computing, sensing, and data analysis. Among these technologies, federated learning has received increased attention as a privacy-preserving collaborative learning paradigm, and has shown significant potential in IoT/CPS-driven large-scale smart-world systems. At the same time, the vulnerabilities of deep neural networks, especially to adversarial attacks, cannot be overstated and should not be minimized. Moreover, the distributed nature of federated learning makes defense against such adversarial attacks a more challenging problem due to the unavailability of local data and resource heterogeneity. To tackle these challenges, in this paper, we propose ZeKoC, a Zero Knowledge Clustering approach to mitigating adversarial attacks. Particularly, we first formulate the problem of resource-constrained adversarial mitigation. Specifically, noting that a global server has no access to training samples, we reformulate the unsupervised weight clustering problem. Our proposed ZeKoC approach allows the server to automatically split and merge weight clusters for weight selection and aggregation. Theoretical analysis demonstrates that convergence is guaranteed. Further, our experimental results illustrate that, in a non-i.i.d. (i.e., independent and identically distributed) data setting, the proposed ZeKoC approach successfully mitigates general attacks while outperforming state-of-art schemes.},
	number = {2},
	journal = {IEEE Transactions on Network Science and Engineering},
	author = {Chen, Zheyi and Tian, Pu and Liao, Weixian and Yu, Wei},
	month = apr,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Network Science and Engineering},
	keywords = {Data models, Distributed databases, Machine learning, Non-i.i.d. data, Peer-to-peer computing, Security, Servers, Training, adversarial mitigation, federated learning},
	pages = {1070--1083},
}

@inproceedings{briggs_federated_2020,
	title = {Federated learning with hierarchical clustering of local updates to improve training on non-{IID} data},
	doi = {10.1109/IJCNN48605.2020.9207469},
	abstract = {Federated learning (FL) is a well established method for performing machine learning tasks over massively distributed data. However in settings where data is distributed in a non-iid (not independent and identically distributed) fashion - as is typical in real world situations - the joint model produced by FL suffers in terms of test set accuracy and/or communication costs compared to training on iid data. We show that learning a single joint model is often not optimal in the presence of certain types of non-iid data. In this work we present a modification to FL by introducing a hierarchical clustering step (FL+HC) to separate clusters of clients by the similarity of their local updates to the global joint model. Once separated, the clusters are trained independently and in parallel on specialised models. We present a robust empirical analysis of the hyperparameters for FL+HC for several iid and non-iid settings. We show how FL+HC allows model training to converge in fewer communication rounds (significantly so under some non-iid settings) compared to FL without clustering. Additionally, FL+HC allows for a greater percentage of clients to reach a target accuracy compared to standard FL. Finally we make suggestions for good default hyperparameters to promote superior performing specialised models without modifying the the underlying federated learning communication protocol.},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Briggs, Christopher and Fan, Zhong and Andras, Peter},
	month = jul,
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {Cats, Clustering algorithms, Data models, Distributed databases, Merging, Task analysis, Training, clustering applications, distributed machine learning, federated learning},
	pages = {1--9},
}

@inproceedings{manyadza_fl-finder_2022,
	title = {{FL}-finder: {Detecting} {Unknown} {Network} {Anomaly} in {Federated} {Learning}},
	shorttitle = {{FL}-finder},
	doi = {10.1109/ICAIBD55127.2022.9820480},
	abstract = {The emergence of federated learning has ensured data and privacy security in deep learning models while enabling models to train more efficiently. However, the transmission of network parameters in federated learning may be subject to attacks by unknown anomalies. In this paper, we attempted to detect unknown anomalies in transmitted parameters in federated learning. We designed and implemented F1-finder, an unknown network anomaly detection framework in federated learning, which detects anomalies based on incremental learning. It retains the unknown anomalies to its prior knowledge base using the network updater, and adopts an online mode that reports new anomalies in a real-time. Extensive experimental results show that our model increased the average accuracy of unknown anomaly detection by 10.4\% and the average F1-Score improved to 19\%.},
	booktitle = {2022 5th {International} {Conference} on {Artificial} {Intelligence} and {Big} {Data} ({ICAIBD})},
	author = {Manyadza, Tinashe Justice and Du, Haizhou and Wang, Shiwei and Yang, Wenbin and Chen, Cheng and Tian, Fei},
	month = may,
	year = {2022},
	keywords = {Collaborative work, Data models, Detectors, Energy consumption, Federated Learning, Incremental learning, Knowledge based systems, Learning (artificial intelligence), Prior Knowledge, Real-time systems, Unknown Anomaly Detection},
	pages = {593--597},
}

@inproceedings{liu_federated_2022,
	title = {Federated {Learning} with {Anomaly} {Client} {Detection} and {Decentralized} {Parameter} {Aggregation}},
	doi = {10.1109/DSN-W54100.2022.00016},
	abstract = {Federated learning is a framework for machine learning that is dedicated to data privacy protection. In federated learning, system cannot fully control the behavior of clients which can be faulty. These behaviors include sharing arbitrary faulty gradients and delaying the process of sharing due to Byzantine attacks or clients’ own software and hardware failures. In federated learning, the parameter server may also be faulty during gradient collection and aggregation, mainly including gradient-based training data inference and model parameter faulty update. The above problems may lead to reduced accuracy of federated learning model training, leakage of client privacy, etc. Existing research enhances the robustness of federated learning by exploiting the decentralization and immutability of Blockchain. For untrusted clients, most research is based on Byzantine fault tolerance to defend against clients indiscriminately, and may cause model accuracy reduction. In addition, most of the research focus on unencrypted gradients, and there is insufficient research on dealing with client anomalies in the case of gradient encryption. For untrusted parameter servers, existing research has problems in energy overhead and scalability. Aiming at the problems above, this paper studies the robustness of federated learning, and proposes a blockchain-based federated learning parameter update architecture PUS-FL. Through experiments simulating distributed machine learning on neural networks, we demonstrate that the anomaly detection algorithm of PUS-FL outperforms conventional gradient filters including geometric median, Multi-Krum and trimmed mean. In addition, our experiments also verify that the scalability-enhanced parameter aggregation consensus algorithm proposed in this paper(SE-PBFT) improves consensus scalability by reducing communication complexity.},
	booktitle = {2022 52nd {Annual} {IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} {Workshops} ({DSN}-{W})},
	author = {Liu, Shu and Shang, Yanlei},
	month = jun,
	year = {2022},
	note = {ISSN: 2325-6664},
	keywords = {Blockchain, Byzantine Attack, Collaborative work, Consensus Algorithm, Fault tolerant systems, Federated learning, Inference algorithms, Machine learning, Privacy, Robustness, Scalability, Trusted Computing},
	pages = {37--43},
}

@article{tan_reputation-aware_2022,
	title = {Reputation-{Aware} {Federated} {Learning} {Client} {Selection} based on {Stochastic} {Integer} {Programming}},
	issn = {2332-7790},
	doi = {10.1109/TBDATA.2022.3191332},
	abstract = {Federated Learning(FL) has attracted wide research interest due to its potential in building machine learning models while preserving users' data privacy. However, due to the distributive nature of FL, it is vulnerable to misbehavior from participating worker nodes. Thus, it is important to select clients to participate in FL. Recent studies on FL client selection focus on the perspective of improving model training efficiency and performance, without holistically considering potential misbehavior and the cost of hiring. To bridge this gap, we propose a first-of-its-kind reputation-aware Stochastic integer programming-based FL Client Selection method (SCS). It can optimally select and compensate clients with different reputation profiles. Extensive experiments show that SCS achieves the most advantageous performance-cost trade-off compared to other existing state-of-the-art approaches.},
	journal = {IEEE Transactions on Big Data},
	author = {Tan, Xavier and Ng, Wei Chong and Lim, Wei Yang Bryan and Xiong, Zehui and Niyato, Dusit and Yu, Han},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Big Data},
	keywords = {Biological system modeling, Computational modeling, Costs, Data models, Federated learning, Stochastic processes, Training, Uncertainty, client selection, reputation, stochastic integer programming},
	pages = {1--12},
}

@article{deng_improving_2022,
	title = {Improving {Federated} {Learning} {With} {Quality}-{Aware} {User} {Incentive} and {Auto}-{Weighted} {Model} {Aggregation}},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2022.3195207},
	abstract = {Federated learning enables distributed model training over various computing nodes, e.g., mobile devices, where instead of sharing raw user data, computing nodes can solely commit model updates without compromising data privacy. The quality of federated learning relies on the model updates contributed by computing nodes training with their local data. However, with various factors (e.g., training data size, mislabeled data samples, skewed data distributions), the model update qualities of computing nodes can vary dramatically, while inclusively aggregating low-quality model updates can deteriorate the global model quality. To achieve efficient federated learning, in this paper, we propose a novel framework named FAIR, i.e., Federated leArning with qualIty awaReness. Particularly, FAIR integrates three major components: 1) learning quality estimation: we adopt the model aggregation weight (learned in the third component) to reversely quantify the individual learning quality of nodes in a privacy-preserving manner, and leverage the historical learning records to infer the next-round learning quality; 2) quality-aware incentive mechanism: within the recruiting budget, we model a reverse auction problem to stimulate the participation of high-quality and low-cost computing nodes, and the method is proved to be truthful, individually rational, and computationally efficient; and 3) auto-weighted model aggregation: based on the gradient descent method, we devise an auto-weighted model aggregation algorithm to automatically learn the optimal aggregation weights to further enhance the global model quality. Based on real-world datasets and learning tasks, extensive experiments are conducted to demonstrate the efficacy of FAIR.},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Deng, Yongheng and Lyu, Feng and Ren, Ju and Chen, Yi-Chao and Yang, Peng and Zhou, Yuezhi and Zhang, Yaoxue},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {Collaborative work, Computational modeling, Data models, Edge computing, Resource management, Task analysis, Training, Training data, federated learning, incentive mechanism, learning quality, mobile computing, model aggregation},
	pages = {1--15},
}

@article{Chaabouni2019,
	title = {Network {Intrusion} {Detection} for {IoT} {Security} {Based} on {Learning} {Techniques}},
	volume = {21},
	issn = {1553-877X},
	url = {https://ieeexplore.ieee.org/document/8629941/},
	doi = {10.1109/COMST.2019.2896380},
	abstract = {Pervasive growth of Internet of Things (IoT) is visible across the globe. The 2016 Dyn cyberattack exposed the critical fault-lines among smart networks. Security of IoT has become a critical concern. The danger exposed by infested Internet-connected Things not only affects the security of IoT but also threatens the complete Internet eco-system which can possibly exploit the vulnerable Things (smart devices) deployed as botnets. Mirai malware compromised the video surveillance devices and paralyzed Internet via distributed denial of service attacks. In the recent past, security attack vectors have evolved bothways, in terms of complexity and diversity. Hence, to identify and prevent or detect novel attacks, it is important to analyze techniques in IoT context. This survey classifies the IoT security threats and challenges for IoT networks by evaluating existing defense techniques. Our main focus is on network intrusion detection systems (NIDSs); hence, this paper reviews existing NIDS implementation tools and datasets as well as free and open-source network sniffing software. Then, it surveys, analyzes, and compares state-of-the-art NIDS proposals in the IoT context in terms of architecture, detection methodologies, validation strategies, treated threats, and algorithm deployments. The review deals with both traditional and machine learning (ML) NIDS techniques and discusses future directions. In this survey, our focus is on IoT NIDS deployed via ML since learning algorithms have a good success rate in security and privacy. The survey provides a comprehensive review of NIDSs deploying different aspects of learning techniques for IoT, unlike other top surveys targeting the traditional systems. We believe that, this paper will be useful for academia and industry research, first, to identify IoT threats and challenges, second, to implement their own NIDS and finally to propose new smart techniques in IoT context considering IoT limitations. Moreover, the survey will enable security individuals differentiate IoT NIDS from traditional ones.},
	number = {3},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Chaabouni, Nadia and Mosbah, Mohamed and Zemmari, Akka and Sauvignac, Cyrille and Faruki, Parvez},
	year = {2019},
	note = {Publisher: IEEE},
	pages = {2671--2701},
}

@article{rathore_blockseciotnet_2019,
	title = {{BlockSecIoTNet}: {Blockchain}-based decentralized security architecture for {IoT} network},
	volume = {143},
	issn = {10848045},
	url = {https://doi.org/10.1016/j.jnca.2019.06.019},
	doi = {10.1016/j.jnca.2019.06.019},
	abstract = {The exponential growth of the use of insecure stationary and portable devices in the Internet of Things (IoT) network of the smart city has made the security of the smart city against cyber-attacks a vital issue. Various mechanisms for detecting security attacks that rely on centralized and distributed architectures have already been proposed, but they tend to be inefficient due to such problems as storage constraints, the high cost of computation, high latency, and a single point of failure. Moreover, existing security mechanisms are faced with the issue of monitoring and collecting historic data throughout the entire IoT network of the smart city in order to deliver optimal security and defense against cyberattacks. To address the current challenges, this paper proposes a decentralized security architecture based on Software Defined Networking (SDN) coupled with a blockchain technology for IoT network in the smart city that relies on the three core technologies of SDN, Blockchain, and Fog and mobile edge computing in order to detect attacks in the IoT network more effectively. Thus, in the proposed architecture, SDN is liable to continuous monitoring and analysis of traffic data in the entire IoT network in order to provide an optimal attack detection model; Blockchain delivers decentralized attack detection to mitigate the “single point of failure” problem inherent to the existing architecture; and Fog and mobile edge computing supports attack detection at the fog node and, subsequently, attack mitigation at the edge node, thus enabling early detection and mitigation with lesser storage constraints, cheaper computation, and low latency. To validate the performance of the proposed architecture, it was subjected to an experimental evaluation, the results of which show that it outperforms both centralized and distributed architectures in terms of accuracy and detection time.},
	number = {December 2018},
	journal = {Journal of Network and Computer Applications},
	author = {Rathore, Shailendra and Wook Kwon, Byung and Park, Jong Hyuk},
	month = oct,
	year = {2019},
	note = {Publisher: Elsevier Ltd},
	keywords = {\_processed},
	pages = {167--177},
}

@article{alkhalidy_new_2022,
	title = {A {New} {Scheme} for {Detecting} {Malicious} {Nodes} in {Vehicular} {Ad} {Hoc} {Networks} {Based} on {Monitoring} {Node} {Behavior}},
	doi = {10.3390/fi14080223},
	abstract = {Vehicular ad hoc networks have played a key role in intelligent transportation systems that considerably improve road safety and management. This new technology allows vehicles to communicate and share road information. However, malicious users may inject false emergency alerts into vehicular ad hoc networks, preventing nodes from accessing accurate road information. In order to assure the reliability and trustworthiness of information through the networks, assessing the credibility of nodes has become a critical task in vehicular ad hoc networks. A new scheme for malicious node detection is proposed in this work. Multiple factors are fed into a fuzzy logic model for evaluating the trust for each node. Vehicles are divided into clusters in our approach, and a road side unit manages each cluster. The road side unit assesses the credibility of nodes before accessing vehicular ad hoc networks. The road side unit evicts a malicious node based on trust value. Simulations are used to validate our technique. We demonstrate that our scheme can detect and evict all malicious nodes in the vehicular ad hoc network over time, lowering the ratio of malicious nodes. Furthermore, it has a positive impact on selﬁsh node participation. The scheme increases the success rate of delivered data to the same level as the ideal cases when no selﬁsh node is present.},
	language = {en},
	journal = {Future Internet},
	author = {Alkhalidy, Muhsen and Al-Serhan, Atalla Fahed and Alsarhan, Ayoub and Igried, Bashar},
	year = {2022},
	pages = {11},
}

@inproceedings{song_profit_2019,
	title = {Profit {Allocation} for {Federated} {Learning}},
	doi = {10.1109/BigData47090.2019.9006327},
	abstract = {Due to stricter data management regulations such as General Data Protection Regulation (GDPR), traditional production mode of machine learning services is shifting to federated learning, a paradigm that allows multiple data providers to train a joint model collaboratively with their data kept locally. A key enabler for practical adoption of federated learning is how to allocate the prolit earned by the joint model to each data provider. For fair prolit allocation, a metric to quantify the contribution of each data provider to the joint model is essential. Shapley value is a classical concept in cooperative game theory which assigns a unique distribution (among the players) of a total surplus generated by the coalition of all players and has been used for data valuation in machine learning services. However, prior Shapley value based data valuation schemes either do not apply to federated learning or involve extra model training which leads to high cost. In this paper, given n data providers with data sets D1, D2, ⋯, Dn, a federated learning algorithm A and a standard test set T, we propose the contribution index, a new Shapley value based metric lit for assessing the contribution of each data provider for the joint model trained by federated learning. The contribution index shares the same properties as Shapley value. However, direct calculation of the contribution index is time consuming, since a large number of joint models with different combinations of data sets are required to be trained and evaluated. To solve this problem, we propose two gradient based methods. The idea is to reconstruct approximately the models on different combinations of the data sets through the intermediate results of the training process of federated learning so as to avoid extra training. The lirst method reconstructs models by updating the initial global model in federated learning with the gradients in different rounds. Then it calculates the contribution index by the performance of these reconstructed models. The second method calculates contribution index in each round by updating the global model in the previous round with the gradients in the current round. Contribution indexes of multiple rounds are then added with elaborated weights to get the linal result. We conduct extensive experiments on the MNIST data set in different settings. The results demonstrate that the proposed methods can approximate the exact contribution index effectively and achieve a time speed up of up to 2x-100x compared with the exact calculation and other baselines extended from existing work.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Song, Tianshu and Tong, Yongxin and Wei, Shuyue},
	month = dec,
	year = {2019},
	keywords = {Data models, Federated Learning, Google, Incentive Mechanism, Indexes, Machine learning, Servers, Shapley Value, Task analysis, Training},
	pages = {2577--2586},
}

@inproceedings{jia_towards_2019,
	title = {Towards {Efficient} {Data} {Valuation} {Based} on the {Shapley} {Value}},
	url = {https://proceedings.mlr.press/v89/jia19a.html},
	abstract = {\{{\textbackslash}em “How much is my data worth?”\} is an increasingly common question posed by organizations and individuals alike. An answer to this question could allow, for instance, fairly distributing profits among multiple data contributors and determining prospective compensation when data breaches happen. In this paper, we study the problem of {\textbackslash}emph\{data valuation\} by utilizing the Shapley value, a popular notion of value which originated in coopoerative game theory. The Shapley value defines a unique payoff scheme that satisfies many desiderata for the notion of data value. However, the Shapley value often requires {\textbackslash}emph\{exponential\} time to compute. To meet this challenge, we propose a repertoire of efficient algorithms for approximating the Shapley value. We also demonstrate the value of each training instance for various benchmark datasets.},
	language = {en},
	urldate = {2022-08-25},
	booktitle = {Proceedings of the {Twenty}-{Second} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Jia, Ruoxi and Dao, David and Wang, Boxin and Hubis, Frances Ann and Hynes, Nick and Gürel, Nezihe Merve and Li, Bo and Zhang, Ce and Song, Dawn and Spanos, Costas J.},
	month = apr,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1167--1176},
}

@article{Vasilomanolakis2015,
	title = {Taxonomy and {Survey} of {Collaborative} {Intrusion} {Detection}},
	volume = {47},
	doi = {10.1145/2716260},
	language = {en},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Vasilomanolakis, Emmanouil and Karuppayah, Shankar and Fischer, Mathias},
	month = may,
	year = {2015},
	keywords = {+survey, \_processed},
	pages = {33},
}

@misc{NIS2016,
	title = {Directive ({EU}) 2016/1148 of 6 {July} 2016 concerning measures for a high common level of security of network and information systems across the {Union}},
	url = {https://eur-lex.europa.eu/eli/dir/2016/1148/oj},
	abstract = {It proposes a wide-ranging set of measures to boost the level of security of network and information systems (cybersecurity*) to secure services vital to the EU economy and society. It aims to ensure that EU countries are well-prepared and are ready to handle and respond to cyberattacks through: the designation of competent authorities, the set-up of computer-security incident response teams (CSIRTs), and the adoption of national cybersecurity strategies. It also establishes EU-level cooperation both at strategic and technical level. Lastly, it introduces the obligation on essential-services providers and digital service providers to take the appropriate security measures and to notify the relevant national authorities about serious incidents.},
	author = {{The European Parliament and The Counsil}},
	year = {2016},
}

@article{yang_personalized_2022,
	title = {Personalized {Federated} {Learning} on {Non}-{IID} {Data} via {Group}-{Based} {Meta}-{Learning}},
	issn = {1556-4681},
	url = {https://doi.org/10.1145/3558005},
	doi = {10.1145/3558005},
	abstract = {Personalized federated learning (PFL) has emerged as a paradigm to provide a personalized model that can fit the local data distribution of each client. One natural choice for PFL is to leverage the fast adaptation capability of meta-learning, where it first obtains a single global model, and each client achieves a personalized model by fine-tuning the global one with its local data. However, existing meta-learning-based approaches implicitly assume that the data distribution among different clients is similar, which may not be applicable due to the property of data heterogeneity in federated learning. In this work, we propose a Group-based Federated Meta-Learning framework, called G-FML, which adaptively divides the clients into groups based on the similarity of their data distribution, and the personalized models are obtained with meta-learning within each group. In particular, we develop a simple yet effective grouping mechanism to adaptively partition the clients into multiple groups. Our mechanism ensures that each group is formed by the clients with similar data distribution such that the group-wise meta-model can achieve “personalization” at large. By doing so, our framework can be generalized to a highly heterogeneous environment. We evaluate the effectiveness of our proposed G-FML framework on three heterogeneous benchmarking datasets. The experimental results show that our framework improves the model accuracy by up to 13.15\% relative to the state-of-the-art federated meta-learning.},
	urldate = {2022-08-29},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Yang, Lei and Huang, Jiaming and Lin, Wanyu and Cao, Jiannong},
	year = {2022},
	note = {Just Accepted},
	keywords = {Federated learning, clustering methods, meta learning, neural networks},
}

@article{miao_privacy-preserving_2022,
	title = {Privacy-{Preserving} {Byzantine}-{Robust} {Federated} {Learning} via {Blockchain} {Systems}},
	volume = {17},
	issn = {1556-6021},
	doi = {10.1109/TIFS.2022.3196274},
	abstract = {Federated learning enables clients to train a machine learning model jointly without sharing their local data. However, due to the centrality of federated learning framework and the untrustworthiness of clients, traditional federated learning solutions are vulnerable to poisoning attacks from malicious clients and servers. In this paper, we aim to mitigate the impact of the central server and malicious clients by designing a Privacy-preserving Byzantine-robust Federated Learning (PBFL) scheme based on blockchain. Specifically, we use cosine similarity to judge the malicious gradients uploaded by malicious clients. Then, we adopt fully homomorphic encryption to provide secure aggregation. Finally, we use blockchain system to facilitate transparent processes and implementation of regulations. Our formal analysis proves that our scheme achieves convergence and provides privacy protection. Our extensive experiments on different datasets demonstrate that our scheme is robust and efficient. Even if the root dataset is small, our scheme can achieve the same efficiency as FedSGD.},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Miao, Yinbin and Liu, Ziteng and Li, Hongwei and Choo, Kim-Kwang Raymond and Deng, Robert H.},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Information Forensics and Security},
	keywords = {Blockchains, Collaborative work, Computational modeling, Federated learning, Privacy, Resists, Servers, Training, blockchain, fully homomorphic encryption, poisoning attacks},
	pages = {2848--2861},
}

@inproceedings{peri_deep_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Deep k-{NN} {Defense} {Against} {Clean}-{Label} {Data} {Poisoning} {Attacks}},
	isbn = {978-3-030-66415-2},
	doi = {10.1007/978-3-030-66415-2_4},
	abstract = {Targeted clean-label data poisoning is a type of adversarial attack on machine learning systems in which an adversary injects a few correctly-labeled, minimally-perturbed samples into the training data, causing a model to misclassify a particular test sample during inference. Although defenses have been proposed for general poisoning attacks, no reliable defense for clean-label attacks has been demonstrated, despite the attacks’ effectiveness and realistic applications. In this work, we propose a simple, yet highly-effective Deep k-NN defense against both feature collision and convex polytope clean-label attacks on the CIFAR-10 dataset. We demonstrate that our proposed strategy is able to detect over 99\% of poisoned examples in both attacks and remove them without compromising model performance. Additionally, through ablation studies, we discover simple guidelines for selecting the value of k as well as for implementing the Deep k-NN defense on real-world datasets with class imbalance. Our proposed defense shows that current clean-label poisoning attack strategies can be annulled, and serves as a strong yet simple-to-implement baseline defense to test future clean-label poisoning attacks. Our code is available on GitHub.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Peri, Neehar and Gupta, Neal and Huang, W. Ronny and Fowl, Liam and Zhu, Chen and Feizi, Soheil and Goldstein, Tom and Dickerson, John P.},
	editor = {Bartoli, Adrien and Fusiello, Andrea},
	year = {2020},
	keywords = {Adversarial attacks, Clean label poisoning, Deep k-NN, Machine learning},
	pages = {55--70},
}

@misc{dunnett_trusted_2022,
	title = {A {Trusted}, {Verifiable} and {Differential} {Cyber} {Threat} {Intelligence} {Sharing} {Framework} using {Blockchain}},
	url = {http://arxiv.org/abs/2208.12031},
	abstract = {Cyber Threat Intelligence (CTI) is the knowledge of cyber and physical threats that help mitigate potential cyber attacks. The rapid evolution of the current threat landscape has seen many organisations share CTI to strengthen their security posture for mutual beneﬁt. However, in many cases, CTI data contains attributes (e.g., software versions) that have the potential to leak sensitive information or cause reputational damage to the sharing organisation. While current approaches allow restricting CTI sharing to trusted organisations, they lack solutions where the shared data can be veriﬁed and disseminated ‘differentially’ (i.e., selective information sharing) with policies and metrics ﬂexibly deﬁned by an organisation. In this paper, we propose a blockchain-based CTI sharing framework that allows organisations to share sensitive CTI data in a trusted, veriﬁable and differential manner. We discuss the limitations associated with existing approaches and highlight the advantages of the proposed CTI sharing framework. We further present a detailed proof of concept using the Ethereum blockchain network. Our experimental results show that the proposed framework can facilitate the exchange of CTI without creating signiﬁcant additional overheads.},
	language = {en},
	urldate = {2022-08-30},
	publisher = {arXiv},
	author = {Dunnett, Kealan and Pal, Shantanu and Putra, Guntur Dharma and Jadidi, Zahra and Jurdak, Raja},
	month = aug,
	year = {2022},
	note = {arXiv:2208.12031 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Distributed, Parallel, and Cluster Computing},
}

@article{wang_threats_2022,
	title = {Threats to {Training}: {A} {Survey} of {Poisoning} {Attacks} and {Defenses} on {Machine} {Learning} {Systems}},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Threats to {Training}},
	url = {https://dl.acm.org/doi/10.1145/3538707},
	doi = {10.1145/3538707},
	abstract = {Machine learning (ML) has been universally adopted for automated decisions in a variety of ields, including recognition and classiication applications, recommendation systems, natural language processing, etc. However, in the light of high expenses on training data and computing resources, recent years have witnessed a rapid increase in outsourced ML training, either partially or completely, which provides vulnerabilities for adversaries to exploit. A prime threat in training phase is called poisoning attack, where adversaries strive to subvert the behavior of machine learning systems by poisoning training data or other means of interference. Although a growing number of relevant studies have been proposed, the research among poisoning attack is still overly scattered, with each paper focusing on a particular task in a speciic domain. In this survey, we summarize and categorize existing attack methods and corresponding defenses, as well as demonstrate compelling application scenarios, thus providing a uniied framework to analyze poisoning attacks. Besides, we also discuss the main limitations of current works, along with the corresponding future directions to facilitate further researches. Our ultimate motivation is to provide a comprehensive and self-contained survey of this growing ield of research and lay the foundation for a more standardized approach to reproducible studies. CCS Concepts: · Theory of computation → Adversarial learning; · Security and privacy → Systems security.},
	language = {en},
	urldate = {2022-09-01},
	journal = {ACM Computing Surveys},
	author = {Wang, Zhibo and Ma, Jingjing and Wang, Xue and Hu, Jiahui and Qin, Zhan and Ren, Kui},
	month = may,
	year = {2022},
	pages = {3538707},
}

@misc{ramirez_poisoning_2022,
	title = {Poisoning {Attacks} and {Defenses} on {Artificial} {Intelligence}: {A} {Survey}},
	shorttitle = {Poisoning {Attacks} and {Defenses} on {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2202.10276},
	doi = {10.48550/arXiv.2202.10276},
	abstract = {Machine learning models have been widely adopted in several fields. However, most recent studies have shown several vulnerabilities from attacks with a potential to jeopardize the integrity of the model, presenting a new window of research opportunity in terms of cyber-security. This survey is conducted with a main intention of highlighting the most relevant information related to security vulnerabilities in the context of machine learning (ML) classifiers; more specifically, directed towards training procedures against data poisoning attacks, representing a type of attack that consists of tampering the data samples fed to the model during the training phase, leading to a degradation in the models accuracy during the inference phase. This work compiles the most relevant insights and findings found in the latest existing literatures addressing this type of attacks. Moreover, this paper also covers several defense techniques that promise feasible detection and mitigation mechanisms, capable of conferring a certain level of robustness to a target model against an attacker. A thorough assessment is performed on the reviewed works, comparing the effects of data poisoning on a wide range of ML models in real-world conditions, performing quantitative and qualitative analyses. This paper analyzes the main characteristics for each approach including performance success metrics, required hyperparameters, and deployment complexity. Moreover, this paper emphasizes the underlying assumptions and limitations considered by both attackers and defenders along with their intrinsic properties such as: availability, reliability, privacy, accountability, interpretability, etc. Finally, this paper concludes by making references of some of main existing research trends that provide pathways towards future research directions in the field of cyber-security.},
	urldate = {2022-09-01},
	publisher = {arXiv},
	author = {Ramirez, Miguel A. and Kim, Song-Kyoo and Hamadi, Hussam Al and Damiani, Ernesto and Byon, Young-Ji and Kim, Tae-Yeon and Cho, Chung-Suk and Yeun, Chan Yeob},
	month = feb,
	year = {2022},
	note = {arXiv:2202.10276 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{guo_robust_2021,
	title = {Robust and {Privacy}-{Preserving} {Collaborative} {Learning}: {A} {Comprehensive} {Survey}},
	shorttitle = {Robust and {Privacy}-{Preserving} {Collaborative} {Learning}},
	url = {http://arxiv.org/abs/2112.10183},
	doi = {10.48550/arXiv.2112.10183},
	abstract = {With the rapid demand of data and computational resources in deep learning systems, a growing number of algorithms to utilize collaborative machine learning techniques, for example, federated learning, to train a shared deep model across multiple participants. It could effectively take advantage of the resources of each participant and obtain a more powerful learning system. However, integrity and privacy threats in such systems have greatly obstructed the applications of collaborative learning. And a large amount of works have been proposed to maintain the model integrity and mitigate the privacy leakage of training data during the training phase for different collaborative learning systems. Compared with existing surveys that mainly focus on one specific collaborative learning system, this survey aims to provide a systematic and comprehensive review of security and privacy researches in collaborative learning. Our survey first provides the system overview of collaborative learning, followed by a brief introduction of integrity and privacy threats. In an organized way, we then detail the existing integrity and privacy attacks as well as their defenses. We also list some open problems in this area and opensource the related papers on GitHub: https://github.com/csl-cqu/awesome-secure-collebrative-learning-papers.},
	urldate = {2022-09-01},
	publisher = {arXiv},
	author = {Guo, Shangwei and Zhang, Xu and Yang, Fei and Zhang, Tianwei and Gan, Yan and Xiang, Tao and Liu, Yang},
	month = dec,
	year = {2021},
	note = {arXiv:2112.10183 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}

@article{tian_comprehensive_2022,
	title = {A {Comprehensive} {Survey} on {Poisoning} {Attacks} and {Countermeasures} in {Machine} {Learning}},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3551636},
	doi = {10.1145/3551636},
	abstract = {The prosperity of machine learning has been accompanied by increasing attacks on the training process. Among them, poisoning attacks have become an emerging threat during model training. Poisoning attacks have profound impacts on the target models, e.g., making them unable to converge or manipulating their prediction results. Moreover, the rapid development of recent distributed learning frameworks, especially federated learning, has further stimulated the development of poisoning attacks. Defending against poisoning attacks is challenging and urgent. However, the systematic review from a unified perspective remains blank. This survey provides an in-depth and up-to-date overview of poisoning attacks and corresponding countermeasures in both centralized and federated learning. We firstly categorize attack methods based on their goals. Secondly, we offer detailed analysis of the differences and connections among the attack techniques. Furthermore, we present countermeasures in different learning framework and highlight their advantages and disadvantages. Finally, we discuss the reasons for the feasibility of poisoning attacks and address the potential research directions from attacks and defenses perspectives, separately.},
	urldate = {2022-09-01},
	journal = {ACM Computing Surveys},
	author = {Tian, Zhiyi and Cui, Lei and Liang, Jie and Yu, Shui},
	year = {2022},
	note = {Just Accepted},
	keywords = {Deep learning, backdoor attack, federated learning, poisoning attack},
}

@inproceedings{hsieh_non-iid_2020,
	title = {The {Non}-{IID} {Data} {Quagmire} of {Decentralized} {Machine} {Learning}},
	url = {https://proceedings.mlr.press/v119/hsieh20a.html},
	abstract = {Many large-scale machine learning (ML) applications need to perform decentralized learning over datasets generated at different devices and locations. Such datasets pose a significant challenge to decentralized learning because their different contexts result in significant data distribution skew across devices/locations. In this paper, we take a step toward better understanding this challenge by presenting a detailed experimental study of decentralized DNN training on a common type of data skew: skewed distribution of data labels across devices/locations. Our study shows that: (i) skewed data labels are a fundamental and pervasive problem for decentralized learning, causing significant accuracy loss across many ML applications, DNN models, training datasets, and decentralized learning algorithms; (ii) the problem is particularly challenging for DNN models with batch normalization; and (iii) the degree of data skew is a key determinant of the difficulty of the problem. Based on these findings, we present SkewScout, a system-level approach that adapts the communication frequency of decentralized learning algorithms to the (skew-induced) accuracy loss between data partitions. We also show that group normalization can recover much of the accuracy loss of batch normalization.},
	language = {en},
	urldate = {2022-09-01},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hsieh, Kevin and Phanishayee, Amar and Mutlu, Onur and Gibbons, Phillip},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {4387--4398},
}

@article{zhou_differentially_2022,
	title = {A {Differentially} {Private} {Federated} {Learning} {Model} against {Poisoning} {Attacks} in {Edge} {Computing}},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2022.3168556},
	abstract = {Federated learning is increasingly popular, as it allows us to circumvent challenges due to data islands, by training a global model using data from one or more data owners/sources. However, in edge computing, resource-constrained end devices are vulnerable to be compromised and abused to facilitate poisoning attacks. Privacy-preserving is another important property to consider when dealing with sensitive user data on end devices. Most existing approaches only consider either defending against poisoning attacks or supporting privacy, but not both properties simultaneously. In this paper, we propose a differentially private federated learning model against poisoning attacks, designed for edge computing deployment. First, we design a weight-based algorithm to perform anomaly detection on the parameters uploaded by end devices in edge nodes, which improves detection rate using only small-size validation datasets and minimizes the communication cost. Then, differential privacy technology is leveraged to protect the privacy of both data and model in an edge computing setting. We also evaluate and compare the detection performance in the presence of random and customized malicious end devices with the state-of-the-art, in terms of attack resiliency, communication and computation costs. Experimental results demonstrate that our scheme can achieve an optimal tradeoff between security, efficiency and accuracy.},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Zhou, Jun and Wu, Nan and Wang, Yisong and Gu, Shouzhen and Cao, Zhenfu and Dong, Xiaolei and Choo, Kim-Kwang Raymond},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Collaborative work, Computational modeling, Data models, Differential privacy, Edge computing, Federated learning, High-practicability, Image edge detection, Poisoning attack, Privacy, Training},
	pages = {1--1},
}

@inproceedings{you_poisoning_2022,
	title = {Poisoning attack detection using client historical similarity in non-iid environments},
	doi = {10.1109/Confluence52989.2022.9734158},
	abstract = {Federated learning has drawn widespread attention as privacy-preserving solution, which has a protective effect on data security and privacy. It has unique distributed machine learning mechanism, namely model sharing instead of data sharing. However, the mechanism also leads to the fact that malicious clients can easily train local model based on poisoned data and upload it to the server for contaminating the global model, thus severely hampering the development of federated learning. In this paper, we build a federated learning system and simulate heterogeneous data on each client for training. Although we cannot directly differentiate malicious customers by the uploaded model in a heterogeneous data environment, by experiments we found some features that are used to distinguish malicious customers from benign customers during training. Given above, we propose a federated learning poisoning attack detection method for detecting malicious clients and ensuring aggregation quality. The method can filter out anomaly models by comparing the similarity of the historical changes of clients and gradually identifying attacker clients through reputation mechanism. We experimentally demonstrate that the method significantly improves the performance of the global model even when the proportion of malicious clients is as high as one-third.},
	booktitle = {2022 12th {International} {Conference} on {Cloud} {Computing}, {Data} {Science} \& {Engineering} ({Confluence})},
	author = {You, XinTong and Liu, Zhengqi and Yang, Xu and Ding, Xuyang},
	month = jan,
	year = {2022},
	keywords = {Collaborative work, Distributed Machine Learning, Distributed databases, Euclidean distance, Federated Learning, Heterogeneous Data, Machine learning, Market research, Poisoning Attack Detection, Resists, Training},
	pages = {439--447},
}

@misc{sun_dpauc_2022,
	title = {{DPAUC}: {Differentially} {Private} {AUC} {Computation} in {Federated} {Learning}},
	shorttitle = {{DPAUC}},
	url = {http://arxiv.org/abs/2208.12294},
	abstract = {Federated learning (FL) has gained signiﬁcant attention recently as a privacyenhancing tool to jointly train a machine learning model by multiple participants. The prior work on FL has mostly studied how to protect label privacy during model training. However, model evaluation in FL might also lead to potential leakage of private label information. In this work, we propose an evaluation algorithm that can accurately compute the widely used AUC (area under the curve) metric when using the label differential privacy (DP) in FL. Through extensive experiments, we show our algorithms can compute accurate AUCs compared to the ground truth.},
	language = {en},
	urldate = {2022-09-05},
	publisher = {arXiv},
	author = {Sun, Jiankai and Yang, Xin and Yao, Yuanshun and Xie, Junyuan and Wu, Di and Wang, Chong},
	month = aug,
	year = {2022},
	note = {arXiv:2208.12294 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{severi_network-level_2022,
	title = {Network-{Level} {Adversaries} in {Federated} {Learning}},
	url = {http://arxiv.org/abs/2208.12911},
	abstract = {Federated learning is a popular strategy for training models on distributed, sensitive data, while preserving data privacy. Prior work identiﬁed a range of security threats on federated learning protocols that poison the data or the model. However, federated learning is a networked system where the communication between clients and server plays a critical role for the learning task performance. We highlight how communication introduces another vulnerability surface in federated learning and study the impact of network-level adversaries on training federated learning models. We show that attackers dropping the network trafﬁc from carefully selected clients can signiﬁcantly decrease model accuracy on a target population. Moreover, we show that a coordinated poisoning campaign from a few clients can amplify the dropping attacks. Finally, we develop a server-side defense which mitigates the impact of our attacks by identifying and up-sampling clients likely to positively contribute towards target accuracy. We comprehensively evaluate our attacks and defenses on three datasets, assuming encrypted communication channels and attackers with partial visibility of the network.},
	language = {en},
	urldate = {2022-09-05},
	publisher = {arXiv},
	author = {Severi, Giorgio and Jagielski, Matthew and Yar, Gökberk and Wang, Yuxuan and Oprea, Alina and Nita-Rotaru, Cristina},
	month = aug,
	year = {2022},
	note = {arXiv:2208.12911 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture},
}

@article{song_reputation-based_2022,
	title = {Reputation-{Based} {Federated} {Learning} for {Secure} {Wireless} {Networks}},
	volume = {9},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2021.3079104},
	abstract = {The dilemma between the ever-increasing demands for data processing, and the limited capabilities of mobile devices in a wireless communication system calls for the appearance of federated learning (FL). As a distributed machine learning (ML) method, FL executes in an iterative manner by distributing the global model parameters and aggregating the local model parameters, which avoids the transmission of huge raw data and preserves data privacy during the training process. However, since FL cannot control the local training and transmission process, this gives malicious users the opportunity to deteriorate the global aggregation. We adopt a reputation model based on beta distribution function to measure the credibility of local users, and propose a reputation-based scheduling policy with user fairness constraint. By taking into account the impact of wireless channel conditions and malicious attack features, we derive tractable expressions for the convergence rate of FL in a wireless setting. Moreover, we validate the superiority of the proposed reputation-based scheduling policy via numerical analysis and empirical simulations. The results show that the proposed secure wireless FL framework can not only distinguish malicious users from normal users but also effectively defend against several typical attack types featured in attack intensity and attack frequency. The analysis also reveals that the effect of average attack intensity on the convergence performance of FL is dominated by the percentage of malicious user equipments (UEs), and imposes even greater negative effect on the convergence performance of FL as the percentage of malicious UEs increases.},
	number = {2},
	journal = {IEEE Internet of Things Journal},
	author = {Song, Zhendong and Sun, Hongguang and Yang, Howard H. and Wang, Xijun and Zhang, Yan and Quek, Tony Q. S.},
	month = jan,
	year = {2022},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Communication system security, Convergence, Convergence analysis, Data models, Reliability, Scheduling, Training, Wireless networks, federated learning (FL), malicious users, reputation-based scheduling policy, secure wireless networks},
	pages = {1212--1226},
}

@article{huang_personalized_2021,
	title = {Personalized {Cross}-{Silo} {Federated} {Learning} on {Non}-{IID} {Data}},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16960},
	doi = {10.1609/aaai.v35i9.16960},
	abstract = {Non-IID data present a tough challenge for federated learning. In this paper, we explore a novel idea of facilitating pairwise collaborations between clients with similar data. We propose FedAMP, a new method employing federated attentive message passing to facilitate similar clients to collaborate more. We establish the convergence of FedAMP for both convex and non-convex models, and propose a heuristic method to further improve the performance of FedAMP when clients adopt deep neural networks as personalized models. Our extensive experiments on benchmark data sets demonstrate the superior performance of the proposed methods.},
	language = {en},
	number = {9},
	urldate = {2022-09-26},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Huang, Yutao and Chu, Lingyang and Zhou, Zirui and Wang, Lanjun and Liu, Jiangchuan and Pei, Jian and Zhang, Yong},
	month = may,
	year = {2021},
	pages = {7865--7873},
}

@inproceedings{zhang_blockchain_2021,
	title = {Blockchain {Empowered} {Reliable} {Federated} {Learning} by {Worker} {Selection}: {A} {Trustworthy} {Reputation} {Evaluation} {Method}},
	shorttitle = {Blockchain {Empowered} {Reliable} {Federated} {Learning} by {Worker} {Selection}},
	doi = {10.1109/WCNCW49093.2021.9420026},
	abstract = {Federated learning is a distributed machine learning framework that enables distributed model training with local datasets, which can effectively protect the data privacy of workers (i.e., intelligent edge nodes). The majority of federated learning algorithms assume that the workers are trusted and voluntarily participate in the cooperative model training process. However, the situation in practical application is not consistent with this. There are many challenges such as worker selection schemes for participating workers, which hamper the widespread adoption of federated learning. The existing research about worker selection scheme focused on multi-weight subjective logic model to calculate reputation value and adopted contract theory to motivate workers, which may exist subjective judgmental factors and unfair profit distribution. To address above challenges, we calculate the reputation value by model quality parameters to evaluate the reliability of workers. Blockchain is designed to store historical reputation value that realized tamperresistance and non-repudiation. Numerical results indicate that the worker selection scheme can improve the accuracy of the model and accelerate the model convergence.},
	booktitle = {2021 {IEEE} {Wireless} {Communications} and {Networking} {Conference} {Workshops} ({WCNCW})},
	author = {Zhang, Qinnan and Ding, Qingyang and Zhu, Jianming and Li, Dandan},
	month = mar,
	year = {2021},
	keywords = {Analytical models, Blockchain, Conferences, Data privacy, Machine learning, Predictive models, Training, blockchain, consensus algorithm, federated learning, reputation evaluation},
	pages = {1--6},
}

@inproceedings{cao_understanding_2019,
	title = {Understanding {Distributed} {Poisoning} {Attack} in {Federated} {Learning}},
	doi = {10.1109/ICPADS47876.2019.00042},
	abstract = {Federated learning is inherently vulnerable to poisoning attacks, since no training samples will be released to and checked by trustworthy authority. Poisoning attacks are widely investigated in centralized learning paradigm, however distributed poisoning attacks, in which more than one attacker colludes with each other, and injects malicious training samples into local models of their own, may result in a greater catastrophe in federated learning intuitively. In this paper, through real implementation of a federated learning system and distributed poisoning attacks, we obtain several observations about the relations between the number of poisoned training samples, attackers, and attack success rate. Moreover, we propose a scheme, Sniper, to eliminate poisoned local models from malicious participants during training. Sniper identifies benign local models by solving a maximum clique problem, and suspected (poisoned) local models will be ignored during global model updating. Experimental results demonstrate the efficacy of Sniper. The attack success rates are reduced to around 2\% even a third of participants are attackers.},
	booktitle = {2019 {IEEE} 25th {International} {Conference} on {Parallel} and {Distributed} {Systems} ({ICPADS})},
	author = {Cao, Di and Chang, Shan and Lin, Zhijian and Liu, Guohua and Sun, Donghong},
	month = dec,
	year = {2019},
	note = {ISSN: 1521-9097},
	keywords = {federated learning, distributed poisoning attack, defense, attack success rate, label-flipping},
	pages = {233--239},
}

@article{huang_eefed_2022,
	title = {{EEFED}: {Personalized} federated learning of {Execution}\&{Evaluation} dual network for {CPS} intrusion detection},
	issn = {1556-6021},
	shorttitle = {{EEFED}},
	doi = {10.1109/TIFS.2022.3214723},
	abstract = {In the modern interconnected world, intelligent networks and computing technologies are increasingly being incorporated in industrial systems. However, this adoption of advanced technology has resulted in increased cyber threats to cyber-physical systems. Existing intrusion detection systems are continually challenged by constantly evolving cyber threats. Machine learning algorithms have been applied for intrusion detection. In these techniques, a classification model is trained by learning cyber behavior patterns. However, these models typically require considerable high-quality datasets. Limited attack samples are available because of the unpredictability and constant evolution of cyber threats. To address these problems, we propose a novel federated Execution\&Evaluation dual network framework (EEFED), which allows multiple federal participants to personalize their local detection models undermining the original purpose of Federated Learning. Thus, a general global detection model was developed for collaboratively improving the performance of a single local model against cyberattacks. The proposed personalized update algorithm and the optimizing backtracking parameters replacement policy effectively reduced the negative influence of federated learning in imbalanced and non-i.i.d distribution of data. The proposed method improved model stability. Furthermore, extensive experiments conducted on a network dataset in various cyber scenarios revealed that the proposed method outperformed single model and state-of-the-art methods.},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Huang, Xianting and Liu, Jing and Lai, Yingxu and Mao, Beifeng and Lyu, Hongshuo},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Information Forensics and Security},
	keywords = {Computational modeling, Computer crime, Data models, Federated learning, Intrusion detection, Security, Training, cyber security, cyber-physical system (CPS), intrusion detection, personalized model},
	pages = {1--1},
}

@misc{chu_securing_2022,
	title = {Securing {Federated} {Sensitive} {Topic} {Classification} against {Poisoning} {Attacks}},
	url = {http://arxiv.org/abs/2201.13086},
	doi = {10.48550/arXiv.2201.13086},
	abstract = {We present a Federated Learning (FL) based solution for building a distributed classifier capable of detecting URLs containing GDPR-sensitive content related to categories such as health, sexual preference, political beliefs, etc. Although such a classifier addresses the limitations of previous offline/centralised classifiers,it is still vulnerable to poisoning attacks from malicious users that may attempt to reduce the accuracy for benign users by disseminating faulty model updates. To guard against this, we develop a robust aggregation scheme based on subjective logic and residual-based attack detection. Employing a combination of theoretical analysis, trace-driven simulation, as well as experimental validation with a prototype and real users, we show that our classifier can detect sensitive content with high accuracy, learn new labels fast, and remain robust in view of poisoning attacks from malicious users, as well as imperfect input from non-malicious ones.},
	urldate = {2022-10-20},
	publisher = {arXiv},
	author = {Chu, Tianyue and Garcia-Recuero, Alvaro and Iordanou, Costas and Smaragdakis, Georgios and Laoutaris, Nikolaos},
	month = jan,
	year = {2022},
	note = {arXiv:2201.13086 [cs]},
	keywords = {68M25, Computer Science - Cryptography and Security, Computer Science - Distributed, Parallel, and Cluster Computing, I.2.11, K.4.1},
}

@inproceedings{karimireddy_learning_2021,
	title = {Learning from {History} for {Byzantine} {Robust} {Optimization}},
	url = {https://proceedings.mlr.press/v139/karimireddy21a.html},
	abstract = {Byzantine robustness has received significant attention recently given its importance for distributed and federated learning. In spite of this, we identify severe flaws in existing algorithms even when the data across the participants is identically distributed. First, we show realistic examples where current state of the art robust aggregation rules fail to converge even in the absence of any Byzantine attackers. Secondly, we prove that even if the aggregation rules may succeed in limiting the influence of the attackers in a single round, the attackers can couple their attacks across time eventually leading to divergence. To address these issues, we present two surprisingly simple strategies: a new robust iterative clipping procedure, and incorporating worker momentum to overcome time-coupled attacks. This is the first provably robust method for the standard stochastic optimization setting.},
	language = {en},
	urldate = {2022-10-21},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Karimireddy, Sai Praneeth and He, Lie and Jaggi, Martin},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {5311--5319},
}
